= Migrate data using {astra-db} {sstable-sideloader}
:page-aliases: data-importer:data-importer-overview.adoc
:description: {sstable-sideloader} lets you migrate data from an {cass-reg} or {dse} cluster into {astra-db} without impacting the origin cluster or your {astra-db} Serverless database.
:loop-var: pass:[${i}]

//TODO: Check for other duplicate content that could be moved to the partial file.
//TODO: Check links and attributes
//TODO: Redirect & unhide

{sstable-sideloader} is a service running in {astra-db} that directly imports data from snapshot backups that you've uploaded to {astra-db} from an existing {cass-reg} or {dse} cluster.

Because it imports data directly, {sstable-sideloader} can offer several advantages over CQL-based tools like xref:dsbulk:overview:dsbulk-about.adoc[{company} Bulk Loader (DSBulk)] and xref:ROOT:cassandra-data-migrator.adoc[{cass-short} Data Migrator (CDM)], including faster, more cost-effective data loading, and minimal performance impacts on your origin cluster and target database.

[TIP]
====
If you need to migrate a live database, you can use {sstable-sideloader} instead of DSBulk or {cass-short} Data Migrator during of xref:ROOT:migrate-and-validate-data.adoc[Phase 2 of Zero Downtime Migration (ZDM)].

.Use {sstable-sideloader} in the context of Zero Downtime Migration
image::sideloader:data-importer-zdm.svg[]
====

[#sideloader-process]
== The {sstable-sideloader} process

Transferring data with {sstable-sideloader} is a multi-phase process.
Before you use {sstable-sideloader}, learn about the events, outcomes, warnings, and requirements of each phase:

.{sstable-sideloader} concepts
[%collapsible]
====
Origin, origin cluster::
In the context of {sstable-sideloader}, this refers to your existing {cass-short} or {dse-short} cluster.

Target, target database::
In the context of {sstable-sideloader}, this refers to the {astra-db} Serverless database where you will migrate your data.

Administration server::
A server where you can run the migration commands, including CLI commands and {astra-db} {devops-api} calls.
This server must have SSH access to each node in your origin cluster.

Migration::
A workflow that you initiate within {sstable-sideloader} that encompasses the lifecycle of uploading and importing snapshot backups of a specific set of keyspaces or CQL tables.
+
This process produces artifacts and parameters including migration buckets, migration IDs, migration directories, and upload credentials.
You use these components throughout the migration workflow.
====

.Prepare your infrastructure
[%collapsible]
====
There are limitations to using {sstable-sideloader} that you must consider before you start a migration.
Additionally, you must take steps to prepare your target database, origin cluster, and administration server before you begin the migration.

For more information, see <<sideloader-requirements>>.
====

.Create snapshot backups
[%collapsible]
====
{sstable-sideloader} uses snapshot backup files to import SSTable data from your existing {cass-short} or {dse-short} cluster (origin).
This is an ideal approach for database migrations because creating a snapshot has negligible performance impact on the origin cluster, and it preserves metadata like write timestamps and expiration times (TTLs).

Each snapshot for each node in the origin cluster must include all the keyspaces and individual CQL tables that you want to migrate.

For more information, see <<create-snapshots>>.
====

.Prepare the target database
[%collapsible]
====
Because snapshots don't store schema definitions, you must pre-configure the schema definition in your target {astra-db} database so that it matches the origin cluster's schema.

For the migration to succeed, the schema in your target database must align with the schema in the origin cluster.
However, you might need to modify your schema or data model to be compatible with {astra-db}.

For specific requirements and more information, see <<record-schema>>.
====

.Initialize a migration
[%collapsible]
====
After you create snapshots on the origin cluster and pre-configure the schema on the target database, you can <<initialize-migration,use the {astra-db} {devops-api} to initialize the migration>>.

image::sideloader:data-importer-workflow.svg[]

When you initialize a migration, {sstable-sideloader} does the following:

. Creates a secure migration bucket.
+
The migration bucket is only created during the first initialization.
All subsequent migrations use different directories in the same migration bucket.
+
{company} owns the migration bucket, and it is located within the {astra-db} perimeter.

. Generates a migration ID that is unique to the new migration.

. Creates a migration directory within the migration bucket that is unique to the new migration.
+
The migration directory is also referred to as the `uploadBucketDir`.
In the next phase of the migration process, you will upload your snapshots to this migration directory.

. Generates upload credentials that grant read/write access to the migration directory.
+
The credentials are formatted according to the cloud provider where your target database is located.
====

.Upload snapshots
[%collapsible]
====
When initialization is complete, use your cloud provider's CLI to <<upload-snapshots-to-migration-directory,upload your snapshots to the migration directory>>.

To upload snapshots directly from the origin cluster, you must install your cloud provider's CLI on each node in the origin cluster.
Alternatively, you can upload copies of the snapshots from a separate staging server that has the CLI installed.

The time required to upload the snapshots depends on the size of your dataset and the network throughput between the origin cluster and the migration bucket.

[cols="10,30,60"]
|===
|Speed |Migration type |Description

|Fastest
|Inter-datacenter
|All else equal, snapshots take the least time to upload when the origin cluster is in the same cloud provider and region as the target database.

|Fast
|Cross-datacenter, co-located
|Uploads are slower by default when they must exit the local datacenter.
The delay increases relative to the physical distance between the datacenters.

For example, all else equal, uploading from AWS `us-east-1` (Dulles, VA, USA) to AWS `ca-central-1` (Montréal, QC, Canada) is faster than uploading from `us-east-1` to `us-west-2` (The Dalles, OR, USA) because Oregon is significantly further from Virginia than Montréal.

|Variable
|Cross-provider, co-located
|If the target database is in a different cloud provider than the origin cluster, the upload can be slower as the data passes from one provider's infrastructure to another.

This is considered a cross-datacenter transfer, and the delay increases relative to the physical distance between the datacenters.

|Slowest
|Transoceanic
|The slowest uploads happen when the data must travel over transoceanic cables.
If the data must also change cloud providers, there can be additional delays.

In this case, consider creating your target database in a co-located datacenter, and then xref:astra-db-serverless:databases:manage-regions.adoc[deploy your database to other regions] after the migration.
|===
====

.Import data
[%collapsible]
======
After uploading the snapshots to the migration directory, <<import-data,use the {devops-api} to start the data import process>>.

During the import process, {sstable-sideloader} does the following:

. Revokes access to the migration directory.
+
You cannot read or write to the migration directory after starting the data import process.

. Discovers all uploaded SSTables in the migration directory, and then groups them into approximately same-sized subsets.

. Runs validation checks on each subset.

. Converts all SSTables of each subset.

. Disables new compactions on the target database.
+
[WARNING]
====
This is the last point at which you can xref:sideloader:sideloader-overview.adoc#abort-migration[abort the migration].

Once {sstable-sideloader} begins to import SSTable metadata (the next step), you cannot stop the migration.
====

. Imports metadata from each SSTable.
+
If the dataset contains tombstones, any read operations on the target database can return inconsistent results during this step.
Since compaction is disabled, there is no risk of permanent inconsistencies.
However, in the context of xref:ROOT:introduction.adoc[Zero Downtime Migration], it's important that the ZDM proxy continues to read from the origin cluster.

. Re-enables compactions on the {astra-db} Serverless database.

Each step must finish successfully.
If one step fails, the import operation stops and no data is imported into your target database.

If all steps finish successfully, the migration is complete and you can access the imported data in your target database.
======

.Validate imported data
[%collapsible]
====
include::sideloader:partial$sideloader-partials.adoc[tags=validate]
====

[#sideloader-requirements]
== Prepare to use {sstable-sideloader}

To use {sstable-sideloader}, your target database, origin cluster, and administration server must meet the following requirements.

=== Target {astra-db} database requirements

* Your {astra-db} organization must be on an *Enterprise* xref:astra-db-serverless:administration:subscription-plans.adoc[subscription plan].
+
{sstable-sideloader} is a premium feature that incurs costs based on usage.

* Your target database must be an {astra-db} Serverless database.
If you don't already have one, xref:astra-db-serverless:databases:create-database.adoc[create a database].
You can use either a {db-serverless} or {db-serverless-vector} database, and {db-serverless-vector} databases can store both vector and non-vector data.

////
//TODO: Uncomment for GA. Link will not work until PCU PR is merge.
* Your target database must be in a PCU group.
+
Because {sstable-sideloader} operations are typically short-term, resource-intensive events, {company} recommends that you xref:astra-db-serverless:administration:create-pcu.adoc#flexible-capacity[create a flexible capacity PCU group] exclusively to support your target database during the migration.
+
After the migration, you can remove your target database out of the flexible capacity PCU group, and then park or delete the group.
Don't park the PCU group during the {sstable-sideloader} process because databases in a parked PCU group are hibernated and unavailable for use.
+
For more information, see xref:astra-db-serverless:administration:provisioned-capacity-units.adoc[].
////

[#origin-cluster-requirements]
=== Origin cluster requirements

The following requirements, recommendations, and limitations apply to origin clusters.
Review all of these to ensure that your cluster is compatible with {sstable-sideloader}.

* *Host*: Your origin cluster can be hosted on premises or on any cloud provider.

* *Database*:
+
** OSS {cass-short} 3.11 or later
** {dse-short} 5.1 or later
** {hcd-short} 1.1 or later
//Due to a potential occasional issue affecting SSTables generated by C* 3.0 / DSE 5.0. Until the fix is rolled out, we need to restrict the SSTable versions.

* *Partitioner*: Must use the default https://cassandra.apache.org/doc/stable/cassandra/configuration/cass_yaml_file.html#partitioner[partitioner], `Murmur3Partitioner`.
+
Older partitioners, such as `RandomPartitioner`, `ByteOrderedPartitioner`, and `OrderPreservingPartitioner`, are not supported.

* *Cloud provider CLI*: To upload snapshots directly from the origin cluster, you must install your cloud provider's CLI on each node in the origin cluster.
The tool you install depends on the region where your target {astra-db} database is deployed:
+
** AWS regions: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[Install AWS CLI]
** Google Cloud regions: https://cloud.google.com/sdk/docs/install-sdk[Install gcloud] and https://cloud.google.com/storage/docs/gsutil_install[install gsutil]
** Microsoft Azure regions: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli[Install Azure CLI]
+
Alternatively, you can upload copies of the snapshots from separate staging server that has the CLI installed, and you must coordinate this through the administration server.
However, this process _isn't_ covered in this guide.
The CLI commands in this guide assume you have installed your cloud provider's CLI on the nodes in the origin cluster.
If you choose the alternative option, you must modify the commands accordingly for your environment.

* *{sstable-sideloader} doesn't support encrypted data*: If your origin cluster uses xref:6.9@dse:securing:transparent-data-encryption.adoc[{dse-short} Transparent Data Encryption], be aware that {sstable-sideloader} can't migrate these SSTables.
+
{sstable-sideloader} can migrate unencrypted data, but you must use other strategies to move encrypted data, such as manually exporting and loading the data after the {sstable-sideloader} migration.

* *{astra-db} doesn't support materialized views*: You must replace these with SAI or an alternative data model design.

* *{sstable-sideloader} doesn't support secondary indexes*: If you don't remove or replace these in your origin cluster, then you must manually remove these directories from your snapshots, as explained in <<create-snapshots>>.

=== Administration server requirements

You need a server where you can run the {sstable-sideloader} commands.

Your administration server must have SSH access to each node in your origin cluster.

{company} recommends that you install the following additional software on your administration server:

* https://github.com/datastax/cassandra-data-migrator[{cass-short} Data Migrator (CDM)] to validate imported data and, in the context of Zero Downtime Migration, reconcile it with the origin cluster.
* https://jqlang.github.io/jq/[jq] to format JSON responses from the {astra-db} {devops-api}.
The {devops-api} commands in this guide use this tool.

[#create-snapshots]
== Create snapshots

On _each node_ in your origin cluster, use `nodetool snapshot` to create a backup of the data you want to migrate.

. Be aware of the {sstable-sideloader} limitations related to materialized views, secondary indexes, and encrypted data, which are described in <<origin-cluster-requirements>>.
If necessary, modify the data model on your origin cluster to prepare for the migration.

. Optional: Consider running `nodetool cleanup` before you create snapshots
+
The `xref:6.9@dse:managing:tools/nodetool/cleanup.adoc[nodetool cleanup]` command removes data that no longer belongs to a node.
It is particularly useful after adding more nodes to a cluster because it helps ensure that each node only contains the data that it is responsible for, according to the current cluster configuration and partitioning scheme.
+
If you run `nodetool cleanup` before you take a snapshot, you can ensure that the snapshot only includes relevant data, potentially reducing the size of the snapshot.
Smaller snapshots can lead to lower overall migration times and lower network transfer costs.
+
However, take adequate precautions before you run this command because the cleanup operations can introduce additional load on your origin cluster.

. Use `xref:6.9@dse:managing:tools/nodetool/snapshot.adoc[nodetool snapshot]` to create snapshots for the tables that you want to migrate.
+
Don't create snapshots of system tables or tables that you don't want to migrate.
The migration can fail if you attempt to migrate snapshots that don't have a matching schema in the target database.
{sstable-sideloader} ignores system keyspaces.
+
The structure of the `nodetool snapshot` command depends on the keyspaces and tables that you want to migrate.
+
[tabs]
======
All keyspaces::
+
--
Create a snapshot of all tables in all keyspaces:

[source,bash,subs="+quotes"]
----
nodetool snapshot -t *SNAPSHOT_NAME*
----

Replace *`SNAPSHOT_NAME`* with a descriptive name for the snapshot.
Use the same snapshot name on each node.
This makes it easier to programmatically upload the snapshots to the migration directory.

.Optional: Use a for loop to simplify snapshot creation
[%collapsible]
====
If the nodes in your origin cluster are named in a predictable way (for example, `dse0`, `dse1`, `dse2`, etc.), you can use a `for` loop to simplify snapshot creation.
For example:

[source,bash,subs="+quotes"]
----
for i in 0 1 2; do ssh dse${i} nodetool snapshot -t *SNAPSHOT_NAME*; done
----

You can use the same `for` loop to verify that each snapshot was successfully created:

[source,bash]
----
for i in 0 1 2; do ssh dse${i} nodetool listsnapshots; done
----
====
--

Specific keyspaces::
+
--
Create a snapshot of all tables in one or more keyspaces:

.Single keyspace
[source,bash,subs="+quotes"]
----
nodetool snapshot -t *SNAPSHOT_NAME* *KEYSPACE_NAME*
----

.Multiple keyspaces
[source,bash,subs="+quotes"]
----
nodetool snapshot -t *SNAPSHOT_NAME* *KEYSPACE_NAME_1* *KEYSPACE_NAME_2*
----

Replace the following:

* *`KEYSPACE_NAME`*: The name of the keyspace that contains the tables you want to migrate.
+
To include multiple keyspaces, list each keyspace separated by a space as shown in the example above.
* *`SNAPSHOT_NAME`*: A descriptive name for the snapshot.
+
Use the same snapshot name on each node.
This makes it easier to programmatically upload the snapshots to the migration directory.

.Optional: Use a for loop to simplify snapshot creation
[%collapsible]
====
If the nodes in your origin cluster are named in a predictable way (for example, `dse0`, `dse1`, `dse2`, etc.), you can use a `for` loop to simplify snapshot creation.
For example:

[source,bash,subs="+quotes"]
----
for i in 0 1 2; do ssh dse${i} nodetool snapshot -t *SNAPSHOT_NAME* *KEYSPACE_NAME*; done
----

To include multiple keyspaces in the snapshot, include multiple comma-separated `*KEYSPACE_NAME*` values, such as `keyspace1,keyspace2`.

You can use the same `for` loop to verify that each snapshot was successfully created:

[source,bash]
----
for i in 0 1 2; do ssh dse${i} nodetool listsnapshots; done
----
====
--

Specific tables::
+
--
Create a snapshot of specific tables within one or more keyspaces:

.Single table
[source,bash,subs="+quotes"]
----
nodetool snapshot -kt *KEYSPACE_NAME*.*TABLE_NAME* -t *SNAPSHOT_NAME*
----

.Multiple tables from one or more keyspaces
[source,bash,subs="+quotes"]
----
nodetool snapshot -kt *KEYSPACE_NAME_1*.*TABLE_NAME_A* *KEYSPACE_NAME_1*.*TABLE_NAME_B* *KEYSPACE_NAME_2*.*TABLE_NAME_X* -t *SNAPSHOT_NAME*
----

Replace the following:

* *`KEYSPACE_NAME`*: The name of the keyspace that contains the table you want to migrate.

* *`TABLE_NAME`*: The name of the table you want to migrate.
+
To include multiple tables from one or more keyspaces, list each *`KEYSPACE_NAME.TABLE_NAME`* pair separated by a space as shown in the example above.

* *`SNAPSHOT_NAME`*: A descriptive name for the snapshot.
+
Use the same snapshot name on each node.
This makes it easier to programmatically upload the snapshots to the migration directory.

.Optional: Use a for loop to simplify snapshot creation
[%collapsible]
====
If the nodes in your origin cluster are named in a predictable way (for example, `dse0`, `dse1`, `dse2`, etc.), you can use a `for` loop to simplify snapshot creation.
For example:

[source,bash,subs="+quotes"]
----
for i in 0 1 2; do ssh dse${i} nodetool snapshot -kt *KEYSPACE_NAME*.*TABLE_NAME* -t *SNAPSHOT_NAME*; done
----

To include multiple tables in the snapshot, include multiple comma-separated `*KEYSPACE_NAME*.*TABLE_NAME*` pairs, such as `keyspace1.table1,keyspace1.table2`.

You can use the same `for` loop to verify that each snapshot was successfully created:

[source,bash]
----
for i in 0 1 2; do ssh dse${i} nodetool listsnapshots; done
----
====
--
======

. Use `xref:6.9@dse:managing:tools/nodetool/list-snapshots.adoc[nodetool listsnapshots]` to verify that the snapshots were created:
+
[source,bash]
----
nodetool listsnapshots
----
+
Snapshots have a specific directory structure, such as `*KEYSPACE_NAME*/*TABLE_NAME*/snapshots/*SNAPSHOT_NAME*/...`.
The {sstable-sideloader} relies on this fixed structure to properly interpret the SSTable components.
**With the exception of secondary index directories (as explained in the following step), don't modify the snapshot's directory structure.**

. If your origin cluster has xref:dse-5.1@cql:develop:indexing/2i/2i-concepts.adoc[secondary indexes], remove all directories related to those indexes from all snapshots before you <<upload-snapshots-to-migration-directory,upload the snapshots>>.
+
[WARNING]
====
Secondary indexes defined in the origin cluster are ignored by {astra-db}, but they will cause the migration to fail.
To avoid errors, you must remove all secondary index directories from your snapshots before you upload them.
====
+
You can find secondary index directories in the table's snapshot directory:
+
[source,plaintext,subs="+quotes"]
----
**NODE_UUID**/**KEYSPACE_NAME**/**TABLE_NAME**-**TABLE_UUID**/snapshots/**SNAPSHOT_NAME**/.**INDEX_NAME**
----
+
For example, given the following table schema, the index directory would be located at `*NODE_UUID*/smart_home/sensor_readings-*TABLE_UUID*/snapshots/*SNAPSHOT_NAME*/.roomidx`:
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS smart_home.sensor_readings (
    device_id UUID,
    room_id UUID,
    reading_type TEXT,
    PRIMARY KEY ((device_id))
);
CREATE INDEX IF NOT EXISTS roomidx ON smart_home.sensor_readings(room_id);
----
//TODO: Troubleshooting - If you did not remove SI from your snapshots, you must contact {company} Support to remove these directories from your migration bucket.

[#record-schema]
== Configure the target database

To prepare your target database for the migration, you must record the schema for each table in your origin cluster that you want to migrate, recreate these schemas in your target database, and then set environment variables required to connect to your database.

[WARNING]
====
For the migration to succeed, your target database must meet the schema requirements described in this section.
Additionally, your snapshots must contain compatible data and directories, as described in <<origin-cluster-requirements>> and <<create-snapshots>>.
For example, {astra-db} doesn't support materialized views, and {sstable-sideloader} can't migrate encrypted data.

However, indexes don't need to match.
You can define indexes in your target database independently from the origin cluster because {sstable-sideloader} ignores SAI defined on the origin cluster.
During the migration, {sstable-sideloader} automatically populates any SAI defined in your target database, even if those SAI weren't present in your origin cluster.
//TODO: Difference between "indexes" and "SAI" here?
//You can define {astra-db}-supported indexes independently on the target database and they will populate as part of the data migration process.
====

. Get the following schema properties for _each table_ that you want to migrate:
+
* Exact keyspace name.
* Exact table name.
* Exact column names, data types, and the order in which they appear in the table creation DDL.
* Exact primary key definition as defined in your origin cluster, including the partition key, clustering columns, and ascending/descending ordering clauses.
You must define partition key columns and clustering columns in the exact order that they are defined on your origin cluster.
+
To retrieve schema properties, you can run the `xref:astra@cql:reference:cqlsh-commands/describe-keyspace.adoc[DESCRIBE KEYSPACE]` command on your origin cluster:
+
[source,cql,subs="+quotes"]
----
DESCRIBE *KEYSPACE_NAME*;
----
+
Replace *`KEYSPACE_NAME`* with the name of the keyspace that contains the tables you want to migrate,
such as `DESCRIBE smart_home;`.
+
Then, get the schema properties from the result:
+
[source,cql]
----
CREATE TABLE smart_home.sensor_readings (
    device_id UUID,
    room_id UUID,
    reading_type TEXT,
    reading_value DOUBLE,
    reading_timestamp TIMESTAMP,
    PRIMARY KEY (device_id, room_id, reading_timestamp)
) WITH CLUSTERING ORDER BY (room_id ASC, reading_timestamp DESC);
----
//However, {sstable-sideloader} can't import data to a xref:astra-db-serverless:databases:collection in a {db-serverless-vector} database.
. Recreate the schemas in your target database:
+
.. In the {astra-ui-link}, go to *Databases*, and then select your xref:astra-db-serverless:databases:create-database.adoc#create-a-serverless-non-vector-database[{db-serverless} database].
.. xref:astra-db-serverless:databases:manage-keyspaces.adoc#keyspaces[Create a keyspace] with the exact same name as your origin cluster's keyspace.
.. In your database's xref:astra-db-serverless:cql:develop-with-cql.adoc#connect-to-the-cql-shell[CQL console], create tables with the exact same names and schemas as your origin cluster.
+
image::sideloader:cql-console-create-identical-schema.png[]
+
{astra-db} rejects or ignores some table properties, such as compaction strategy.
See xref:astra-db-serverless:databases:database-limits.adoc[] for more information.
//TODO: Does this matter?

. In your terminal, set environment variables for your target database:
+
[source,bash,subs="+quotes"]
----
export dbID=*DATABASE_ID*
export token=*TOKEN*
----
+
Replace *`DATABASE_ID`* with the xref:astra-db-serverless:databases:create-database.adoc#get-db-id[database ID], and replace *`TOKEN`* with an xref:astra-db-serverless:administration:manage-application-tokens.adoc[application token] with the *Database Administrator* role.
+
[TIP]
====
Later, you will add another environment variable for the migration ID.

The curl commands in this guide assume that you have set environment variables for token, database ID, and migration ID.
Running the commands without these environment variables causes error messages like `<a href="/v2/databases/migrations/">Moved Permanently</a>` and `404 page not found`.

Additionally, the curl command use https://jqlang.github.io/jq/[jq] to format the JSON responses.
If you don't have jq installed, remove `| jq .` from the end of each command.
====

[#initialize-migration]
== Initialize the migration

Use the {devops-api} to initialize the migration and get your migration directory path and credentials.

The initialization process can take several minutes to complete, especially if the migration bucket doesn't already exist.
For more information about what happens during initialization, see <<sideloader-process>>.

. In your terminal, use the {devops-api} to initialize the data migration:
+
[source,curl]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/initialize \
    | jq .
----

. Get the `migrationID` from the response:
+
[source,json]
----
{
  "migrationID": "272eac1d-df8e-4d1b-a7c6-71d5af232182",
  "dbID": "b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d",
  "status": "Initializing",
  "progressInfo": "",
  "uploadBucketDir": "",
  "uploadCredentials": {
    "name": "",
    "keys": null,
    "credentialExpiration": null
  },
  "expectedCleanupTime": "2025-03-04T15:14:38Z"
}
----
+
The `migrationID` is a unique identifier (UUID) for the migration.
+
The response also includes the migration `status`.
You will refer to this status multiple times throughout the migration process.

. Assign the migration ID to an environment variable:
+
[source,bash,subs="+quotes"]
----
export migrationID=*MIGRATION_ID*
----
+
Replace *`MIGRATION_ID`* with the `migrationID` returned by the `initialize` endpoint.

. Check the migration status:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]

. Check the `status` field in the response:
+
* `"status": "ReceivingFiles"`: Initialization is complete and your upload credentials are available.
Proceed to the next step.
* `"status": "Initializing"`: The migration is still initializing.
Wait a few minutes before you check the status again.

. Get your migration directory path and upload credentials from the response.
You need these values to <<upload-snapshots-to-migration-directory,upload snapshots to the migration directory>>.
+
[tabs]
======
AWS::
+
--
.MigrationStatus with AWS credentials
[source,json]
----
{
  "migrationID": "272eac1d-df8e-4d1b-a7c6-71d5af232182",
  "dbID": "b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d",
  "status": "ReceivingFiles",
  "progressInfo": "",
  "uploadBucketDir": "s3://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/",
  "uploadCredentials": {
    "name": "sessionToken",
    "keys": {
      "accessKeyID": "ASXXXXXXXXXXXXXXXXXX",
      "secretAccessKey": "2XXXXXXXXXXXXXXXWqcdV519ZubYbyfuNxbZg1Rw",
      "sessionToken": "XXXXXXXXXX"
    },
    "credentialExpiration": "2024-01-18T19:45:09Z",
    "hint": "\nexport AWS_ACCESS_KEY_ID=ASXXXXXXXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=2XXXXXXXXXXXXXXXWqcdV519ZubYbyfuNxbZg1Rw\nexport AWS_SESSION_TOKEN=XXXXXXXXXXXXXX\n"
  },
  "expectedCleanupTime": "2024-01-25T15:14:38Z"
}
----

Securely store the `uploadBucketDir`, `accessKeyID`, `secretAccessKey`, and `sessionToken`:

* `uploadBucketDir` is the migration directory URL.
Note the trailing slash.

* `uploadCredentials` contains the AWS credentials that authorize uploads to the migration directory, namely `accessKeyID`, `secretAccessKey`, and `sessionToken`.

[IMPORTANT]
====
The `sessionToken` expires after one hour.
If your total migration takes longer than one hour, <<get-new-upload-credentials,generate new credentials>>, and then <<pause-resume-migration,resume the migration>> with the fresh credentials.

If you use automation to handle {sstable-sideloader} migrations, you might need to scheduled a <<pause-resume-migration,pause>> every hour so you can generate new credentials without unexpectedly interrupting the migration.
====
--

Google Cloud::
+
--
.MigrationStatus with Google Cloud credentials
[source,json]
----
{
  "migrationID": "272eac1d-df8e-4d1b-a7c6-71d5af232182",
  "dbID": "b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d",
  "status": "ReceivingFiles",
  "progressInfo": "",
  "uploadBucketDir": "gs://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/",
  "uploadCredentials": {
    "name": "TYPE_GOOGLE_CREDENTIALS_FILE",
    "keys": {
      "file": "CREDENTIALS_FILE"
    },
    "credentialExpiration": "2024-08-07T18:51:39Z"
  },
  "expectedCleanupTime": "2024-08-14T15:14:38Z"
}
----

.. Locate the `uploadBucketDir` and the `uploadCredentials` in the response:
+
* `uploadBucketDir` is the migration directory URL.
Note the trailing slash.
* `uploadCredentials` includes a base64-encoded file containing Google Cloud credentials that authorize uploads to the migration directory.

.. Pipe the Google Cloud credentials `file` to a `creds.json` file:
+
[source,bash]
----
curl -H "Authorization: Bearer " "$token" \
https://api.astra.datastax.com/v2/databases/"$dbID"/migrations/"$migrationID" \
| jq -r '.uploadCredentials.keys.file' | base64 -d > creds.json
----

.. Securely store the `uploadBucketDir` and `creds.json`.
--

Microsoft Azure::
+
--
[IMPORTANT]
====
{sstable-sideloader} for Microsoft Azure is available only to qualified participants in the private preview release.
Development is ongoing, and the features and functionality are subject to change.
This private preview is governed by your Agreement and the https://www.datastax.com/legal/previewterms[{company} Preview Terms].
====

.MigrationStatus with Azure credentials
[source,json]
----
{
  "migrationID": "456ca4a9-0551-46c4-b8bb-90fcd136a0c3",
  "dbID": "ccefd141-8fda-4e4d-a746-a102a96657bc",
  "status": "ReceivingFiles",
  "progressInfo": "",
  "uploadBucketDir": "https://muztx5cqmp3jhe3j2guebksz.blob.core.windows.net/mig-upload-456ca4a9-0551-46c4-b8bb-90fcd136a0c3/sstables/",
  "uploadCredentials": {
    "name": "URL signature",
    "keys": {
      "url": "https://UPLOAD_BUCKET_DIR/?si=AZURE_SAS_TOKEN",
      "urlSignature": "si=AZURE_SAS_TOKEN"
    },
    "credentialExpiration": "2025-04-02T15:14:31Z"
  },
  "expectedCleanupTime": "2025-03-04T15:14:38Z"
}
----
Securely store the `uploadBucketDir` and `urlSignature`:

* `uploadBucketDir` is the migration directory URL.
Note the trailing slash.

* `uploadCredentials` contains `url` and `urlSignature` keys that represent a https://learn.microsoft.com/en-us/azure/ai-services/translator/document-translation/how-to-guides/create-sas-tokens[Azure Shared Access Signature (SAS) token].
In the preceding example, the these strings are truncated for readability.
+
You need the `urlSignature` to upload snapshots to the migration directory.
--
======

[#get-new-upload-credentials]
=== Get new upload credentials

//TODO: Does checking the migration status always generate new creds or only if they are expired?

You can check the `MigrationStatus` at any time.
The command always returns the current status of the migration process and the cloud provider credentials required to upload to the migration directory.

If your credentials expire, do the following:

. Use the `MigrationStatus` endpoint to generate new credentials:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]

. Continue the migration with the fresh credentials.
+
If you set environment variables for your credentials, be sure to update those values.

[#upload-snapshots-to-migration-directory]
== Upload snapshots to the migration directory

//TODO: ENV VARS: A variable for MIGRATION_DIR would simplify these steps slightly. Env vars for all the values except the ones that change each time (Node name, snapshot name) would be most efficient.

Use your upload credentials to upload snapshots for _each origin node_ into the migration directory.

[IMPORTANT]
====
* Use the CLI that corresponds with your target database's cloud provider.
These commands assume that you installed the cloud provider's CLI on the nodes in your origin cluster.

* You must include the asterisk (`*`) character as shown in the commands, otherwise the commands won't work properly.

* With the exception of the leading `://` in the migration directory path, your paths must _not_ include double slashes (`//`).
====

[tabs]
======
AWS::
+
--
////
Originals:
[source,bash,subs="+quotes"]
----
export AWS_ACCESS_KEY_ID=**ACCESS_KEY_ID**; export AWS_SECRET_ACCESS_KEY=**SECRET_ACCESS_KEY**; export AWS_SESSION_TOKEN=**SESSION_TOKEN**; \
du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/\*/snapshots/***SNAPSHOT_NAME***; \
aws s3 sync --only-show-errors --exclude '\*' --include '*/snapshots/**SNAPSHOT_NAME***' **CASSANDRA_DATA_DIR**/ **MIGRATION_DIR**/**NODE_NAME**
----

[source,bash]
----
export AWS_ACCESS_KEY_ID=ASXXXXXXXXXXXXXXXXXX; export AWS_SECRET_ACCESS_KEY=2XXXXXXXXXXXXXXXWqcdV519ZubYbyfuNxbZg1Rw; AWS_SESSION_TOKEN=XXXXXXXXXX; \
du -sh /var/lib/cassandra/data/smart_home/*/snapshots/*sensor_readings*; \
aws s3 sync --only-show-errors --exclude '*' --include '*/snapshots/sensor_readings*' /var/lib/cassandra/data/ s3://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/dse0
----
////
. Set environment variables for the AWS credentials that were generated when you <<initialize-migration,initialized the migration>>:
+
[source,bash,subs="+quotes"]
----
export AWS_ACCESS_KEY_ID=**ACCESS_KEY_ID**
export AWS_SECRET_ACCESS_KEY=**SECRET_ACCESS_KEY**
export AWS_SESSION_TOKEN=**SESSION_TOKEN**
----

. Use the AWS CLI to upload one snapshot from one node into the migration directory:
+
[source,bash,subs="+quotes,attributes"]
----
du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}/snapshots/{asterisk}**SNAPSHOT_NAME**{asterisk}; \
aws s3 sync --only-show-errors --exclude '{asterisk}' --include '{asterisk}/snapshots/**SNAPSHOT_NAME**{asterisk}' **CASSANDRA_DATA_DIR**/ **MIGRATION_DIR****NODE_NAME**
----
+
Replace the following:
+
include::sideloader:partial$sideloader-partials.adoc[tags=command-placeholders-common]

+
.Example: Upload a snapshot with AWS CLI
[%collapsible]
====
[source,bash]
----
# Set environment variables
export AWS_ACCESS_KEY_ID=XXXXXXXX
export AWS_SECRET_ACCESS_KEY=XXXXXXXXXX
export AWS_SESSION_TOKEN=XXXXXXXXXX

# Upload "sensor_readings" snapshot from "dse0" node
du -sh /var/lib/cassandra/data/smart_home/*/snapshots/*sensor_readings*; \
aws s3 sync --only-show-errors --exclude '*' --include '*/snapshots/sensor_readings*' /var/lib/cassandra/data/ s3://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/dse0
----
====

. Monitor upload progress:
+
.. Use the AWS CLI to get a list of cloud storage keys for the files that have been successfully uploaded to the migration directory:
+
[source,bash,subs="+quotes"]
----
aws s3 ls --human-readable --summarize --recursive *MIGRATION_DIR*
----
+
Replace *`MIGRATION_DIR`* with the `uploadBucketDir` that was generated when you <<initialize-migration,initialized the migration>>.
+
.. Compare the returned list against the files in your snapshot directory.
When the lists match, the upload is complete.
+
You can _potentially_ increase upload speeds by adjusting the `max_concurrent_requests`, `multipart_threshold`, and `multipart_chunksize` parameters in your https://docs.aws.amazon.com/cli/latest/topic/s3-config.html[AWS CLI S3 configuration].
However, upload time primarily depends on the snapshot size and the network throughput from your origin cluster to the migration bucket.
Upload times are significantly lower when the origin cluster and the migration bucket are in the same cloud provider region.
For more information, see *Upload snapshots* in <<sideloader-process>>.

. Repeat the upload process for each snapshot (*`SNAPSHOT_NAME`*) and node (*`NODE_NAME`*) in your origin cluster.
+
If your credentials expire, see <<get-new-upload-credentials>>.
+
[TIP]
====
If the nodes in your origin cluster have predictable names (for example, `dse0`, `dse1`, and `dse2`), then you can use a `for` loop to streamline the execution of the upload commands.
For example:

[source,bash,subs="+quotes,attributes"]
----
# Set environment variables
export AWS_ACCESS_KEY_ID=**ACCESS_KEY_ID**
export AWS_SECRET_ACCESS_KEY=**SECRET_ACCESS_KEY**
export AWS_SESSION_TOKEN=**SESSION_TOKEN**

# Loop over the sync command for all nodes
for i in 0 1 2; do ssh dse{loop-var} \
"du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}/snapshots/{asterisk}**SNAPSHOT_NAME**{asterisk}; \
aws s3 sync --only-show-errors --exclude '{asterisk}' --include '{asterisk}/snapshots/**SNAPSHOT_NAME**{asterisk}' **CASSANDRA_DATA_DIR**/ **MIGRATION_DIR**dse{loop-var}" & done
----
====
--

Google Cloud::
+
--
. Authenticate to Google Cloud with the `creds.json` file that you created when you <<initialize-migration,initialized the migration>>:
+
[source,bash,subs="+quotes,attributes"]
----
gcloud auth activate-service-account --key-file=creds.json
----
+
If necessary, modify the `--key-file` path to match the location of your `creds.json` file, such as `--key-file=~/.gcloud_credentials/creds.json`.
+
You can also use `gcloud auth login --cred-file creds.json`.

. Use `gsutil` to upload one snapshot from one node into the migration directory:
+
[source,bash,subs="+quotes,attributes"]
----
gsutil -m rsync -r -d **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}{asterisk}/snapshots/**SNAPSHOT_NAME**/ **MIGRATION_DIR****NODE_NAME**/
----
+
Replace the following:
+
include::sideloader:partial$sideloader-partials.adoc[tags=command-placeholders-common]

+
.Example: Upload a snapshot with gcloud and gsutil
[%collapsible]
====
[source,bash,subs="attributes"]
----
# Authenticate
gcloud auth activate-service-account --key-file=creds.json

# Upload "sensor_readings" snapshot from "dse0" node
gsutil -m rsync -r -d /var/lib/cassandra/data/smart_home/{asterisk}{asterisk}/snapshots/sensor_readings/ gs://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/dse0
----
====

. Monitor upload progress:
+
.. Use `gsutil` to get a list of objects that have been successfully uploaded to the migration directory:
+
[source,bash,subs="+quotes"]
----
gsutil ls -r *MIGRATION_DIR*
----
+
Replace *`MIGRATION_DIR`* with the `uploadBucketDir` that was generated when you <<initialize-migration,initialized the migration>>.
+
.. Compare the returned list against the files in your snapshot directory.
When the lists match, the upload is complete.
+
The `https://cloud.google.com/storage/docs/gsutil/commands/rsync#description[-m]` flag in `gsutil -m rsync` enables parallel synchronization, which it can improve upload speed.
However, upload time primarily depends on the snapshot size and the network throughput from your origin cluster to the migration bucket.
Upload times are significantly lower when the origin cluster and the migration bucket are in the same cloud provider region.
For more information, see *Upload snapshots* in <<sideloader-process>>.

. Repeat the upload process for each snapshot (*`SNAPSHOT_NAME`*) and node (*`NODE_NAME`*) in your origin cluster.
+
[TIP]
====
If the nodes in your origin cluster have predictable names (for example, `dse0`, `dse1`, and `dse2`), then you can use a `for` loop to streamline the execution of the `gsutil rsync` commands.
For example:

[source,bash,subs="+quotes,attributes"]
----
for i in 0 1 2; do ssh dse{loop-var} \
du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}/snapshots/{asterisk}**SNAPSHOT_NAME**{asterisk}; \
gsutil -m rsync -r -d **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}{asterisk}/snapshots/**SNAPSHOT_NAME**/ **MIGRATION_DIR**dse{loop-var} & done
----
====
--

Microsoft Azure::
+
--
[IMPORTANT]
====
{sstable-sideloader} for Microsoft Azure is available only to qualified participants in the private preview release.
Development is ongoing, and the features and functionality are subject to change.
This private preview is governed by your Agreement and the https://www.datastax.com/legal/previewterms[{company} Preview Terms].
====
//----
//for dir in $(find "$CASSANDRA_DATA_DIR" -type d -path "*/snapshots/${SNAPSHOT_NAME}*"); do
//       REL_PATH=${dir#"$CASSANDRA_DATA_DIR"}  # Remove the base path
//       azcopy sync "$dir" "${MIGRATION_DIR}${NODE_NAME}/${REL_PATH}/"?${AZURE_SAS_TOKEN} --recursive
//     done
//   '
//----

. Set environment variables for the following values:
+
* *`AZURE_SAS_TOKEN`*: The `urlSignature` key that was generated when you <<initialize-migration,initialized the migration>>.
* *`CASSANDRA_DATA_DIR`*: The absolute file system path to where {cass-short} data is stored on the node, including the trailing slash.
For example, `/var/lib/cassandra/data/`.
* *`SNAPSHOT_NAME`*: The name of the xref:sideloader:sideloader-overview.adoc#create-snapshots[snapshot backup] that you created with `nodetool snapshot`.
* *`MIGRATION_DIR`*: The entire `uploadBucketDir` value that was generated when you <<initialize-migration,initialized the migration>>, including the trailing slash.
* *`NODE_NAME`*: The host name of the current node you are uploading the snapshot from.

+
[source,bash,subs="+quotes"]
----
export AZURE_SAS_TOKEN="**AZURE_CREDENTIALS_URL**"
export CASSANDRA_DATA_DIR="**CASSANDRA_DATA_DIR**"
export SNAPSHOT_NAME="**SNAPSHOT_NAME**"
export MIGRATION_DIR="**MIGRATION_DIR**"
export NODE_NAME="**NODE_NAME**"
----

. Use the Azure CLI to upload one snapshot from one node into the migration directory:
+
[source,bash]
----
for dir in $(find "$CASSANDRA_DATA_DIR" -type d -path "*/snapshots/${SNAPSHOT_NAME}*"); do
    REL_PATH="${dir#"$CASSANDRA_DATA_DIR"}"  # Remove the base path
    DEST_PATH="${MIGRATION_DIR}${NODE_NAME}/${REL_PATH}/?${AZURE_SAS_TOKEN}"

    azcopy sync "$dir" "$DEST_PATH" --recursive
done
----

. Monitor upload progress:
+
.. Use the Azure CLI to get the curent contents of the migration directory:
+
[source,bash]
----
azcopy list ${MIGRATION_DIR}?${AZURE_SAS_TOKEN}
----
+
.. Compare the returned list against the files in your snapshot directory.
When the lists match, the upload is complete.
+
Upload time primarily depends on the snapshot size and the network throughput from your origin cluster to the migration bucket.
Upload times are significantly lower when the origin cluster and the migration bucket are in the same cloud provider region.
For more information, see *Upload snapshots* in <<sideloader-process>>.

. Repeat the upload process for each snapshot and node in your origin cluster.
Be sure to change the `SNAPSHOT_NAME` and `NODE_NAME` environment variables as needed.
--
======

Uploaded snapshots are staged in the migration directory, but the data is not yet written to the target database.
After uploading snapshots, you must <<import-data,import the data>> to finish the migration.

=== Idle migration directories are evicted

As an added security measure, migrations that remain continuously idle for one week are subject to <<migration-cleanup,automatic cleanup>>, which deletes all associated snapshots, revokes any unexpired upload credentials, and then closes the migration.

You can upload snapshots into the migration directory to reset the idle timer.
However, this is not a reliable way to avoid automatic cleanup.

If you sporadically upload snapshots with large gaps of time in between uploads, you risk hitting the idle timeout, and your uploaded snapshots will be deleted before you can complete the migration.

For this reason, if you need to delay your migration, {company} recommends that you override automatic cleanup by <<schedule-a-cleanup,scheduling a cleanup>> to run at a date and time that better aligns with your overall migration plan.

[#import-data]
== Import data

After you upload snapshots for each origin node, import the data into your target database.

Data import is a multi-step operation that requires complete success.
If one step fails, then the entire import operation stops and the migration fails.
//Does all data fail to import or is it possible to have a partial import?

If necessary, you can <<pause-resume-migration,pause or abort the migration>> during the import process.

For more information about what happens during data import, see <<sideloader-process>>.

. Use the {devops-api} to launch the data import:
+
[source,curl]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/launch \
    | jq .
----
+
Although this call returns immediately, the import process takes time.

. Check the migration status periodically:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]

. Check the `status` field in the response:
+
* `"status": "ImportInProgress"`: The data is still being imported.
Wait a few minutes before you check the status again.
* `"status": "MigrationDone"`: The import is complete.
Proceed to <<validate-the-migrated-data>>.

[#validate-the-migrated-data]
== Validate the migrated data

include::sideloader:partial$sideloader-partials.adoc[tags=validate]

[#pause-resume-migration]
== Interrupt or abandon a migration

If necessary, you can use the {devops-api} to pause or cancel a {sstable-sideloader} migration.

=== Pause a migration

Use the {devops-api} to pause a migration:

[source,curl]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/pause \
    | jq .
----

A paused migration retains its current state and progress.

Any in-progress jobs will complete, but no new jobs will start.

=== Resume a migration

Resume a previously-paused migration from the point at which it was paused:

[source,curl]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/resume \
    | jq .
----

You can only resume an active migration that has been paused.
Running this command against migrations in other statuses, such as idle migrations that were automatically cleaned up, has no effect.

[#abort-migration]
=== Abort a migration

Abort a migration only if you want to abandon it completely.

. Abort a migration and remove all migration progress:
+
[source,curl]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/abort \
    | jq .
----
+
You can abort a migration up until the point at which {sstable-sideloader} starts importing SSTable metadata.
After this point, you must wait for the migration to finish, and then you can use the CQL shell to drop the keyspace/table in your target database before repeating the entire migration procedure.
For more information about what happens during each phase of a migration and the point of no return, see <<sideloader-process>>.

. Wait a few minutes, and then check the migration status:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]

[#migration-cleanup]
== Migration cleanup

A migration becomes idle if it is _not_ in `Initializing` or `ImportInProgress` status.
If a migration remains continuously idle for one week, it hits the idle timeout and triggers the automatic migration cleanup process.

The cleanup process deletes all SSTable snapshots from the migration directory, revokes any unexpired upload credentials, and then closes the migration.

Each migration ID has its own idle timeout, and the cleanup process deletes only the files and credentials associated with the migration ID that reached the idle timeout.
If one migration reaches the idle timeout, it doesn't affect other migrations associated with the same database.

A migration's idle timer resets each time you <<upload-snapshots-to-migration-directory,upload snapshots>> or <<import-data,import data>>.
Alternatively, you can you can override the idle timer by manually starting a cleanup or scheduling a cleanup.
However, _you cannot permanently postpone the cleanup process_.

=== Manually start a cleanup

. Use the {devops-api} to immediately start the cleanup process for a migration:
+
[source,curl]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/cleanup \
    | jq .
----
+
The cleanup process never runs on migrations in `ImportInProgress` status.
If the request fails due to `ImportInProgress`, you must either wait for the import process to end, <<abort-migration,abort the migration>>, or schedule the cleanup process for a later time.

. Wait a few minutes, and then check the migration status:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]
+
While the cleanup is running, the migration status is `CleaningUpFiles`.
When complete, the migration status is `Closed`.

=== Schedule a cleanup

You can use the {devops-api} to schedule a migration cleanup for a specific date and time.
This overrides the idle timeout for the specified migration ID only.

[source,curl,subs="+quotes"]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/cleanup \
    ?option.cleanupTime=**CLEANUP_TIME** \
    | jq .
----

Replace `*CLEANUP_TIME*` with the date and time that you want the cleanup process to run in https://en.wikipedia.org/wiki/ISO_8601[ISO 8601] format (`YYYY-MM-DDTHH:MM:SSZ`).
For example, `option.cleanupTime=2025-03-31T14:30Z`.

Setting a cleanup time in the past immediately starts the cleanup process.

The cleanup process never runs on migrations in `ImportInProgress` status.
If the migration is in `ImportInProgress` at the scheduled cleanup time, the cleanup process will start when the migration's status changes.