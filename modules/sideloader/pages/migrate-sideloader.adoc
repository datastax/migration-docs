= Migrate data with {sstable-sideloader}
:description: You can use {sstable-sideloader} to migrate data to {astra-db} from {cass-reg}, {dse}, or {hcd}.
:loop-var: pass:[${i}]

{description}

== Prerequisites

Before you use {sstable-sideloader} for a migration, xref:sideloader:sideloader-overview.adoc[learn about the {sstable-sideloader} process] and xref:sideloader:prepare-sideloader.adoc[prepare your environments for {sstable-sideloader}].

[#create-snapshots]
== Create snapshots

On _each node_ in your origin cluster, use `nodetool` to create a backup of the data that you want to migrate, including all keyspaces and CQL tables that you want to migrate.

. Be aware of the {sstable-sideloader} limitations related to materialized views, secondary indexes, and encrypted data that are described in xref:sideloader:prepare-sideloader.adoc#origin-cluster-requirements[Origin cluster requirements].
If necessary, modify the data model on your origin cluster to prepare for the migration.

. Optional: Before you create snapshots, consider running `xref:dse:managing:tools/nodetool/cleanup.adoc[nodetool cleanup]` to remove data that no longer belongs to your nodes.
This command is particularly useful after adding more nodes to a cluster because it helps ensure that each node only contains the data that it is responsible for, according to the current cluster configuration and partitioning scheme.
+
If you run `nodetool cleanup` before you take a snapshot, you can ensure that the snapshot only includes relevant data, potentially reducing the size of the snapshot.
Smaller snapshots can lead to lower overall migration times and lower network transfer costs.
+
However, take adequate precautions before you run this command because the cleanup operations can introduce additional load on your origin cluster.

. Use `xref:dse:managing:tools/nodetool/snapshot.adoc[nodetool snapshot]` to create snapshots for the tables that you want to migrate.
+
Don't create snapshots of system tables or tables that you don't want to migrate.
The migration can fail if you attempt to migrate snapshots that don't have a matching schema in the target database.
{sstable-sideloader} ignores system keyspaces.
+
The structure of the `nodetool snapshot` command depends on the keyspaces and tables that you want to migrate.
+
[tabs]
======
All keyspaces::
+
--
Create a snapshot of all tables in all keyspaces:

[source,bash,subs="+quotes"]
----
nodetool snapshot -t *SNAPSHOT_NAME*
----

Replace *`SNAPSHOT_NAME`* with a descriptive name for the snapshot.
Use the same snapshot name on each node.
This makes it easier to programmatically upload the snapshots to the migration directory.

.Optional: Use a for loop to simplify snapshot creation
[%collapsible]
====
If the nodes in your origin cluster are named in a predictable way (for example, `dse0`, `dse1`, `dse2`, etc.), you can use a `for` loop to simplify snapshot creation.
For example:

[source,bash,subs="+quotes"]
----
for i in 0 1 2; do ssh dse${i} nodetool snapshot -t *SNAPSHOT_NAME*; done
----

You can use the same `for` loop to verify that each snapshot was successfully created:

[source,bash]
----
for i in 0 1 2; do ssh dse${i} nodetool listsnapshots; done
----
====
--

Specific keyspaces::
+
--
Create a snapshot of all tables in one or more keyspaces:

.Single keyspace
[source,bash,subs="+quotes"]
----
nodetool snapshot -t *SNAPSHOT_NAME* *KEYSPACE_NAME*
----

.Multiple keyspaces
[source,bash,subs="+quotes"]
----
nodetool snapshot -t *SNAPSHOT_NAME* *KEYSPACE_NAME_1* *KEYSPACE_NAME_2*
----

Replace the following:

* *`KEYSPACE_NAME`*: The name of the keyspace that contains the tables you want to migrate.
+
To include multiple keyspaces, list each keyspace separated by a space as shown in the example above.
* *`SNAPSHOT_NAME`*: A descriptive name for the snapshot.
+
Use the same snapshot name on each node.
This makes it easier to programmatically upload the snapshots to the migration directory.

.Optional: Use a for loop to simplify snapshot creation
[%collapsible]
====
If the nodes in your origin cluster are named in a predictable way (for example, `dse0`, `dse1`, `dse2`, etc.), you can use a `for` loop to simplify snapshot creation.
For example:

[source,bash,subs="+quotes"]
----
for i in 0 1 2; do ssh dse${i} nodetool snapshot -t *SNAPSHOT_NAME* *KEYSPACE_NAME*; done
----

To include multiple keyspaces in the snapshot, include multiple comma-separated `*KEYSPACE_NAME*` values, such as `keyspace1,keyspace2`.

You can use the same `for` loop to verify that each snapshot was successfully created:

[source,bash]
----
for i in 0 1 2; do ssh dse${i} nodetool listsnapshots; done
----
====
--

Specific tables::
+
--
Create a snapshot of specific tables within one or more keyspaces:

.Single table
[source,bash,subs="+quotes"]
----
nodetool snapshot -kt *KEYSPACE_NAME*.*TABLE_NAME* -t *SNAPSHOT_NAME*
----

.Multiple tables from one or more keyspaces
[source,bash,subs="+quotes"]
----
nodetool snapshot -kt *KEYSPACE_NAME_1*.*TABLE_NAME_A* *KEYSPACE_NAME_1*.*TABLE_NAME_B* *KEYSPACE_NAME_2*.*TABLE_NAME_X* -t *SNAPSHOT_NAME*
----

Replace the following:

* *`KEYSPACE_NAME`*: The name of the keyspace that contains the table you want to migrate.

* *`TABLE_NAME`*: The name of the table you want to migrate.
+
To include multiple tables from one or more keyspaces, list each *`KEYSPACE_NAME.TABLE_NAME`* pair separated by a space as shown in the example above.

* *`SNAPSHOT_NAME`*: A descriptive name for the snapshot.
+
Use the same snapshot name on each node.
This makes it easier to programmatically upload the snapshots to the migration directory.

.Optional: Use a for loop to simplify snapshot creation
[%collapsible]
====
If the nodes in your origin cluster are named in a predictable way (for example, `dse0`, `dse1`, `dse2`, etc.), you can use a `for` loop to simplify snapshot creation.
For example:

[source,bash,subs="+quotes"]
----
for i in 0 1 2; do ssh dse${i} nodetool snapshot -kt *KEYSPACE_NAME*.*TABLE_NAME* -t *SNAPSHOT_NAME*; done
----

To include multiple tables in the snapshot, include multiple comma-separated `*KEYSPACE_NAME*.*TABLE_NAME*` pairs, such as `keyspace1.table1,keyspace1.table2`.

You can use the same `for` loop to verify that each snapshot was successfully created:

[source,bash]
----
for i in 0 1 2; do ssh dse${i} nodetool listsnapshots; done
----
====
--
======

. Use `xref:6.9@dse:managing:tools/nodetool/list-snapshots.adoc[nodetool listsnapshots]` to verify that the snapshots were created:
+
[source,bash]
----
nodetool listsnapshots
----
+
Snapshots have a specific directory structure, such as `*KEYSPACE_NAME*/*TABLE_NAME*/snapshots/*SNAPSHOT_NAME*/...`.
{sstable-sideloader} relies on this fixed structure to properly interpret the SSTable components.
**With the exception of secondary index directories (as explained in the following step), don't modify the snapshot's directory structure.**

. If your origin cluster has xref:dse-5.1@cql:develop:indexing/2i/2i-concepts.adoc[secondary indexes (2i)], remove all directories related to those indexes from all snapshots before you xref:sideloader:migrate-sideloader.adoc#upload-snapshots-to-migration-directory[upload the snapshots].
+
[WARNING]
====
Secondary indexes defined in the origin cluster are ignored by {astra-db}, but they will cause the migration to fail.
To avoid errors, you must remove all secondary index directories from your snapshots before you upload them.
====
+
You can find secondary index directories in the table's snapshot directory:
+
[source,plaintext,subs="+quotes"]
----
**NODE_UUID**/**KEYSPACE_NAME**/**TABLE_NAME**-**TABLE_UUID**/snapshots/**SNAPSHOT_NAME**/.**INDEX_NAME**
----
+
For example, given the following table schema, the index directory is found at `*NODE_UUID*/smart_home/sensor_readings-*TABLE_UUID*/snapshots/*SNAPSHOT_NAME*/.roomidx`:
+
[source,cql]
----
CREATE TABLE IF NOT EXISTS smart_home.sensor_readings (
    device_id UUID,
    room_id UUID,
    reading_type TEXT,
    PRIMARY KEY ((device_id))
);
CREATE INDEX IF NOT EXISTS roomidx ON smart_home.sensor_readings(room_id);
----

[#record-schema]
== Configure the target database

To prepare your target database for the migration, you must record the schema for each table in your origin cluster that you want to migrate, recreate these schemas in your target database, and then set environment variables required to connect to your database.

[WARNING]
====
For the migration to succeed, your target database must meet the schema requirements described in this section.
Additionally, your snapshots must contain compatible data and directories, as described in xref:sideloader:prepare-sideloader.adoc#origin-cluster-requirements[Origin cluster requirements] and xref:sideloader:migrate-sideloader.adoc#create-snapshots[Create snapshots].
For example, {astra-db} doesn't support materialized views, and {sstable-sideloader} can't migrate encrypted data.

However, indexes don't need to match.
You can define indexes in your target database independently from the origin cluster because {sstable-sideloader} ignores Storage Attached Indexes (SAI) defined on the origin cluster.
During the migration, {sstable-sideloader} automatically populates any SAI defined in your target database, even if those SAI weren't present in your origin cluster.
//TODO: Difference between "indexes" and "SAI" here?
//You can define {astra-db}-supported indexes independently on the target database and they will populate as part of the data migration process.
====

. Get the following schema properties for _each table_ that you want to migrate:
+
* Exact keyspace name.
* Exact table name.
* Exact column names, data types, and the order in which they appear in the table creation DDL.
* Exact primary key definition as defined in your origin cluster, including the partition key, clustering columns, and ascending/descending ordering clauses.
You must define partition key columns and clustering columns in the exact order that they are defined on your origin cluster.
+
To retrieve schema properties, you can run the `xref:astra@cql:reference:cqlsh-commands/describe-keyspace.adoc[DESCRIBE KEYSPACE]` command on your origin cluster:
+
[source,cql,subs="+quotes"]
----
DESCRIBE *KEYSPACE_NAME*;
----
+
Replace *`KEYSPACE_NAME`* with the name of the keyspace that contains the tables you want to migrate,
such as `DESCRIBE smart_home;`.
+
Then, get the schema properties from the result:
+
[source,cql]
----
CREATE TABLE smart_home.sensor_readings (
    device_id UUID,
    room_id UUID,
    reading_type TEXT,
    reading_value DOUBLE,
    reading_timestamp TIMESTAMP,
    PRIMARY KEY (device_id, room_id, reading_timestamp)
) WITH CLUSTERING ORDER BY (room_id ASC, reading_timestamp DESC);
----
//However, {sstable-sideloader} can't import data to a xref:astra-db-serverless:databases:collection in a {db-serverless-vector} database.
. Recreate the schemas in your target database:
+
.. In the {astra-ui-link} navigation menu, click *Databases*, and then click the name of your {astra-db} database.
.. xref:astra-db-serverless:databases:manage-keyspaces.adoc#keyspaces[Create a keyspace] with the exact same name as your origin cluster's keyspace.
.. In your database's xref:astra-db-serverless:cql:develop-with-cql.adoc#connect-to-the-cql-shell[CQL console], create tables with the exact same names and schemas as your origin cluster.
+
image::sideloader:cql-console-create-identical-schema.png[]
+
{astra-db} rejects or ignores some table properties, such as compaction strategy.
See xref:astra-db-serverless:databases:database-limits.adoc[] for more information.
//TODO: Does this matter?

. In your terminal, set environment variables for your target database:
+
[source,bash,subs="+quotes"]
----
export dbID=*DATABASE_ID*
export token=*TOKEN*
----
+
Replace *`DATABASE_ID`* with the xref:astra-db-serverless:databases:create-database.adoc#get-db-id[database ID], and replace *`TOKEN`* with an xref:astra-db-serverless:administration:manage-application-tokens.adoc[application token] with the *Database Administrator* role.
+
[TIP]
====
Later, you will add another environment variable for the migration ID.

The curl commands in this guide assume that you have set environment variables for token, database ID, and migration ID.
Running the commands without these environment variables causes error messages like `<a href="/v2/databases/migrations/">Moved Permanently</a>` and `404 page not found`.

Additionally, the curl command use https://jqlang.github.io/jq/[jq] to format the JSON responses.
If you don't have jq installed, remove `| jq .` from the end of each command.
====

[#initialize-migration]
== Initialize the migration

Use the {devops-api} to initialize the migration and get your migration directory path and credentials.

.What happens during initialization?
[%collapsible]
====
include::sideloader:partial$sideloader-partials.adoc[tags=initialize]
====

The initialization process can take several minutes to complete, especially if the migration bucket doesn't already exist.

. In your terminal, use the {devops-api} to initialize the data migration:
+
[source,bash]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/initialize \
    | jq .
----

. Get the `migrationID` from the response:
+
[source,json]
----
{
  "migrationID": "272eac1d-df8e-4d1b-a7c6-71d5af232182",
  "dbID": "b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d",
  "status": "Initializing",
  "progressInfo": "",
  "uploadBucketDir": "",
  "uploadCredentials": {
    "name": "",
    "keys": null,
    "credentialExpiration": null
  },
  "expectedCleanupTime": "2025-03-04T15:14:38Z"
}
----
+
The `migrationID` is a unique identifier (UUID) for the migration.
+
The response also includes the migration `status`.
You will refer to this status multiple times throughout the migration process.

. Assign the migration ID to an environment variable:
+
[source,bash,subs="+quotes"]
----
export migrationID=*MIGRATION_ID*
----
+
Replace *`MIGRATION_ID`* with the `migrationID` returned by the `initialize` endpoint.

. Check the migration status:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]

. Check the `status` field in the response:
+
* `"status": "ReceivingFiles"`: Initialization is complete and your upload credentials are available.
Proceed to the next step.
* `"status": "Initializing"`: The migration is still initializing.
Wait a few minutes before you check the status again.

. Get your migration directory path and upload credentials from the response.
You need these values to xref:sideloader:migrate-sideloader.adoc#upload-snapshots-to-migration-directory[upload snapshots to the migration directory].
+
[tabs]
======
AWS::
+
--
.MigrationStatus with AWS credentials
[source,json]
----
{
  "migrationID": "272eac1d-df8e-4d1b-a7c6-71d5af232182",
  "dbID": "b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d",
  "status": "ReceivingFiles",
  "progressInfo": "",
  "uploadBucketDir": "s3://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/",
  "uploadCredentials": {
    "name": "sessionToken",
    "keys": {
      "accessKeyID": "ASXXXXXXXXXXXXXXXXXX",
      "secretAccessKey": "2XXXXXXXXXXXXXXXWqcdV519ZubYbyfuNxbZg1Rw",
      "sessionToken": "XXXXXXXXXX"
    },
    "credentialExpiration": "2024-01-18T19:45:09Z",
    "hint": "\nexport AWS_ACCESS_KEY_ID=ASXXXXXXXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=2XXXXXXXXXXXXXXXWqcdV519ZubYbyfuNxbZg1Rw\nexport AWS_SESSION_TOKEN=XXXXXXXXXXXXXX\n"
  },
  "expectedCleanupTime": "2024-01-25T15:14:38Z"
}
----

Securely store the `uploadBucketDir`, `accessKeyID`, `secretAccessKey`, and `sessionToken`:

* `uploadBucketDir` is the migration directory URL.
Note the trailing slash.

* `uploadCredentials` contains the AWS credentials that authorize uploads to the migration directory, namely `accessKeyID`, `secretAccessKey`, and `sessionToken`.

[IMPORTANT]
====
The `sessionToken` expires after one hour.
If your total migration takes longer than one hour, xref:sideloader:troubleshoot-sideloader.adoc#get-new-upload-credentials[generate new credentials], and then xref:sideloader:stop-restart-sideloader.adoc[resume the migration] with the fresh credentials.

If you use automation to handle {sstable-sideloader} migrations, you might need to script a xref:sideloader:stop-restart-sideloader.adoc[pause] every hour so you can generate new credentials without unexpectedly interrupting the migration.
====
--

Google Cloud::
+
--
.MigrationStatus with Google Cloud credentials
[source,json]
----
{
  "migrationID": "272eac1d-df8e-4d1b-a7c6-71d5af232182",
  "dbID": "b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d",
  "status": "ReceivingFiles",
  "progressInfo": "",
  "uploadBucketDir": "gs://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/",
  "uploadCredentials": {
    "name": "TYPE_GOOGLE_CREDENTIALS_FILE",
    "keys": {
      "file": "CREDENTIALS_FILE"
    },
    "credentialExpiration": "2024-08-07T18:51:39Z"
  },
  "expectedCleanupTime": "2024-08-14T15:14:38Z"
}
----

.. Find the `uploadBucketDir` and the `uploadCredentials` in the response:
+
* `uploadBucketDir` is the migration directory URL.
Note the trailing slash.
* `uploadCredentials` includes a base64-encoded file containing Google Cloud credentials that authorize uploads to the migration directory.

.. Pipe the Google Cloud credentials `file` to a `creds.json` file:
+
[source,bash]
----
curl -X GET \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID} \
    | jq -r '.uploadCredentials.keys.file' \
    | base64 -d > creds.json
----

.. Securely store the `uploadBucketDir` and `creds.json`.
--

Microsoft Azure::
+
--
.MigrationStatus with Azure credentials
[source,json]
----
{
  "migrationID": "456ca4a9-0551-46c4-b8bb-90fcd136a0c3",
  "dbID": "ccefd141-8fda-4e4d-a746-a102a96657bc",
  "status": "ReceivingFiles",
  "progressInfo": "",
  "uploadBucketDir": "https://muztx5cqmp3jhe3j2guebksz.blob.core.windows.net/mig-upload-456ca4a9-0551-46c4-b8bb-90fcd136a0c3/sstables/",
  "uploadCredentials": {
    "name": "URL signature",
    "keys": {
      "url": "https://UPLOAD_BUCKET_DIR/?si=AZURE_SAS_TOKEN",
      "urlSignature": "si=AZURE_SAS_TOKEN"
    },
    "credentialExpiration": "2025-04-02T15:14:31Z"
  },
  "expectedCleanupTime": "2025-03-04T15:14:38Z"
}
----
Securely store the `uploadBucketDir` and `urlSignature`:

* `uploadBucketDir` is the migration directory URL.
Note the trailing slash.

* `uploadCredentials` contains `url` and `urlSignature` keys that represent an https://learn.microsoft.com/en-us/azure/ai-services/translator/document-translation/how-to-guides/create-sas-tokens[Azure Shared Access Signature (SAS) token].
In the preceding example, these strings are truncated for readability.
+
You need the `urlSignature` to upload snapshots to the migration directory.
--
======

[#upload-snapshots-to-migration-directory]
== Upload snapshots to the migration directory

//TODO: ENV VARS: A variable for MIGRATION_DIR would simplify these steps slightly. Env vars for all the values except the ones that change each time (Node name, snapshot name) would be most efficient.

Use your cloud provider's CLI and your upload credentials to upload snapshots for _each origin node_ into the migration directory.

[IMPORTANT]
====
Be aware of the following requirements for the upload commands:

* You must include the asterisk (`*`) character as shown in the commands, otherwise the commands won't work properly.

* With the exception of the leading `://` in the migration directory path, your paths must _not_ include double slashes (`//`).

* Use the CLI that corresponds with your target database's cloud provider.
For more information, see xref:sideloader:prepare-sideloader.adoc[].

* These commands assume that you installed the cloud provider's CLI on the nodes in your origin cluster.
For more information, see xref:sideloader:prepare-sideloader.adoc[].

* You might need to modify these commands depending on your environment, node names, directory structures, and other variables.
====

[tabs]
======
AWS::
+
--
////
Originals:
[source,bash,subs="+quotes"]
----
export AWS_ACCESS_KEY_ID=**ACCESS_KEY_ID**; export AWS_SECRET_ACCESS_KEY=**SECRET_ACCESS_KEY**; export AWS_SESSION_TOKEN=**SESSION_TOKEN**; \
du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/\*/snapshots/***SNAPSHOT_NAME***; \
aws s3 sync --only-show-errors --exclude '\*' --include '*/snapshots/**SNAPSHOT_NAME***' **CASSANDRA_DATA_DIR**/ **MIGRATION_DIR**/**NODE_NAME**
----

[source,bash]
----
export AWS_ACCESS_KEY_ID=ASXXXXXXXXXXXXXXXXXX; export AWS_SECRET_ACCESS_KEY=2XXXXXXXXXXXXXXXWqcdV519ZubYbyfuNxbZg1Rw; AWS_SESSION_TOKEN=XXXXXXXXXX; \
du -sh /var/lib/cassandra/data/smart_home/*/snapshots/*sensor_readings*; \
aws s3 sync --only-show-errors --exclude '*' --include '*/snapshots/sensor_readings*' /var/lib/cassandra/data/ s3://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/dse0
----
////
. Set environment variables for the AWS credentials that were generated when you xref:sideloader:migrate-sideloader.adoc#initialize-migration[initialized the migration]:
+
[source,bash,subs="+quotes"]
----
export AWS_ACCESS_KEY_ID=**ACCESS_KEY_ID**
export AWS_SECRET_ACCESS_KEY=**SECRET_ACCESS_KEY**
export AWS_SESSION_TOKEN=**SESSION_TOKEN**
----

. Use the AWS CLI to upload one snapshot from one node into the migration directory:
+
[source,bash,subs="+quotes,attributes"]
----
du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}/snapshots/{asterisk}**SNAPSHOT_NAME**{asterisk}; \
aws s3 sync --only-show-errors --exclude '{asterisk}' --include '{asterisk}/snapshots/**SNAPSHOT_NAME**{asterisk}' **CASSANDRA_DATA_DIR**/ **MIGRATION_DIR****NODE_NAME**
----
+
Replace the following:
+
include::sideloader:partial$sideloader-partials.adoc[tags=command-placeholders-common]

+
.Example: Upload a snapshot with AWS CLI
[%collapsible]
====
[source,bash]
----
# Set environment variables
export AWS_ACCESS_KEY_ID=XXXXXXXX
export AWS_SECRET_ACCESS_KEY=XXXXXXXXXX
export AWS_SESSION_TOKEN=XXXXXXXXXX

# Upload "sensor_readings" snapshot from "dse0" node
du -sh /var/lib/cassandra/data/smart_home/*/snapshots/*sensor_readings*; \
aws s3 sync --only-show-errors --exclude '*' --include '*/snapshots/sensor_readings*' /var/lib/cassandra/data/ s3://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/dse0
----
====

. Monitor upload progress:
+
.. Use the AWS CLI to get a list of cloud storage keys for the files that have been successfully uploaded to the migration directory:
+
[source,bash,subs="+quotes"]
----
aws s3 ls --human-readable --summarize --recursive *MIGRATION_DIR*
----
+
Replace *`MIGRATION_DIR`* with the `uploadBucketDir` that was generated when you xref:sideloader:migrate-sideloader.adoc#initialize-migration[initialized the migration].
+
.. Compare the returned list against the files in your snapshot directory.
When the lists match, the upload is complete.
+
You can _potentially_ increase upload speeds by adjusting the `max_concurrent_requests`, `multipart_threshold`, and `multipart_chunksize` parameters in your https://docs.aws.amazon.com/cli/latest/topic/s3-config.html[AWS CLI S3 configuration].
However, upload time primarily depends on the snapshot size, network throughput from your origin cluster to the migration bucket, and whether the origin cluster and migration bucket are in the same region.

. Repeat the upload process for each snapshot (*`SNAPSHOT_NAME`*) and node (*`NODE_NAME`*) in your origin cluster.
+
If your credentials expire, see xref:sideloader:troubleshoot-sideloader.adoc#get-new-upload-credentials[Get new upload credentials].

.Optional: Use a for loop to simplify snapshot uploads
[%collapsible]
====
If the nodes in your origin cluster have predictable names (for example, `dse0`, `dse1`, and `dse2`), then you can use a `for` loop to streamline the execution of the upload commands.
For example:

[source,bash,subs="+quotes,attributes"]
----
# Set environment variables
export AWS_ACCESS_KEY_ID=**ACCESS_KEY_ID**
export AWS_SECRET_ACCESS_KEY=**SECRET_ACCESS_KEY**
export AWS_SESSION_TOKEN=**SESSION_TOKEN**

# Loop over the sync command for all nodes
for i in 0 1 2; do ssh dse{loop-var} \
"du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}/snapshots/{asterisk}**SNAPSHOT_NAME**{asterisk}; \
aws s3 sync --only-show-errors --exclude '{asterisk}' --include '{asterisk}/snapshots/**SNAPSHOT_NAME**{asterisk}' **CASSANDRA_DATA_DIR**/ **MIGRATION_DIR**dse{loop-var}" & done
----
====
--

Google Cloud::
+
--
. Authenticate to Google Cloud with the `creds.json` file that you created when you xref:sideloader:migrate-sideloader.adoc#initialize-migration[initialized the migration]:
+
[source,bash,subs="+quotes,attributes"]
----
gcloud auth activate-service-account --key-file=creds.json
----
+
If necessary, modify the `--key-file` path to match the location of your `creds.json` file, such as `--key-file=~/.gcloud_credentials/creds.json`.
+
You can also use `gcloud auth login --cred-file creds.json`.

. Use `gsutil` to upload one snapshot from one node into the migration directory:
+
[source,bash,subs="+quotes,attributes"]
----
gsutil -m rsync -r -d **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}{asterisk}/snapshots/**SNAPSHOT_NAME**/ **MIGRATION_DIR****NODE_NAME**/
----
+
Replace the following:
+
include::sideloader:partial$sideloader-partials.adoc[tags=command-placeholders-common]

+
.Example: Upload a snapshot with gcloud and gsutil
[%collapsible]
====
[source,bash,subs="attributes"]
----
# Authenticate
gcloud auth activate-service-account --key-file=creds.json

# Upload "sensor_readings" snapshot from "dse0" node
gsutil -m rsync -r -d /var/lib/cassandra/data/smart_home/{asterisk}{asterisk}/snapshots/sensor_readings/ gs://ds-mig-b7e7761f-6f7f-4116-81a5-e8eefcf0cc1d/272eac1d-df8e-4d1b-a7c6-71d5af232182/sstables/dse0
----
====

. Monitor upload progress:
+
.. Use `gsutil` to get a list of objects that have been successfully uploaded to the migration directory:
+
[source,bash,subs="+quotes"]
----
gsutil ls -r *MIGRATION_DIR*
----
+
Replace *`MIGRATION_DIR`* with the `uploadBucketDir` that was generated when you xref:sideloader:migrate-sideloader.adoc#initialize-migration[initialized the migration].
+
.. Compare the returned list against the files in your snapshot directory.
When the lists match, the upload is complete.
+
The `https://cloud.google.com/storage/docs/gsutil/commands/rsync#description[-m]` flag in `gsutil -m rsync` enables parallel synchronization, which can improve upload speed.
However, upload time primarily depends on the snapshot size, network throughput from your origin cluster to the migration bucket, and whether the origin cluster and migration bucket are in the same region.

. Repeat the upload process for each snapshot (*`SNAPSHOT_NAME`*) and node (*`NODE_NAME`*) in your origin cluster.

.Optional: Use a for loop to simplify snapshot uploads
[%collapsible]
====
If the nodes in your origin cluster have predictable names (for example, `dse0`, `dse1`, and `dse2`), then you can use a `for` loop to streamline the execution of the `gsutil rsync` commands.
For example:

[source,bash,subs="+quotes,attributes"]
----
for i in 0 1 2; do ssh dse{loop-var} \
du -sh **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}/snapshots/{asterisk}**SNAPSHOT_NAME**{asterisk}; \
gsutil -m rsync -r -d **CASSANDRA_DATA_DIR**/**KEYSPACE_NAME**/{asterisk}{asterisk}/snapshots/**SNAPSHOT_NAME**/ **MIGRATION_DIR**dse{loop-var} & done
----
====
--

Microsoft Azure::
+
--
//----
//for dir in $(find "$CASSANDRA_DATA_DIR" -type d -path "*/snapshots/${SNAPSHOT_NAME}*"); do
//       REL_PATH=${dir#"$CASSANDRA_DATA_DIR"}  # Remove the base path
//       azcopy sync "$dir" "${MIGRATION_DIR}${NODE_NAME}/${REL_PATH}/"?${AZURE_SAS_TOKEN} --recursive
//     done
//   '
//----

. Set environment variables for the following values:
+
* *`AZURE_SAS_TOKEN`*: The `urlSignature` key that was generated when you xref:sideloader:migrate-sideloader.adoc#initialize-migration[initialized the migration].
* *`CASSANDRA_DATA_DIR`*: The absolute file system path to where {cass-short} data is stored on the node, including the trailing slash.
For example, `/var/lib/cassandra/data/`.
* *`SNAPSHOT_NAME`*: The name of the xref:sideloader:migrate-sideloader.adoc#create-snapshots[snapshot backup] that you created with `nodetool snapshot`.
* *`MIGRATION_DIR`*: The entire `uploadBucketDir` value that was generated when you xref:sideloader:migrate-sideloader.adoc#initialize-migration[initialized the migration], including the trailing slash.
* *`NODE_NAME`*: The host name of the current node you are uploading the snapshot from.

+
[source,bash,subs="+quotes"]
----
export AZURE_SAS_TOKEN="**AZURE_CREDENTIALS_URL**"
export CASSANDRA_DATA_DIR="**CASSANDRA_DATA_DIR**"
export SNAPSHOT_NAME="**SNAPSHOT_NAME**"
export MIGRATION_DIR="**MIGRATION_DIR**"
export NODE_NAME="**NODE_NAME**"
----

. Use the Azure CLI to upload one snapshot from one node into the migration directory:
+
[source,bash]
----
for dir in $(find "$CASSANDRA_DATA_DIR" -type d -path "*/snapshots/${SNAPSHOT_NAME}*"); do
    REL_PATH="${dir#"$CASSANDRA_DATA_DIR"}"  # Remove the base path
    DEST_PATH="${MIGRATION_DIR}${NODE_NAME}/${REL_PATH}/?${AZURE_SAS_TOKEN}"

    azcopy sync "$dir" "$DEST_PATH" --recursive
done
----

. Monitor upload progress:
+
.. Use the Azure CLI to get the curent contents of the migration directory:
+
[source,bash]
----
azcopy list ${MIGRATION_DIR}?${AZURE_SAS_TOKEN}
----
+
.. Compare the returned list against the files in your snapshot directory.
When the lists match, the upload is complete.
+
Upload time primarily depends on the snapshot size, network throughput from your origin cluster to the migration bucket, and whether the origin cluster and migration bucket are in the same region.

. Repeat the upload process for each snapshot and node in your origin cluster.
Be sure to change the `SNAPSHOT_NAME` and `NODE_NAME` environment variables as needed.
--
======

Uploaded snapshots are staged in the migration directory, but the data is not yet written to the target database.
After uploading snapshots, you must xref:sideloader:migrate-sideloader.adoc#import-data[import the data] to finish the migration.

=== Idle migration directories are evicted

As an added security measure, migrations that remain continuously idle for one week are subject to xref:sideloader:cleanup-sideloader.adoc[automatic cleanup], which deletes all associated snapshots, revokes any unexpired upload credentials, and then closes the migration.

{company} recommends that you xref:sideloader:cleanup-sideloader.adoc#reschedule-a-cleanup[manually reschedule the cleanup] if you don't plan to launch the migration within one week or if you need several days to upload snapshots or import data.

[WARNING]
====
For large migrations, it can take several days to upload snapshots and import data.
Make sure you xref:sideloader:cleanup-sideloader.adoc#reschedule-a-cleanup[manually reschedule the cleanup] to avoid automatic cleanup.
====

[#import-data]
== Import data

After you upload snapshots for each origin node, import the data into your target database.

Data import is a multi-step operation that requires complete success.
If one step fails, then the entire import operation stops and the migration fails.
//Does all data fail to import or is it possible to have a partial import?

.What happens during data import?
[%collapsible]
======
include::sideloader:partial$sideloader-partials.adoc[tags=import]
======

[TIP]
====
If necessary, you can xref:sideloader:stop-restart-sideloader.adoc[pause or abort the migration] during the import process.

include::sideloader:partial$sideloader-partials.adoc[tags=no-return]
====

. Use the {devops-api} to launch the data import:
+
[source,bash]
----
curl -X POST \
    -H "Authorization: Bearer ${token}" \
    https://api.astra.datastax.com/v2/databases/${dbID}/migrations/${migrationID}/launch \
    | jq .
----
+
Although this call returns immediately, the import process takes time.

. Check the migration status periodically:
+
include::sideloader:partial$sideloader-partials.adoc[tags=check-status]

. Check the `status` field in the response:
+
* `"status": "ImportInProgress"`: The data is still being imported.
Wait a few minutes before you check the status again.
* `"status": "MigrationDone"`: The import is complete, and you can proceed to <<validate-the-migrated-data>>.

. If the migration takes more than a few days, xref:sideloader:cleanup-sideloader.adoc#reschedule-a-cleanup[manually reschedule the cleanup] to avoid automatic cleanup.

. If the migration fails, see xref:sideloader:troubleshoot-sideloader.adoc[].

[#validate-the-migrated-data]
== Validate the migrated data

include::sideloader:partial$sideloader-partials.adoc[tags=validate]

== See also

* xref:sideloader:cleanup-sideloader.adoc[]
* xref:sideloader:troubleshoot-sideloader.adoc[]