= Troubleshooting tips
:page-tag: migration,zdm,zero-downtime,zdm-proxy,troubleshooting
:page-aliases: ROOT:troubleshooting.adoc
:description: Get help with {product}.

This page provides general troubleshooting advice and describes some common issues you might encounter with {product}.

For specific error messages, see xref:troubleshooting-scenarios.adoc[].

You can also contact your {company} account representative or {support-url}[{company} Support], if you have a https://www.datastax.com/products/luna[Luna service contract].

[#proxy-logs]
== {product-proxy} logs

The {product-proxy} logs can help you troubleshoot issues with {product}.

=== Set the {product-proxy} log level

Set the {product-proxy} log level to print the messages that you need.

The default log level is `INFO`, which is adequate for most logging.

If you need more detail for temporary troubleshooting, you can set the log level to `DEBUG`.
However, this can slightly degrade performance, and {company} recommends that you revert to `INFO` logging as soon as possible.

How you set the log level depends on how you deployed the {product-proxy}:

* If you used {product-automation} to deploy the {product-proxy}, set `log_level` in `vars/zdm_proxy_core_config.yml`.
+
You can change this value in a rolling fashion by editing the variable and running the `rolling_update_zdm_proxy.yml` playbook.
For more information, see xref:manage-proxy-instances.adoc#change-mutable-config-variable[Change a mutable configuration variable].

* If you didn't use {product-automation} to deploy the {product-proxy}, set the `ZDM_LOG_LEVEL` environment variable on each proxy instance, and then restart each instance.

=== Retrieve the {product-proxy} log files

//TODO: Reconcile with manage-proxy-instance.adoc content.

If you used the {product-automation} to deploy {product-proxy}, then you can get logs for a single proxy instance, and you can use a playbook to retrieve logs for all instances.
For instructions and more information, see xref:ROOT:manage-proxy-instances.adoc#access-the-proxy-logs[Access the proxy logs].

If you did not use the {product-automation} to deploy {product-proxy}, you might have to access the logs another way.
For example, if you used Docker, you can use the following command to export a container's logs to a `log.txt` file:

[source,bash]
----
docker logs my-container > log.txt
----

Keep in mind that Docker logs are deleted if the container is recreated.

=== Message levels

Some log messages contain text that sounds like an error, but they are not errors.
The message's `level` typically indicates severity:

* `level=debug` and `level=info`: Expected and normal messages that are typically not errors.
However, if you enable `DEBUG` logging, `debug` messages can help you find the source of a problem.

* `level=warn`: Reports an event that wasn't fatal to the overall process, but could indicate an issue with an individual request or connection.

* `level=error`: Indicates an issue with the {product-proxy}, client application, or clusters.
These messages require further examination.

If the meaning of a `warn` or `error` message isn't clear, you can submit an issue in the {product-proxy-repo}/issues[{product-proxy} GitHub repository].

=== Common log messages

Here are the most common messages in the {product-proxy} logs.

==== {product-proxy} startup message

If the log level doesn't filter out `info` entries, you can look for a `Proxy started` log message to verify that the {product-proxy} started correctly.
For example:

[source,json]
----
{"log":"time=\"2023-01-13T11:50:48Z\" level=info
msg=\"Proxy started. Waiting for SIGINT/SIGTERM to shutdown.
\"\n","stream":"stderr","time":"2023-01-13T11:50:48.522097083Z"}
----

==== {product-proxy} configuration message

If the log level doesn't filter out `info` entries, the first few lines of a {product-proxy} log file contain all configuration variables and values in a long JSON string.

For example, this log message has been truncated for readability:

[source,json]
----
{"log":"time=\"2023-01-13T11:50:48Z\" level=info
msg=\"Parsed configuration: {\\\"ProxyIndex\\\":1,\\\"ProxyAddresses\\\":"...",
...TRUNCATED...
","stream":"stderr","time":"2023-01-13T11:50:48.339225051Z"}
----

Configuration settings can help with troubleshooting.

To make this message easier to read, pass it through a JSON formatter or paste it into a text editor that can reformat JSON.

==== Protocol log messages

There are cases where protocol errors are fatal, and they will kill an active connection that was being used to serve requests.
However, it is also possible to get normal protocol log messages that contain wording that sounds like an error.

For example, the following `DEBUG` message contains the phrases `force a downgrade` and `unsupported protocol version`, which can sound like errors:

[source,json]
----
{"log":"time=\"2023-01-13T12:02:12Z\" level=debug msg=\"[TARGET-CONNECTOR]
Protocol v5 detected while decoding a frame. Returning a protocol message
to the client to force a downgrade: PROTOCOL (code=Code Protocol [0x0000000A],
msg=Invalid or unsupported protocol version (5)).\"\n","stream":"stderr","time":"2023-01-13T12:02:12.379287735Z"}
----

However, `level=debug` indicates that this is not an error.
Instead, this is a normal part of protocol version negotiation (handshake) during connection initialization.

[#check-version]
== Check your {product-proxy} version

//TODO: Possibly duplicated on manage-proxy-instances.html#_upgrade_the_proxy_version
In the {product-proxy} logs, the first message contains the version string:

[source,console]
----
time="2023-01-13T13:37:28+01:00" level=info msg="Starting ZDM proxy version 2.1.0"
time="2023-01-13T13:37:28+01:00" level=info msg="Parsed configuration: ..."
----

This message is logged immediately before the long `Parsed configuration` string.

You can also pass the `-version` flag to the {product-proxy} to print the version.
For example, you can use the following Docker command:

[source,bash]
----
docker run --rm datastax/zdm-proxy:2.x -version
ZDM proxy version 2.1.0
----

[IMPORTANT]
====
Don't use `--rm` when you launch the {product-proxy} container.
This flag will prevent you from accessing the logs when {product-proxy} stops or crashes.
====

== Query system.peers and system.local to check for {product-proxy} configuration issues

Querying `system.peers` and `system.local` can help you investigate {product-proxy} configuration issues:

. xref:ROOT:connect-clients-to-proxy.adoc#connecting-cqlsh-to-the-zdm-proxy[Connect CQL shell to a {product-proxy} instance.]

. Query `system.peers`:
+
[source,cql]
----
SELECT * FROM system.peers
----

. Query `system.local`:
+
[source,cql]
----
SELECT * FROM system.local
----

. Repeat for each of your {product-proxy} instances.
+
Because `system.peers` and `system.local` reflect the local {product-proxy} instance's configuration, you need to query all instances to get all information and identify potential misconfigurations.

. Inspect the results for values related to an error that you are troubleshooting, such as IP addresses or tokens.
+
For example, you might compare `cluster_name` to ensure that all instances are connected to the same cluster, rather than mixing contact points from different clusters.

== Report an issue

To report an issue or get additional support, submit an issue in the {product-short} component GitHub repositories:

* {product-proxy-repo}/issues[{product-proxy} repository]
* {product-automation-repo}/issues[{product-automation} repository] (includes {product-automation} and the {product-utility})
* {cass-migrator-repo}/issues[{cass-migrator} repository]
* {dsbulk-migrator-repo}/issues[{dsbulk-migrator} repository]

[IMPORTANT]
====
These repositories are public.

Don't include any proprietary or private information in issues, pull requests, or comments that you make in these repositories.
====

In the issue description, include as much of the following information as possible, and make sure to remove all proprietary and private information before submitting the issue:

* Your <<check-version,{product-proxy} version>>.

* <<proxy-logs,{product-proxy} logs>>, ideally at `DEBUG` level, if you can easily reproduce the issue and tolerate restarting the proxy instances to apply the log level configuration change.

* Database deployment type ({dse-short}, {hcd-short}, {cass-short}, or {astra-db}) and version for the origin and target clusters.
The version isn't required for {astra-db}.

* Screenshots of the xref:ROOT:metrics.adoc[{product-proxy} metrics] dashboards from Grafana or your chosen visualization tool.
+
Direct read access to your metrics dashboard is preferred, if permitted by your security policy.
This is particularly helpful for performance-related issues.

* Client application and driver logs.

* The driver language and version that the client application is using.

For performance-related issues, provide the following additional information:

* Which statement types (simple, prepared, batch) do you use?

* If you use batch statements:
+
** Which driver API do you use to create these batches?
** Are you passing a `BEGIN BATCH` CQL query string to a simple/prepared statement, or do you use the actual batch statement objects that the drivers allow you to create?

* How many parameters does each statement have?

* Is CQL function replacement enabled?
This feature is disabled by default.
To determine if this feature is enabled, check the following variables:
+
** If you use {product-automation}, check the Ansible advanced configuration variable `replace_cql_functions`.
** If you don't use {product-automation}, check the environment variable `ZDM_REPLACE_CQL_FUNCTIONS`.

== See also

* xref:ROOT:troubleshooting-scenarios.adoc[]
* xref:ROOT:metrics.adoc[]