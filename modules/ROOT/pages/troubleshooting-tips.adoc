= Troubleshooting tips
:page-aliases: ROOT:troubleshooting.adoc
:description: Get help with {product}.
:page-aliases: ROOT:troubleshooting-scenarios.adoc

This page provides general troubleshooting advice and describes some common issues you might encounter with {product}.

For additional assistance, you can <<report-an-issue>>, contact your {company} account representative, or contact {support-url}[{company} Support].

[#proxy-logs]
== {product-proxy} logs

{product-proxy} logs can help you troubleshoot issues with {product}.

=== Set the {product-proxy} log level

Set the {product-proxy} log level to print the messages that you need.

The default log level is `INFO`, which is adequate for most logging.

If you need more detail for temporary troubleshooting, you can set the log level to `DEBUG`.
However, this can slightly degrade performance, and {company} recommends that you revert to `INFO` logging as soon as possible.

How you set the log level depends on how you deployed {product-proxy}:

* If you used {product-automation} to deploy {product-proxy}, set `log_level` in `vars/zdm_proxy_core_config.yml`.
+
You can change this value in a rolling fashion by editing the variable and running the `rolling_update_zdm_proxy.yml` playbook.
For more information, see xref:manage-proxy-instances.adoc#change-mutable-config-variable[Change a mutable configuration variable].

* If you didn't use {product-automation} to deploy {product-proxy}, set the `ZDM_LOG_LEVEL` environment variable on each proxy instance and then restart each instance.

=== Retrieve the {product-proxy} log files

//TODO: Reconcile with manage-proxy-instance.adoc content.

If you used {product-automation} to deploy {product-proxy}, then you can get logs for a single proxy instance, and you can use a playbook to retrieve logs for all instances.
For instructions and more information, see xref:ROOT:manage-proxy-instances.adoc#access-the-proxy-logs[Access the proxy logs].

If you did not use {product-automation} to deploy {product-proxy}, you might have to access the logs another way.
For example, if you used Docker, you can use the following command to export a container's logs to a `log.txt` file:

[source,bash]
----
docker logs my-container > log.txt
----

Keep in mind that Docker logs are deleted if the container is recreated.

=== Message levels

Some log messages contain text that sounds like an error, but they are not errors.
The message's `level` typically indicates severity:

* `level=debug` and `level=info`: Expected and normal messages that are typically not errors.
However, if you enable `DEBUG` logging, `debug` messages can help you find the source of a problem.

* `level=warn`: Reports an event that wasn't fatal to the overall process, but could indicate an issue with an individual request or connection.

* `level=error`: Indicates an issue with {product-proxy}, the client application, or the clusters.
These messages require further examination.

If the meaning of a `warn` or `error` message isn't clear, you can submit an issue in the {product-proxy-repo}/issues[{product-proxy} GitHub repository].

=== Common log messages

Here are the most common messages in the {product-proxy} logs.

==== {product-proxy} startup message

If the log level doesn't filter out `info` entries, you can look for a `Proxy started` log message to verify that {product-proxy} started correctly.
For example:

[source,json]
----
{"log":"time=\"2023-01-13T11:50:48Z\" level=info
msg=\"Proxy started. Waiting for SIGINT/SIGTERM to shutdown.
\"\n","stream":"stderr","time":"2023-01-13T11:50:48.522097083Z"}
----

==== {product-proxy} configuration message

If the log level doesn't filter out `info` entries, the first few lines of a {product-proxy} log file contain all configuration variables and values in a long JSON string.

For example, this log message has been truncated for readability:

[source,json]
----
{"log":"time=\"2023-01-13T11:50:48Z\" level=info
msg=\"Parsed configuration: {\\\"ProxyIndex\\\":1,\\\"ProxyAddresses\\\":"...",
...TRUNCATED...
","stream":"stderr","time":"2023-01-13T11:50:48.339225051Z"}
----

Configuration settings can help with troubleshooting.

To make this message easier to read, pass it through a JSON formatter or paste it into a text editor that can reformat JSON.

==== Protocol log messages

There are cases where protocol errors are fatal, and they will kill an active connection that was being used to serve requests.
However, it is also possible to get normal protocol log messages that contain wording that sounds like an error.

For example, the following `DEBUG` message contains the phrases `force a downgrade` and `unsupported protocol version`, which can sound like errors:

[source,json]
----
{"log":"time=\"2023-01-13T12:02:12Z\" level=debug msg=\"[TARGET-CONNECTOR]
Protocol v5 detected while decoding a frame. Returning a protocol message
to the client to force a downgrade: PROTOCOL (code=Code Protocol [0x0000000A],
msg=Invalid or unsupported protocol version (5)).\"\n","stream":"stderr","time":"2023-01-13T12:02:12.379287735Z"}
----

However, `level=debug` indicates that this is not an error.
Instead, this is a normal part of protocol version negotiation (handshake) during connection initialization.

[#check-version]
== Check your {product-proxy} version

//TODO: Possibly duplicated on manage-proxy-instances.html#_upgrade_the_proxy_version
In the {product-proxy} logs, the first message contains the version string:

[source,console]
----
time="2023-01-13T13:37:28+01:00" level=info msg="Starting ZDM proxy version 2.1.0"
time="2023-01-13T13:37:28+01:00" level=info msg="Parsed configuration: ..."
----

This message is logged immediately before the long `Parsed configuration` string.

You can also pass the `-version` flag to {product-proxy} to print the version.
For example, you can use the following Docker command:

[source,bash]
----
docker run --rm datastax/zdm-proxy:2.x -version
ZDM proxy version 2.1.0
----

[IMPORTANT]
====
Don't use `--rm` when you launch the {product-proxy} container.
This flag will prevent you from accessing the logs when {product-proxy} stops or crashes.
====

== Query system.peers and system.local to check for {product-proxy} configuration issues

Querying `system.peers` and `system.local` can help you investigate {product-proxy} configuration issues:

. xref:ROOT:connect-clients-to-proxy.adoc#connect-the-cql-shell-to-zdm-proxy[Connect cqlsh to a {product-proxy} instance].

. Query `system.peers`:
+
[source,cql]
----
SELECT * FROM system.peers
----

. Query `system.local`:
+
[source,cql]
----
SELECT * FROM system.local
----

. Repeat for each of your {product-proxy} instances.
+
Because `system.peers` and `system.local` reflect the local {product-proxy} instance's configuration, you need to query all instances to get all information and identify potential misconfigurations.

. Inspect the results for values related to an error that you are troubleshooting, such as IP addresses or tokens.
+
For example, you might compare `cluster_name` to ensure that all instances are connected to the same cluster, rather than mixing contact points from different clusters.

== Troubleshooting scenarios

//TODO: use same format as driver troubleshooting.
//TODO: Remove or hide issues that have been resolved by a later release.

This page provides troubleshooting advice for specific issues or error messages related to {product}.

Each section includes symptoms, causes, and suggested solutions or workarounds.

=== Configuration changes are not being applied by the automation

==== Symptoms

You changed the values of some configuration variables in the automation and then rolled them out using the `rolling_update_zdm_proxy.yml` playbook, but these changes are not taking effect on your {product-proxy} instances.

==== Cause

The {product-proxy} configuration comprises a number of variables, but only a subset of these can be changed on an existing deployment in a rolling fashion.
The variables that can be changed with a rolling update are listed xref:manage-proxy-instances.adoc#change-mutable-config-variable[here].

All other configuration variables excluded from the list above are considered immutable and can only be changed by a redeployment.
This is by design: immutable configuration variables should not be changed after finalizing the deployment prior to starting the migration, so allowing them to be changed through a rolling update would risk accidentally propagating some misconfiguration that could compromise the deployment's integrity.

==== Solution or Workaround

To change the value of configuration variables that are considered immutable, simply run the `deploy_zdm_proxy.yml` playbook again.
This playbook can be run as many times as necessary and will just recreate the entire {product-proxy} deployment from scratch with the provided configuration.
This doesn't happen in a rolling fashion: the existing {product-proxy} instances are torn down all at the same time prior to being recreated, resulting in a brief window in which the whole {product-proxy} deployment will become unavailable.


=== Unsupported protocol version error on the client application

==== Symptoms

In the logs for the Java driver 4.x series, the following issues can manifest during session initialization, or after initialization.

[source,log]
----
[s0|/10.169.241.224:9042] Fatal error while initializing pool, forcing the node down (UnsupportedProtocolVersionException: [/10.169.241.224:9042] Host does not support protocol version DSE_V2)

[s0|/10.169.241.24:9042] Fatal error while initializing pool, forcing the node down (UnsupportedProtocolVersionException: [/10.169.241.24:9042] Host does not support protocol version DSE_V2)

[s0|/10.169.241.251:9042] Fatal error while initializing pool, forcing the node down (UnsupportedProtocolVersionException: [/10.169.241.251:9042] Host does not support protocol version DSE_V2)

[s0] Failed to connect with protocol DSE_V1, retrying with V4

[s0] Failed to connect with protocol DSE_V2, retrying with DSE_V1
----

==== Cause

https://datastax-oss.atlassian.net/browse/JAVA-2905[JAVA-2905] is a driver bug that manifests itself in this way. It affects Java driver 4.x, and was fixed on the 4.10.0 release.

==== Solution or Workaround

If you are using spring boot and/or spring-data-cassandra then an upgrade of these dependencies will be necessary to a version that has the java driver fix.

Alternatively, you can force the protocol version on the driver to the max supported version by both clusters.
V4 is a good recommendation that usually fits all but if the user is migrating from {dse-short} to {dse-short} then DSE_V1 should be used for {dse-short} 5.x and DSE_V2 should be used for {dse-short} 6.x.

To force the protocol version on the Java driver, see the documentation for your version of the Java driver:

* https://apache.github.io/cassandra-java-driver/4.19.0/core/native_protocol/?h=controlling#controlling-the-protocol-version[{cass-reg} Java driver 4.18 and later: Controlling the protocol version]
* https://docs.datastax.com/en/developer/java-driver/latest/manual/core/native_protocol/index.html#controlling-the-protocol-version[{company} Java driver 4.17 and earlier: Controlling the protocol version]

=== Protocol errors in the proxy logs but clients can connect successfully

==== Symptoms

{product-proxy} logs contain:

[source,log]
----
{"log":"time=\"2022-10-01T12:02:12Z\" level=debug msg=\"[TARGET-CONNECTOR] Protocol v5 detected while decoding a frame.
Returning a protocol error to the client to force a downgrade: ERROR PROTOCOL ERROR (code=ErrorCode ProtocolError [0x0000000A],
msg=Invalid or unsupported protocol version (5)).\"\n","stream":"stderr","time":"2022-07-20T12:02:12.379287735Z"}
----

==== Cause

Protocol errors like these are a normal part of the handshake process where the protocol version is being negotiated.
These protocol version downgrades happen when either {product-proxy} or at least one of the clusters doesn't support the version requested by the client.

V5 downgrades are enforced by {product-proxy} but any other downgrade is requested by one of the clusters when they don't support the version that the client requested.
The proxy supports V3, V4, DSE_V1 and DSE_V2.

==== Solution or Workaround

These log messages are informative only (log level `DEBUG`).

If you find one of these messages with a higher log level (especially `level=error`) then there might be a bug.
At that point the issue will need to be investigated by the {product-short} team.
This log message with a log level of `ERROR` means that the protocol error occurred after the handshake, and this is a fatal unexpected error that results in a disconnect for that particular connection.

=== Error during proxy startup: `Invalid or unsupported protocol version: 3`

If the {product-proxy} logs contain the following type of output, it indicates that one of the origin clusters doesn't support at least V3 (e.g. {cass-short} 2.0, {dse-short} 4.6), and {product-short} cannot be used for that migration.

[source,log]
----
time="2022-10-01T19:58:15+01:00" level=info msg="Starting proxy..."
time="2022-10-01T19:58:15+01:00" level=info msg="Parsed Topology Config: TopologyConfig{VirtualizationEnabled=false, Addresses=[127.0.0.1], Count=1, Index=0, NumTokens=8}"
time="2022-10-01T19:58:15+01:00" level=info msg="Parsed Origin contact points: [127.0.0.1]"
time="2022-10-01T19:58:15+01:00" level=info msg="Parsed Target contact points: [127.0.0.1]"
time="2022-10-01T19:58:15+01:00" level=info msg="TLS was not configured for Origin"
time="2022-10-01T19:58:15+01:00" level=info msg="TLS was not configured for Target"
time="2022-10-01T19:58:15+01:00" level=info msg="[openTCPConnection] Opening connection to 127.0.0.1:9042"
time="2022-10-01T19:58:15+01:00" level=info msg="[openTCPConnection] Successfully established connection with 127.0.0.1:9042"
time="2022-10-01T19:58:15+01:00" level=debug msg="performing handshake"
time="2022-10-01T19:58:15+01:00" level=error msg="cqlConn{conn: 127.0.0.1:9042}: handshake failed: expected AUTHENTICATE or READY, got ERROR PROTOCOL ERROR (code=ErrorCode ProtocolError [0x0000000A], msg=Invalid or unsupported protocol version: 3)"
time="2022-10-01T19:58:15+01:00" level=warning msg="Error while initializing a new cql connection for the control connection of ORIGIN: failed to perform handshake: expected AUTHENTICATE or READY, got ERROR PROTOCOL ERROR (code=ErrorCode ProtocolError [0x0000000A], msg=Invalid or unsupported protocol version: 3)"
time="2022-10-01T19:58:15+01:00" level=debug msg="Shutting down request loop on cqlConn{conn: 127.0.0.1:9042}"
time="2022-10-01T19:58:15+01:00" level=debug msg="Shutting down response loop on cqlConn{conn: 127.0.0.1:9042}."
time="2022-10-01T19:58:15+01:00" level=debug msg="Shutting down event loop on cqlConn{conn: 127.0.0.1:9042}."
time="2022-10-01T19:58:15+01:00" level=error msg="Couldn't start proxy: failed to initialize origin control connection: could not open control connection to ORIGIN, tried endpoints: [127.0.0.1:9042]."
time="2022-10-01T19:58:15+01:00" level=info msg="Initiating proxy shutdown..."
time="2022-10-01T19:58:15+01:00" level=debug msg="Requesting shutdown of the client listener..."
time="2022-10-01T19:58:15+01:00" level=debug msg="Requesting shutdown of the client handlers..."
time="2022-10-01T19:58:15+01:00" level=debug msg="Waiting until all client handlers are done..."
time="2022-10-01T19:58:15+01:00" level=debug msg="Requesting shutdown of the control connections..."
time="2022-10-01T19:58:15+01:00" level=debug msg="Waiting until control connections done..."
time="2022-10-01T19:58:15+01:00" level=debug msg="Shutting down the schedulers and metrics handler..."
time="2022-10-01T19:58:15+01:00" level=info msg="Proxy shutdown complete."
time="2022-10-01T19:58:15+01:00" level=error msg="Couldn't start proxy, retrying in 2.229151525s: failed to initialize origin control connection: could not open control connection to ORIGIN, tried endpoints: [127.0.0.1:9042]."
----

The control connections of {product-proxy} don't perform protocol version negotiation, they only attempt to use protocol version 3.

=== Authentication errors

Authentication errors indicate that credentials are incorrect or have insufficient permissions.

There are three sets of credentials used with {product-proxy}:

* Target: credentials that you set in the proxy configuration through the `ZDM_TARGET_USERNAME` and `ZDM_TARGET_PASSWORD` settings.

* Origin: credentials that you set in the proxy configuration through the `ZDM_ORIGIN_USERNAME` and `ZDM_ORIGIN_PASSWORD` settings.

* Client: credentials that the client application sends to the proxy during the connection handshake, these are set in the application configuration, not the proxy configuration.

Authentication errors mean that at least one of these three sets of credentials is incorrect or has insufficient permissions.

If the authentication error is preventing the proxy from starting then it's either the origin or target credentials that are incorrect or have insufficient permissions.
The log message shows whether it is the origin or target handshake that is failing.

If the proxy is able to start up, and you can see the following message in the logs: `Proxy started. Waiting for SIGINT/SIGTERM to shutdown`, then the authentication error is happening when a client application tries to open a connection to the proxy.
In this case, the issue is with the client credentials.
The application itself is using invalid credentials, such as an incorrect username/password, expired token, or insufficient permissions.

Note that the proxy startup message has log level `INFO`, so if the configured log level on the proxy is `warning` or `error`, you must rely on other ways to know whether {product-proxy} started correctly.
You can check if the docker container is running (or process if docker isn't being used) or if there is a log message similar to `Error launching proxy`.

=== {product-proxy} listens on a custom port, and all applications are able to connect to one proxy instance only

==== Symptoms

{product-proxy} is listening on a custom port (not 9042) and:

* The Grafana dashboard shows only one proxy instance receiving all the connections from the application.
* Only one proxy instance has log messages such as `level=info msg="Accepted connection from 10.4.77.210:39458"`.

==== Cause

The application is specifying the custom port as part of the contact points using the format
`<proxy_ip_address>:<proxy_custom_port>`.

For example, using the Java driver, if the {product-proxy} instances were listening on port 14035, this would look like:

`.addContactPoints("172.18.10.36:14035", "172.18.11.48:14035", "172.18.12.61:14035")`

The contact point is used as the first point of contact to the cluster, but the driver discovers the rest of the nodes via CQL queries.
However, this discovery process doesn't discover the ports, just the addresses so the driver uses the addresses it discovers with the port that is configured at startup.

As a result, port 14035 will only be used for the contact point initially discovered, while for all other nodes the driver will attempt to use the default 9042 port.

==== Solution or Workaround

In the application, ensure that the custom port is explicitly indicated using the `.withPort(<customPort>)` API. In the above example:

[source,java]
----
.addContactPoints("172.18.10.36", "172.18.11.48", "172.18.12.61")
.withPort(14035)
----


=== Syntax error "no viable alternative at input 'CALL'" in proxy logs

==== Symptoms

{product-proxy} logs contain:

[source,log]
----
{"log":"time=\"2022-10-01T13:10:47Z\" level=debug msg=\"Recording TARGET-CONNECTOR other error:
ERROR SYNTAX ERROR (code=ErrorCode SyntaxError [0x00002000], msg=line 1:0 no viable alternative
at input 'CALL' ([CALL]...))\"\n","stream":"stderr","time":"2022-07-20T13:10:47.322882877Z"}
----

==== Cause

The log message indicates that the server doesn't recognize the word “CALL” in the query string which most likely means that it is an RPC (remote procedure call).
From the proxy logs alone, it is not possible to see what method is being called by the query but it's very likely the RPC that the drivers use to send {dse-short} Insights data to the server.

Most {company}-compatible drivers have {dse-short} Insights reporting enabled by default when they detect a server version that supports it (regardless of whether the feature is enabled on the server side or not).
The driver might also have it enabled for {astra-db} depending on what server version {astra-db} is returning for queries involving the `system.local` and `system.peers` tables.

==== Solution or Workaround

These log messages are harmless, but if you need to remove them, you can disable {dse-short} Insights in the driver configuration.
For example, in the Java driver, you can set `https://github.com/apache/cassandra-java-driver/blob/4.x/core/src/main/resources/reference.conf#L1365[advanced.monitor-reporting]` to `false`.

=== Default Grafana credentials don't work

==== Symptoms

Consider a case where you deploy the metrics component of our {product-automation}, a Grafana instance is deployed but you cannot login using the usual default `admin/admin` credentials.

==== Cause

{product-automation} specifies a custom set of credentials instead of relying on the `admin/admin` ones that are typically the default for Grafana deployments.

==== Solution or Workaround

Check the credentials that are being used by looking up the `vars/zdm_monitoring_config.yml` file on the {product-automation} directory.
These credentials can also be modified before deploying the metrics stack.

=== Proxy starts but client cannot connect (connection timeout/closed)

==== Symptoms

{product-proxy} log contains:

[source]
----
INFO[0000] [openTCPConnection] Opening connection to 10.0.63.163:9042
INFO[0000] [openTCPConnection] Successfully established connection with 10.0.63.163:9042
INFO[0000] [openTLSConnection] Opening TLS connection to 10.0.63.163:9042 using underlying TCP connection
INFO[0000] [openTLSConnection] Successfully established connection with 10.0.63.163:9042
INFO[0000] Successfully opened control connection to ORIGIN using endpoint 10.0.63.163:9042.
INFO[0000] [openTCPConnection] Opening connection to 5bc479c2-c3d0-45be-bfba-25388f2caff7-us-east-1.db.astra.datastax.com:29042
INFO[0000] [openTCPConnection] Successfully established connection with 54.84.75.118:29042
INFO[0000] [openTLSConnection] Opening TLS connection to 211d66bf-de8d-48ac-a25b-bd57d504bd7c using underlying TCP connection
INFO[0000] [openTLSConnection] Successfully established connection with 211d66bf-de8d-48ac-a25b-bd57d504bd7
INFO[0000] Successfully opened control connection to TARGET using endpoint 5bc479c2-c3d0-45be-bfba-25388f2caff7-us-east-1.db.astra.datastax.com:29042-211d66bf-de8d-48ac-a25b-bd57d504bd7c.
INFO[0000] Proxy connected and ready to accept queries on 0.0.0.0:9042
INFO[0000] Proxy started. Waiting for SIGINT/SIGTERM to shutdown.
INFO[0043] Accepted connection from 10.0.62.255:33808
INFO[0043] [ORIGIN-CONNECTOR] Opening request connection to ORIGIN (10.0.63.20:9042).
ERRO[0043] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 100ms...
ERRO[0043] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 200ms...
ERRO[0043] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 400ms...
ERRO[0043] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 800ms...
ERRO[0044] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 1.6s...
ERRO[0046] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 3.2s...
ERRO[0049] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 6.4s...
ERRO[0056] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 10s...
ERRO[0066] [openTCPConnectionWithBackoff] Couldn't connect to 10.0.63.20:9042, retrying in 10s...
ERRO[0076] Client Handler could not be created: ORIGIN-CONNECTOR context timed out or cancelled while opening connection to ORIGIN: context deadline exceeded
----

==== Cause

{product-proxy} has connectivity only to a subset of the nodes.

The control connection (during {product-proxy} startup) cycles through the nodes until it finds one that can be connected to.
For client connections, each proxy instance cycles through its "assigned nodes" only.
_(The "assigned nodes" are a different subset of the cluster nodes for each proxy instance, generally non-overlapping between proxy instances so as to avoid any interference with the load balancing already in place at client-side driver level.
The assigned nodes are not necessarily contact points: even discovered nodes undergo assignment to proxy instances.)_

In the example above, {product-proxy} doesn't have connectivity to 10.0.63.20, which was chosen as the origin node for the incoming client connection, but it connected to 10.0.63.163 during startup.

==== Solution or Workaround

Ensure that network connectivity exists and is stable between the {product-proxy} instances and all {cass-short} / {dse-short} nodes of the local datacenter.

=== Client application driver takes too long to reconnect to a proxy instance

==== Symptoms

After a {product-proxy} instance has been unavailable for some time and it gets back up, the client application takes too long to reconnect.

There should never be a reason to stop a {product-proxy} instance other than a configuration change, but maybe the proxy crashed or the user tried to do a configuration change and took a long time to get the {product-proxy} instance back up.

==== Cause

{product-proxy} does not send topology events to the client applications, so the reconnection policy determines the time required for the driver to reconnect to a {product-proxy} instance.

==== Solution or Workaround

Restart the client application to force an immediate reconnect.

If you expect {product-proxy} instances to go down frequently, change the reconnection policy on the driver so that the interval between reconnection attempts has a shorter limit.

=== Error with {astra} DevOps API when using {product-automation}

==== Symptoms

{product-automation}'s logs:

[source,log]
----
fatal: [10.255.13.6]: FAILED! => {"changed": false, "elapsed": 0, "msg": "Status code was -1 and not [200]:
Connection failure: Remote end closed connection without response", "redirected": false, "status": -1, "url":
"https://api.astra.datastax.com/v2/databases/REDACTED/secureBundleURL"}
----

==== Cause

The {astra} DevOps API is likely temporarily unavailable.

==== Solution or Workaround

xref:astra-db-serverless:databases:secure-connect-bundle.adoc[Download the {astra-db} {scb}] manually and provide its path in the xref:deploy-proxy-monitoring.adoc#_core_configuration[{product-automation} configuration].

=== Metadata service returned not successful status code 4xx or 5xx

==== Symptoms

{product-proxy} doesn't start and the following appears on the proxy logs:

[source,log]
----
Couldn't start proxy: error initializing the connection configuration or control connection for Target:
metadata service (Astra) returned not successful status code
----

==== Cause

There are two possible causes for this:

* The credentials that {product-proxy} is using for {astra-db} don't have sufficient permissions.
* The {astra-db} database is hibernated or otherwise unavailable.

==== Solution or Workaround

In the {astra-ui}, check the xref:astra-db-serverless:databases:database-statuses.adoc[database status].

If the database is not in *Active* status, you might need to take action or wait for the database to return to active status.
For example, if the database is hibernated, xref:astra-db-serverless:databases:database-statuses.adoc#hibernated[reactivate the database].
When the database is active again, retry the connection.

If the database is in *Active* status, then the issue is likely due to the credentials permissions.
Try using an xref:astra-db-serverless:administration:manage-application-tokens.adoc[application token scoped to a database], specifically a token with the *Database Administrator* role for your target database.

[[_async_read_timeouts_stream_id_map_exhausted]]
=== Async read timeouts / stream id map exhausted

//Supposedly resolved in 2.1.0 release?

==== Symptoms

Dual reads are enabled and the following messages are found in the {product-proxy} logs:

[source,log]
----
{"log":"\u001b[33mWARN\u001b[0m[430352] Async Request (OpCode EXECUTE [0x0A]) timed out after 10000 ms. \r\n","stream":"stdout","time":"2022-10-03T17:29:42.548941854Z"}

{"log":"\u001b[33mWARN\u001b[0m[430368] Could not find async request context for stream id 331 received from async connector. It either timed out or a protocol error occurred. \r\n","stream":"stdout","time":"2022-10-03T17:29:58.378080933Z"}

{"log":"\u001b[33mWARN\u001b[0m[431533] Could not send async request due to an error while storing the request state: stream id map ran out of stream ids: channel was empty. \r\n","stream":"stdout","time":"2022-10-03T17:49:23.786335428Z"}
----

==== Cause

The last log message is logged when the async connection runs out of stream ids.
The async connection is a connection dedicated to the async reads (asynchronous dual reads feature).
This can be caused by timeouts (first log message) or the connection not being able to keep up with the load.

If the log files are being spammed with these messages then it is likely that an outage occurred which caused all responses to arrive after requests timed out (second log message).
In this case the async connection might not be able to recover.

==== Solution or Workaround

Keep in mind that any errors in the async request path (dual reads) will not affect the client application so these log messages might be useful to predict what may happen when the reads are switched over to the TARGET cluster but async read errors/warnings by themselves do not cause any impact to the client.

Starting in version 2.1.0, you can now tune the maximum number of stream ids available per connection, which by default is 2048.
You can increase it to match your driver configuration through the xref:manage-proxy-instances.adoc#zdm_proxy_max_stream_ids[zdm_proxy_max_stream_ids] property.

If these errors are being constantly written to the log files (for minutes or even hours) then it is likely that only an application OR {product-proxy} restart will fix it.
If you find an issue like this, submit a {product-proxy-repo}/issues[GitHub issue].

=== Client application closed connection errors every 10 minutes when migrating to {astra-db}

//TODO: Remove - resolved in 2.1.0
[NOTE]
====
This issue is fixed in {product-proxy} 2.1.0. See the Fix section below.
====

==== Symptoms

Every 10 minutes a message is logged in the {product-proxy} logs showing a disconnect that was caused by {astra-db}:

[source,log]
----
{"log":"\u001b[36mINFO\u001b[0m[426871] [TARGET-CONNECTOR] REDACTED disconnected \r\n","stream":"stdout","time":"2022-10-01T16:31:41.48598498Z"}
----

==== Cause

{astra-db} terminates idle connections after 10 minutes of inactivity.
If a client application only sends reads through a connection then the target cluster, which is an {astra-db} database in this example, then the connection won't get any traffic because {product-short} forwards all reads to the origin connection.

==== Solution or Workaround

This issue has been fixed in {product-proxy} 2.1.0. 
We encourage you to upgrade to that version or greater. 
By default, {product-proxy} now sends heartbeats after 30 seconds of inactivity on a cluster connection, to keep it alive. 
You can tune the heartbeat interval with the Ansible configuration variable `heartbeat_insterval_ms`, or by directly setting the `ZDM_HEARTBEAT_INTERVAL_MS` environment variable if you do not use {product-automation}.

=== Performance degradation with {product-proxy}

==== Symptoms

Consider a case where a user runs separate benchmarks against:

* {astra-db} directly
* Origin directly
* {product-proxy} (with {astra-db} and the origin cluster)

The results of these tests show latency/throughput values are worse with {product-proxy} than when connecting to {astra-db} or origin cluster directly.

==== Cause

{product-short} always increases latency and, depending on the nature of the test, reduces throughput.
Whether this performance hit is expected or not depends on the difference between the {product-short} test results and the test results with the cluster that performed the worst.

Writes in {product-short} require successful acknowledgement from both clusters, while reads only require the result from the primary cluster, which is typically the origin cluster.
This means that if the origin cluster has better performance than the target cluster, then {product-short} will have worse write performance.

It is typical for latency to increase with {product-proxy}.
To minimize performance degradation with {product-proxy}, note the following:

* Make sure your {product-proxy} infrastructure or configuration doesn't unnecessarily increase latency.
For example, make sure your {product-proxy} instances are in the same availability zone (AZ) as your origin cluster or application instances.
* Understand the impact of simple and batch statements on latency, as compared to typical prepared statements.
+
Avoid simple statements with {product-proxy} because they require significant time for {product-proxy} to parse the queries.
+
In contrast, prepared statements are parsed once, and then reused on subsequent requests, if repreparation isn't required.

==== Solution or Workaround

If you are using simple statements, consider using prepared statements as the best first step.

Increasing the number of proxies might help, but only if the VMs resources (CPU, RAM or network IO) are near capacity.
{product-proxy} doesn't use a lot of RAM, but it uses a lot of CPU and network IO.

Deploying the proxy instances on VMs with faster CPUs and faster network IO might help, but only your own tests will reveal  whether it helps, because it depends on the workload type and details about your environment such as network/VPC configurations, hardware, and so on.

=== `InsightsRpc` related permissions errors

==== Symptoms

{product-proxy} logs contain:

[source,log]
----
time="2023-05-05T19:14:31Z" level=debug msg="Recording ORIGIN-CONNECTOR other error: ERROR UNAUTHORIZED (code=ErrorCode Unauthorized [0x00002100], msg=User my_user has no EXECUTE permission on <rpc method InsightsRpc.reportInsight> or any of its parents)"
time="2023-05-05T19:14:31Z" level=debug msg="Recording TARGET-CONNECTOR other error: ERROR SERVER ERROR (code=ErrorCode ServerError [0x00000000], msg=Unexpected persistence error: Unable to authorize statement com.datastax.bdp.cassandra.cql3.RpcCallStatement)"
----

==== Cause

This could be the case if the origin ({dse-short}) cluster has Metrics Collector enabled to report metrics for {company} drivers and `my_user` does not have the required permissions.
{product-proxy} simply passes through these.

==== Solution or Workaround

There are two options to get this fixed.

===== Option 1: Disable {dse-short} Metrics Collector

* On the origin {dse-short} cluster, run `dsetool insights_config --mode DISABLED`
* Run `dsetool insights_config --show_config` and ensure the `mode` has a value of `DISABLED`

===== Option 2: Use this option if disabling metrics collector is not an option

* Using a superuser role, grant the appropriate permissions to `my_user` role by running `GRANT EXECUTE ON REMOTE OBJECT InsightsRpc TO my_user;`

[#report-an-issue]
== Report an issue

To report an issue or get additional support, submit an issue in the {product-short} component GitHub repositories:

* {product-proxy-repo}/issues[{product-proxy} repository]
* {product-automation-repo}/issues[{product-automation} repository] (includes {product-automation} and {product-utility})
* {cass-migrator-repo}/issues[{cass-migrator} repository]
* {dsbulk-migrator-repo}/issues[{dsbulk-migrator} repository]

[IMPORTANT]
====
These repositories are public.

Don't include any proprietary or private information in issues, pull requests, or comments that you make in these repositories.
====

In the issue description, include as much of the following information as possible, and make sure to remove all proprietary and private information before submitting the issue:

* Your <<check-version,{product-proxy} version>>.

* <<proxy-logs,{product-proxy} logs>>, ideally at `DEBUG` level, if you can easily reproduce the issue and tolerate restarting the proxy instances to apply the log level configuration change.

* Database deployment type ({dse-short}, {hcd-short}, {cass-short}, or {astra-db}) and version for the origin and target clusters.
The version isn't required for {astra-db}.

* Screenshots of the xref:ROOT:metrics.adoc[{product-proxy} metrics] dashboards from Grafana or your chosen visualization tool.
+
Direct read access to your metrics dashboard is preferred, if permitted by your security policy.
This is particularly helpful for performance-related issues.

* Client application and driver logs.

* The driver language and version that the client application is using.

For performance-related issues, provide the following additional information:

* Which statement types (simple, prepared, batch) do you use?

* If you use batch statements:
+
** Which driver API do you use to create these batches?
** Are you passing a `BEGIN BATCH` CQL query string to a simple/prepared statement, or do you use the actual batch statement objects that the drivers allow you to create?

* How many parameters does each statement have?

* Is CQL function replacement enabled?
This feature is disabled by default.
To determine if this feature is enabled, check the following variables:
+
** If you use {product-automation}, check the Ansible advanced configuration variable `replace_cql_functions`.
** If you don't use {product-automation}, check the environment variable `ZDM_REPLACE_CQL_FUNCTIONS`.

== See also

* xref:ROOT:metrics.adoc[]