= Feasibility checks
:page-aliases: ROOT:preliminary-steps.adoc

True zero downtime migration with {product-proxy} is possible only if your clusters, data model, and application logic meet the compatibility requirements described on this page.

You might need to adjust your data model or application logic to ensure compatibility between the origin and target clusters during the migration and ongoing compatibility with the target cluster after the migration.

If you cannot meet these requirements, particularly the cluster and schema compatibility requirements, see xref:ROOT:components.adoc[] for alternative migration tools and strategies.

== {cass-short} Native Protocol version and cluster version support

{product-proxy} supports protocol versions `v3`, `v4`, `DSE_V1`, and `DSE_V2`.

//TODO: V5 status: https://github.com/datastax/zdm-proxy/blob/main/faq.md#what-versions-of-apache-cassandra-or-cql-compatible-data-stores-does-the-zdm-proxy-support
{product-proxy} technically doesn't support `v5`.
If `v5` is requested, the proxy handles protocol negotiation so that the client application properly downgrades the protocol version to `v4`.
This means that you can use {product-proxy} with any client application that uses a driver version supporting protocol version `v5`, as long as the application doesn't use v5-specific functionality.

=== Thrift isn't supported by {product-proxy}

If you are using an earlier driver or cluster version that only supports Thrift, you must change your client application to use CQL, and, if needed, upgrade your cluster before starting the migration process.

=== Supported cluster versions and migration paths

include::ROOT:partial$migration-scenarios.adoc[]

////
TODO: Need to verify as these are in conflict with other information in this guide:

{product-proxy} supports migrations to and from the following cluster versions:

* {cass-reg} 2.1 and later, including {cass-short} 4.x.
+
{cass-short} 2.0 migration support may be introduced when protocol version v2 is supported.

* {dse} 4.7.1 and later.
+
{dse-short} 4.6 migration support may be introduced when protocol version v2 is supported.

* {astra-db}.

See also: https://github.com/datastax/zdm-proxy/blob/main/faq.md#what-versions-of-apache-cassandra-or-cql-compatible-data-stores-does-the-zdm-proxy-support
"Other CQL-compatible data stores might be compatible targets with {product-proxy}, but {company} doesn't test all possible targets."
////

[TIP]
====
Before you begin the migration process, try directly connecting your client application to your target cluster without {product-proxy}.
This ensures that you know the connection will work when you disconnect {product-proxy} at the end of the migration.
====

== Schema/keyspace compatibility

[IMPORTANT]
====
To successfully migrate with {product-proxy}, the origin and target clusters must have matching schemas, including keyspace names, table names, column names, and data types.
A CQL statement produced by your client application must be able to succeed _without modification_ on both clusters because {product-proxy} routes the exact same CQL statements to both clusters.
====

{product-proxy} does not modify or transform CQL statements besides the optional feature that can replace `now()` functions with timestamp literals.
See <<cql-function-replacement>> for more information about this feature.

A CQL statement that your client application sends to {product-proxy} must be able to succeed on both clusters.
This means that any keyspace that your client application uses must exist on both the origin and target clusters with the same name (although they can have different replication strategies and durable writes settings).
Table names must also match.

The schema doesn't have to be an exact match as long as the CQL statements can be executed successfully on both clusters.
For example, if a table has 10 columns but your client application only uses 5 of those columns then you could create that table on the target with just those 5 columns.

You can also change the primary key in some cases.
For example, if your compound primary key is `PRIMARY KEY (A, B)` and you always provide parameters for the `A` and `B` columns in your CQL statements then you could change the key to `PRIMARY KEY (B, A)` when creating the schema on the target because your CQL statements will still run successfully.

== Request and error handling expectations with {product-proxy}

See xref:ROOT:components.adoc#how-zdm-proxy-handles-reads-and-writes[How {product-proxy} handles reads and writes].

== Considerations for {astra-db} migrations

{astra-db} implements guardrails and sets limits to ensure good practices, foster availability, and promote optimal configurations for your databases.
Check the list of xref:astra-db-serverless:databases:database-limits.adoc[guardrails and limits] to make sure that your application workload can be successful within these limits.

If you need to make changes to the application or data model to ensure that your workload can run successfully in {astra-db}, then you need to do these changes before you start the migration process.

It is also highly recommended to perform tests and benchmarks when connected directly to {astra-db} prior to the migration, so that you don't find unexpected issues during the migration process.

=== {astra} doesn't support CL.ONE

`CL.ONE` isn't supported by {astra}, and read and write requests sent through {product-proxy} with `CL.ONE` to {astra-db} databases always fail.

{product-proxy} doesn't mute these failures because you need to be aware of them.
You must adapt your client application to use a consistency level that is supported by both clusters to ensure that the migration is seamless and error-free.

=== {astra} doesn't support the Stargate APIs

The Stargate APIs (Document, REST, GraphQL, gRPC) are deprecated for {astra}.

If you are migrating to {astra} from an origin cluster that uses any of these APIs, your client applications won't work with {astra}.
Before you migrate, you must change your applications to use other programmatic access, such as {cass-short} drivers or the {data-api}.
For more information, see xref:astra-db-serverless:api-reference:compare-dataapi-to-stargate.adoc[].

[[_read_only_applications]]
=== Read-only applications

In versions 2.1.0 and later, {product-proxy} sends periodic heartbeats to keep idle cluster connections alive.
The default interval is 30,000 milliseconds, and it can be configured with the `xref:ROOT:manage-proxy-instances.adoc#change-mutable-config-variable[heartbeat_interval_ms]` variable, or by directly setting the `ZDM_HEARTBEAT_INTERVAL_MS` environment variable if you aren't using {product-automation}.

In {product-proxy} versions earlier than 2.1.0, read-only applications require special handling to avoid connection termination due to inactivity.

{company} recommends that you use {product-proxy} version 2.1.0 or later to benefit from the heartbeat feature.
If you cannot use version 2.1.0 or later, see the alternatives described in xref:ROOT:troubleshooting-tips.adoc#client-application-closed-connection-errors-every-10-minutes-when-migrating-to-astra-db[Client application closed connection errors every 10 minutes when migrating to {astra-db}].

[[non-idempotent-operations]]
== Lightweight Transactions and other non-idempotent operations

Examples of non-idempotent operations in CQL are:

* Lightweight Transactions (LWTs) (see <<_lightweight_transactions_and_the_applied_flag>>)
* Counter updates
* Collection updates with `+=` and `-=` operators
* Non-deterministic functions like `now()` and `uuid()` (see <<cql-function-replacement>>)

Given that there are two separate clusters involved, the state of each cluster can be different.
For conditional writes, this can create a temporary divergent state.

If you use non-idempotent operations, {company} recommends adding a reconciliation phase to your migration before and after Phase 4 (where you switch reads to the target).
This allows you additional opportunities to resolve any data inconsistencies that are produced by non-idempotent operations.

The {cass-migrator} is ideal for detecting and reconciling these types of inconsistencies.
For more information, see xref:migrate-and-validate-data.adoc[].

If your application workloads can tolerate inconsistencies produced by LWTs and non-idempotent operations, you might not need to perform any additional validation or reconciliation steps.
This depends entirely on your application business logic and requirements.
It is your responsibility to determine whether your workloads can tolerate these inconsistencies and to what extent.

[[_lightweight_transactions_and_the_applied_flag]]
=== Lightweight Transactions and the applied flag

//TODO: Align with the write request language on components.adoc

////
The ZDM proxy can bifurcate Lightweight Transactions to the ORIGIN and TARGET clusters.
However, it only returns the applied flag from one cluster, whichever cluster is the source of truth. 
Given that there are two separate clusters involved, the state of each cluster may be different.
For conditional writes, this may create a divergent state for a time.
It may not make a difference in many cases, but if Lightweight Transactions are used, we would recommend a reconciliation phase in the migration before switching reads to rely on the TARGET cluster.
////

{product-proxy} handles LWTs as write operations.
The proxy sends the LWT to the origin and target clusters concurrently, and then waits for a response from both.
{product-proxy} will return a `success` status to the client if both the origin and target send successful acknowledgements.
Otherwise, it will return a `failure` status if one or both do not return an acknowledgement.

What sets LWTs apart from regular writes is that they are conditional.
In other words, a LWT can appear to have been successful (its execution worked as expected).
However, the change will be applied only if the LWT's condition was met.
Whether the condition was met depends on the state of the data on the cluster.
In a migration, the clusters will not be in sync until all existing data has been imported into the target.
Up to that point, an LWT's condition can be evaluated differently on each side, leading to a different outcome even though the LWT was technically successful on both sides.

The response that a cluster sends after executing a LWT includes a flag called `applied`.
This flag tells the client whether the LWT update was actually applied.
The status depends on the condition, which in turn depends on the state of the data.
When {product-proxy} receives a response from both the origin and target, each response would have its own `applied` flag.

However, {product-proxy} can only return a *single response* to the client.
Recall that the client has no knowledge that there are two clusters behind the proxy.
Therefore, {product-proxy} returns the `applied` flag from the cluster that is *currently used as primary*.
If your client has logic that depends on the `applied` flag, be aware that during the migration, you will only have visibility of the flag coming from the primary cluster; that is, the cluster to which synchronous reads are routed.

To reiterate, {product-proxy} only returns the `applied` value from the primary cluster, which is the cluster from where read results are returned to the client application. By default, this is the origin cluster.
This means that when you set the target cluster as your primary cluster, then the `applied` value returned to the client application will come from the target cluster.

[[cql-function-replacement]]
=== Server-side non-deterministic functions in the primary key

Statements with xref:dse-6.9@cql:reference:uuid.adoc[UUID and timeuuid functions], like `now()` and `uuid()`, create data inconsistencies between the origin and target clusters because the values are computed at the cluster level.

If these functions are used for regular non-primary key columns, you must determine if it is acceptable to have different values in the two clusters depending on your application business logic.
However, if these functions are used in any primary key column, then your data migration phase will fail because of data inconsistencies between the two clusters.
Effectively, the clusters will never be truly in sync from a programmatic perspective.

{product-proxy} has an option to replace `now()` with a timeUUID calculated at the proxy level to ensure that these records write the same value to both clusters.

To enable this feature, set `replace_cql_functions` to `true`.
For more information, see xref:manage-proxy-instances.adoc#change-mutable-config-variable[Change a mutable configuration variable].

[IMPORTANT]
====
The `replace_cql_functions` option only replaces the `now()` function.

This feature is disabled by default because it has a noticeable impact on performance.
{company} recommends that you test this feature extensively before using it in production.
====

If the performance impact is unacceptable for your application, or you are using functions other than `now()`, then you must change your client application to use values calculated locally at the client-level before the statement is sent to the database.
Most drivers have utility methods that help you compute these values locally.
For more information, see your driver's documentation and xref:datastax-drivers:developing:query-timestamps.adoc[Query timestamps in {cass-short} drivers].

[#driver-retry-policy-and-query-idempotence]
== Driver retry policy and query idempotence

[IMPORTANT]
====
The {product-short} process requires you to perform rolling restarts of your client applications during the migration.
This is standard practice for client applications that are deployed over multiple instances, and it is a widely used approach to roll out releases and configuration changes.
====

As part of the normal migration process, the {product-proxy} instances will have to be restarted in between phases to apply configuration changes.
From the point of view of the client application, this is a similar behavior to a {dse-short} or {cass-short} cluster going through a rolling restart in a non-migration scenario.

If your application already tolerates rolling restarts of your current cluster then you should see no issues when there is a rolling restart of {product-proxy} instances.

To ensure that your client application retries requests when a database connection is closed you should check the section of your driver's documentation related to retry policies.

Most {company}-compatible drivers require a statement to be marked as `idempotent` in order to retry it in case of a connection error (such as the termination of a database connection).
This means that, by default, these drivers treat statements as non-idempotent, and the drivers don't automatically retry them in the event of a connection error.

For more information, see the following driver documentation:

* xref:datastax-drivers:developing:query-idempotence.adoc[]
* xref:datastax-drivers:connecting:retry-policies.adoc[]

== {dse-short} advanced workloads

Graph::
{product-proxy} handles all {dse-short} Graph requests as write requests even if the traversals are read-only. There is no special handling for these requests, so you need to take a look at the traversals that your client application sends and determine whether the traversals are idempotent. If the traversals are non-idempotent then the reconciliation step is needed.
+
Keep in mind that our recommended tools for data migration and reconciliation are CQL-based, so they can be used for migrations where the origin cluster is a database that uses the new {dse-short} Graph engine released with {dse-short} 6.8, but *cannot be used for the old Graph engine* that older {dse-short} versions relied on.
See <<non-idempotent-operations,this section>> for more information about non-idempotent operations.

Search::
Read-only {dse-short} Search workloads can be moved directly from the origin to the target without {product-proxy} being involved.
If your client application uses Search and also issues writes, or if you need the read routing capabilities from {product-proxy}, then you can connect your Search workloads to it as long as you are using xref:datastax-drivers:compatibility:driver-matrix.adoc[{company}-compatible drivers] to submit these queries.
This approach means the queries are regular CQL `SELECT` statements, so {product-proxy} handles them as regular read requests.
+
If you use the HTTP API then you can either modify your applications to use the CQL API instead or you will have to move those applications directly from the origin to the target when the migration is complete if that is acceptable.

For more {dse-short}-specific migration considerations, see xref:6.9@dse:managing:operations/migrate-data.adoc[].

== Client compression

The binary protocol used by {cass-short}, {dse-short}, {hcd-short}, and {astra-db} supports optional compression of transport-level requests and responses that reduces network traffic at the cost of CPU overhead.

When establishing connections from client applications, {product-proxy} responds with a list of compression algorithms supported by both clusters.
The compression algorithm configured in your {company}-compatible driver must match any item from the common list, or CQL request compression must be disabled completely.
{product-proxy} cannot decompress and recompress CQL requests using different compression algorithms.

This isn't related to storage compression, which you can configure on specific tables with the `compression` table property.
Storage/table compression doesn't affect the client application or {product-proxy} in any way.

== Authenticator and authorizer configuration

A cluster's _authorizer_ doesn't affect client applications or {product-proxy}, which means that you can use any kind of authorizer configuration on your clusters, and they can use different authorizers.

In contrast, a cluster's _authenticator_ must be compatible with {product-proxy}.

{product-proxy} supports the following cluster authenticator configurations:

* No authenticator
* `PasswordAuthenticator`
* `DseAuthenticator` with `internal` or `ldap` scheme

{product-proxy} _doesn't_ support `DseAuthenticator` with `kerberos` scheme.

The origin and target clusters can have different authentication configurations because {product-proxy} treats them independently.

[#considerations-for-multi-datacenter-clusters-and-other-complex-migrations]
== Considerations for multi-datacenter clusters and other complex migrations

Complex migration scenarios, such as multi-datacenter migrations or many-to-one migrations, require additional planning to configure {product-proxy} and migrate the data efficiently.

include::ROOT:partial$multi-region-migrations.adoc[]

To configure {product-proxy} for a multi-datacenter migration, see the xref:ROOT:deployment-infrastructure.adoc[{product-proxy} infrastructure guidelines for multi-datacenter clusters].

For more guidance on migrations to {astra}, see the xref:sideloader:prepare-sideloader.adoc#additional-preparation-for-specific-migration-scenarios[{sstable-sideloader} preparations for specific migration scenarios].

== Next steps

Next, xref:ROOT:deployment-infrastructure.adoc[prepare the {product-proxy} infrastructure].