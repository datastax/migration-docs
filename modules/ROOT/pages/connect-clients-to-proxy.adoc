= Connect your client applications to {product-proxy}
:navtitle: Connect client applications to {product-proxy}
:page-tag: migration,zdm,zero-downtime,zdm-proxy,connect-apps

{product-proxy} is designed to be similar to a conventional {cass-reg} cluster.
You communicate with it using the CQL query language used in your existing client applications.
It understands the same messaging protocols used by {cass-short}, {dse}, and {astra-db}.
As a result, most of your client applications won't be able to distinguish between connecting to {product-proxy} and connecting directly to your {cass-short} cluster.

On this page, we explain how to connect your client applications to a {cass-short} cluster.
We then move on to discuss how this process changes when connecting to a {product-proxy}.
We conclude by describing two sample client applications that serve as real-world examples of how to build a client application that works effectively with {product-proxy}.

You can use the provided sample client applications, in addition to your own, as a quick way to validate that the deployed {product-proxy} is reading and writing data from the expected origin and target clusters.

This topic also explains how to connect `cqlsh` to {product-proxy}.

== {company}-compatible drivers

You can use {cass-short} drivers to connect your client applications to {cass-short}, {dse-short}, {hcd-short}, and {astra-db}.
With drivers, you can allow execute queries, iterate through results, access metadata about your cluster, and perform other related activities.

For available drivers and driver documentation, see xref:datastax-drivers:compatibility:driver-matrix.adoc[].

[[_connecting_company_drivers_to_cassandra]]
== Connect drivers to {cass-short}

Perhaps the simplest way to demonstrate how to use the drivers to connect your client application to a {cass-short} cluster is an example in the form of some sample code.
But there's a bit of a problem: the drivers are independent projects implemented natively in the relevant programming language.

This approach offers the benefit of allowing each project to provide an API that makes the most sense for the language or platform on which it's implemented.
Unfortunately it also means there is some variation between languages.
With that in mind, the following pseudocode provides reasonable guidance for understanding how a client application might use one of the drivers.

[source]
----
// Create an object to represent a Cassandra cluster listening for connections at
// 10.20.30.40 on the default port (9042).  The username and password are necessary
// only if your Cassandra cluster has authentication enabled.
Cluster my_cluster = Cluster.build_new_cluster(contact_points = "10.20.30.40", username="myusername", password="mypassword")

// Connect our Cluster object to our Cassandra cluster, returning a Session
Session my_session = my_cluster.connect()

// Execute a query, returning a ResultSet
ResultSet my_result_set = my_session.execute("select release_version from system.local")

// Retrieve the "release_version" column from the first row of our result set
String release_version = my_result_set.first_row().get_column("release_version")

// Close our Session and Cluster
my_session.close()
my_cluster.close()

// Display the release version to the user
print(release_version)
----

As noted, you'll see some differences in individual drivers:

* New versions of the Java driver no longer define a Cluster object.
Client programs create a Session directly.
* The Node.js driver has no notion of a Cluster or Session at all, instead using a Client object to represent this functionality.

The details may vary but you'll still see the same general pattern described in the pseudocode in each of the drivers.

This topic does not describe details or APIs for any of the drivers mentioned above.
All the drivers come with a complete set of documentation for exactly this task.
The following links provide some good starting points for learning about the interfaces for each specific driver:

//TODO: Move this to the driver docs and replace this whole list with a link to the connect page.
* The https://docs.datastax.com/en/developer/java-driver/latest/manual/core/[core driver section] of the Java driver manual.
* The https://docs.datastax.com/en/developer/python-driver/latest/getting_started/[getting started guide] for the Python driver.
* The https://docs.datastax.com/en/developer/csharp-driver/latest/index.html#basic-usage[basic usage section] of the C# driver documentation.
* The https://docs.datastax.com/en/developer/cpp-driver/latest/topics/[getting started section] of the C/C++ driver documentation.
* The https://docs.datastax.com/en/developer/nodejs-driver/latest/#basic-usage[basic usage section] of the Node.js driver documentation.

== Connect applications to {product-proxy}

We mentioned above that connecting to a {product-proxy} should be almost indistinguishable from connecting directly to your {cass-short} cluster.
This design decision means there isn't much to say here; everything we discussed in the section above also applies when connecting your driver to {product-proxy}.
There are a few extra considerations to keep in mind, though, when using the proxy.

[[_client_application_credentials]]
=== Client application credentials

Client applications provide cluster credentials to authenticate requests.

Client applications connect to {product-proxy} in the same way that they connect to a cluster: by providing a set of credentials.

Clients have no awareness of the {product-proxy} architecture or the existence of the two separate clusters (the origin and target).
Therefore, a client only provides a single set of credentials when connecting to {product-proxy}, the same as it would when connecting directly to a cluster.

{product-proxy} uses the credentials provided by the client to forward requests to the cluster that corresponds with those credentials, which is usually the target cluster.
If necessary, {product-proxy} uses the credentials defined in `xref:ROOT:deploy-proxy-monitoring.adoc#cluster-and-core-configuration[zdm_proxy_cluster_config.yml]` to forward requests to the other cluster, which is usually the origin cluster.

.Credential usage by {product-proxy} when authentication is required for both clusters
image::zdm-proxy-credential-usage.png[{product-proxy} credentials usage when authentication is required for both clusters, 550]

==== Determine which credentials to provide

The credentials your client must provide depend on the authentication requirements of the origin and target clusters:

* *Authentication required for both clusters*: Your client application must supply credentials for the target cluster.
* *Authentication required for target cluster only*: Your client application must supply credentials for the target cluster.
* *Authentication required for origin cluster only*: Your client application must supply credentials for the origin cluster.
* *No authentication required for either cluster*: Your client application doesn't need to supply any cluster credentials.

==== Expected authentication credentials for self-managed clusters

For a self-managed clusters that require authentication, your client application must provide valid `username` and `password` values to access the cluster.

For information about self-managed cluster credentials in your {product-proxy} configuration, see xref:ROOT:deploy-proxy-monitoring.adoc#cluster-and-core-configuration[Cluster and core configuration].

[#expected-authentication-credentials-for-astra-db]
==== Expected authentication credentials for {astra-db}

For {astra-db} databases, your client application can provide either application token credentials or a {scb}.

[tabs]
======
Application token::
+
--
With token-based authentication, the {product-proxy} automatically calls the {astra} DevOps API to download the {scb-short}.
The provided application token is used to authenticate this request.

To use token-based authentication, xref:astra-db-serverless:administration:manage-application-tokens.adoc[generate an application token] with the *Organization Administrator* role.
This role is recommended to avoid permissions-related request failures during the migration.
After the migration, you can configure your client applications to use tokens with reduced permissions.

The token has three keys: `clientId`, `secret`, and `token`.
Using these keys, you must specify one of the following sets of credentials in your {product-proxy} configuration:

* Token-only authentication (Recommended):
+
** Set `username` to the literal string `token`.
** Set `password` to the actual application token value (`AstraCS:...`).

* Legacy authentication for older applications and drivers:
+
** Set `username` to the `clientId` value generated with the token.
** Set `password` to the `secret` value generated with the token.
--

{scb-short}::
+
--
To use {scb-short} authentication, xref:astra-db-serverless:databases:secure-connect-bundle.adoc[download your database's {scb-short}], and then provide the path to the {scb-short} zip file in your {product-proxy} configuration.
--
======

For information about setting {astra-db} credentials in your {product-proxy} configuration, see xref:ROOT:deploy-proxy-monitoring.adoc#cluster-and-core-configuration[Cluster and core configuration].


. xref:astra-db-serverless:administration:manage-application-tokens.adoc[Generate an application token] with the *Organization Administrator* role.


. Download your database's xref:astra-db-serverless:databases:secure-connect-bundle.adoc[{scb-short}].
+
[IMPORTANT]
====
The {scb-short} contains sensitive information that establishes a connection to your database, including key pairs and certificates.
Treat it as you would any other sensitive values, such as passwords or tokens.
====

=== Disable client-side compression with {product-proxy}

Client applications must not enable client-side compression when connecting through {product-proxy}, as this is not currently supported.
This is disabled by default in all drivers, but if it was enabled in your client application configuration, it will have to be temporarily disabled when connecting to {product-proxy}.

=== {product-proxy} ignores token-aware routing

Token-aware routing isn't enforced when connecting through {product-proxy} because these instances don't hold actual token ranges in the same way as database nodes.
Instead, each {product-proxy} instance has a unique, non-overlapping set of synthetic tokens that simulate token ownership and enable balanced load distribution across the instances.

Upon receiving a request, a {product-proxy} instance routes the request to appropriate source and target database nodes, independent of token ownership.

If your clients have token-aware routing enabled, you don't need to disable this behavior while using {product-proxy}.
Clients can continue to operate with token-aware routing enabled without negative impacts to functionality or performance.

== Sample client applications

[IMPORTANT]
====
These sample applications are for demonstration purposes only.
They are not intended for production use or for production-scale performance testing.

To test your target cluster's ability to handle production workloads, you can xref:ROOT:enable-async-dual-reads.adoc[enable asynchronous dual reads].

To assess the performance of {product-proxy}, {company} recommends http://docs.nosqlbench.io/getting-started/[NoSQLBench].
====

The following sample client applications demonstrate how to use the Java driver with {product-proxy} and the origin and target for that proxy.

See your driver's documentation for code samples that are specific to your chosen driver, including cluster connection examples and statement execution examples.

=== {product-demo}

https://github.com/alicel/zdm-demo-client/[{product-demo}] is a minimal Java web application which provides a simple, stripped-down example of an application built to work with {product-proxy}.
After updating connection information you can compile and run the application locally and interact with it using HTTP clients such as `curl` or `wget`.

You can find the details of building and running {product-demo} in the https://github.com/alicel/zdm-demo-client/blob/master/README.md[README].

[[_themis_client]]
=== Themis client

https://github.com/absurdfarce/themis[Themis] is a Java command-line client application that allows you to write randomly generated data directly to the origin cluster, directly to the target cluster, or indirectly to both clusters through {product-proxy}.

Then, you can use the client application to query the data and confirm that {product-proxy} is reading and writing data from the expected sources.

Configuration details for the clusters and {product-proxy} are defined in a YAML file.
For more information, see the https://github.com/absurdfarce/themis/blob/main/README.md[Themis README].

In addition to any utility as a validation tool, Themis also serves as an example of a larger client application which uses the Java driver to connect to a {product-proxy} -- as well as directly to {cass-short} clusters or {astra-db} -- and perform operations.
The configuration logic as well as the cluster and session management code have been cleanly separated into distinct packages to make them easy to understand.

== Connect cqlsh to {product-proxy}

`cqlsh` is a command-line tool that you can use to send {cass-short} Query Language (CQL) statements to your {cass-short}-based clusters, including {astra-db}, {dse-short}, {hcd-short}, and {cass} databases.

You can use your database's included version of `cqlsh`, or you can download and run a standalone `cqlsh`.

Your origin and target clusters must have a common `cql_version` between them.
If there is no CQL version that is compatible with both clusters, `cqlsh` won't be able to connect to {product-proxy}.

To connect `cqlsh` to a {product-proxy} instance, do the following:

. On a machine that can connect to your {product-proxy} instance, https://downloads.datastax.com/#cqlsh[download `cqlsh`].
+
Any version of `cqlsh` can connect to {product-proxy}, but some clusters require a specific `cqlsh` version.

. Install `cqlsh` by extracting the downloaded archive:
+
[source,shell,subs="+quotes"]
----
tar -xvf **CQLSH_ARCHIVE**
----
+
Replace `**CQLSH_ARCHIVE**` with the file name of the downloaded `cqlsh` archive, such as `cqlsh-astra-20210304-bin.tar.gz`.

. Change to the `bin` directory in your `cqlsh` installation directory.
For example, if you installed `cqlsh` for {astra-db}, you would run `cd cqlsh-astra/bin`.

. Launch `cqlsh`:
+
[source,shell,subs="+quotes"]
----
./cqlsh **ZDM_PROXY_IP** **PORT** -u **USERNAME** -p **PASSWORD**
----
+
Replace the following:
+
* `**ZDM_PROXY_IP**`: The IP address of your {product-proxy} instance.
* `**PORT**`: The port on which the {product-proxy} instance listens for client connections.
If you are using the default port, 9042, you can omit this argument.
* `**USERNAME**` and `**PASSWORD**`: Valid xref:_client_application_credentials[client connection credentials], depending on the authentication requirements for your origin and target clusters:
+
** *Authentication required for both clusters*: Provide credentials for the target cluster.
** *Authentication required for target cluster only*: Provide credentials for the target cluster.
** *Authentication required for origin cluster only*: Provide credentials for the origin cluster.
** *No authentication required for either cluster*: Omit the `-u` and `-p` arguments.

+
[IMPORTANT]
====
If you need to provide credentials for an {astra-db} database, don't use the {scb-short} when attempting to connect `cqlsh` to {product-proxy}.
Instead, use the token-based authentication option explained in <<expected-authentication-credentials-for-astra-db>>.

If you include the {scb-short}, `cqlsh` ignores all other connection arguments and connects exclusively to your {astra-db} database instead of {product-proxy}.
====