= Phase 2: Migrate and validate data
:page-aliases: ROOT:sideloader-zdm.adoc, ROOT:dsbulk-migrator-overview.adoc, ROOT:dsbulk-migrator.adoc

In xref:ROOT:phase1.adoc[Phase 1], you set up {product-proxy} to orchestrate live traffic to your origin and target clusters.

In Phase 2 of {product}, you migrate data from the origin to the target, and then validate the migrated data.

image::migration-phase2ra.png[In {product-short} Phase 2, you migrate data from the origin cluster to the target cluster]

To move and validate data, you can use a dedicated data migration tool, such as {sstable-sideloader}, {cass-migrator}, or {dsbulk-loader}, or your can create your own custom data migration script.

//Migration tool summaries are also on ROOT:components.adoc.

== {sstable-sideloader}

This tool is exclusively for migrations that move data to {astra-db}.

{sstable-sideloader} is a service running in {astra-db} that imports data from snapshots of your existing {cass-reg}-based cluster.
Because it imports data directly, {sstable-sideloader} can offer several advantages over CQL-based tools like {dsbulk-loader} and {cass-migrator}, including faster, more cost-effective data loading, and minimal performance impacts on your origin cluster and target database.

To migrate data with {sstable-sideloader}, you use `nodetool`, a cloud provider's CLI, and the {astra} {devops-api}:

* *`nodetool`*: Create snapshots of your existing {dse-short}, {hcd-short}, or open-source {cass-short} cluster.
For compatible origin clusters, see xref:ROOT:astra-migration-paths.adoc[].
* *Cloud provider CLI*: Upload snapshots to a dedicated cloud storage bucket for your migration.
* *{astra} {devops-api}*: Run the {sstable-sideloader} commands to write the data from cloud storage to your {astra-db} database.

You can use {sstable-sideloader} alone or with {product-proxy}.

For more information and instructions, see xref:sideloader:sideloader-overview.adoc[].

.Use {sstable-sideloader} with {product-proxy}
svg::sideloader:astra-migration-toolkit.svg[]

== {cass-migrator}

You can use {cass-migrator} ({cass-migrator-short}) for data migration and validation between {cass-short}-based databases.
It offers extensive functionality and configuration options to support large and complex migrations as well as post-migration data validation.

You can use {cass-migrator-short} alone, with {product-proxy}, or for data validation after using another data migration tool.

For more information, see xref:ROOT:cassandra-data-migrator.adoc[].

== {dsbulk-loader}

{dsbulk-loader} is a high-performance data loading and unloading tool for {cass-short}-based databases.
You can use it to load, unload, and count records.

Because {dsbulk-loader} doesn't have the same data validation capabilities as {cass-migrator-short}, it is best for migrations that don't require extensive data validation, aside from post-migration row counts.

You can use {dsbulk-loader} alone or with {product-proxy}.

For more information, see xref:dsbulk:overview:dsbulk-about.adoc[].

[TIP]
====
include::ROOT:partial$dsbulk-migrator-deprecation.adoc[]
====

== Other data migration processes

Depending on your origin and target databases, there might be other {product-short}-compatible data migration tools available, or you can write your own custom data migration processes with a tool like Apache Spark(TM).

To use a data migration tool with {product-proxy}, it must meet the following requirements:

* Built-in data validation functionality or compatibility with another data validation tool, such as {cass-migrator-short}.
This is crucial to a successful migration.

* Preserves the data model, including column names and data types, so that {product-proxy} can send the same read/write statements to both databases successfully.
+
Migrations that perform significant data transformations might not be compatible with {product-proxy}.
The impact of data transformations depends on your specific data model, database platforms, and the scale of your migration.

== Next steps

[IMPORTANT]
====
Don't proceed to Phase 3 until you have replicated _all_ preexisting data from your origin cluster to your target cluster, _and_ you have taken time to validate that the data was migrated correctly and completely.

The success of your migration and future performance of the target cluster depends on correct and complete data.

If your chosen data migration tool doesn't have built-in validation features, you need to use a separate tool for validation.
====

After using your chosen data migration tool to migrate and thoroughly validate your data, proceed to xref:ROOT:enable-async-dual-reads.adoc[Phase 3] to test your target cluster's production readiness.