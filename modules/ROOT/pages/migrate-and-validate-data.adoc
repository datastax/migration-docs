= Phase 2: Migrate and validate data
:page-tag: migration,zdm,zero-downtime,validate-data
ifdef::env-github,env-browser,env-vscode[:imagesprefix: ../images/]
ifndef::env-github,env-browser,env-vscode[:imagesprefix: ]

In phase 2 of data migration, you migrate data from the origin to the target, and then validate the migrated data.

image::{imagesprefix}migration-phase2ra.png[Phase 2 diagram shows using tools to migrate data from Origin to Target.]

//For illustrations of all the migration phases, see the xref:introduction.adoc#_migration_phases[Introduction].

This topic introduces data migration tools that you can use during phase 2 of your migration project:

* {cass-migrator} ({cass-migrator-short}): Best for migrating large amounts of data and for migrations that need support for detailed logging, data verification, table column renaming, and reconciliation.

* {dsbulk-migrator}: Extends {dsbulk-loader} with migration-specific commands. Best for simple migration of smaller amounts of data quantities, and migrations that don't require support for data validation during the migration.

* {astra-db} {sstable-sideloader}: Exclusively for migrations from a {cass-reg} or {dse} cluster to an {astra-db} database.
You can use {cass-migrator-short} to validate data after the migration.

[[cass-migrator-key-features]]
== {cass-migrator} features

{cass-migrator-short} offers extensive functionalities to support large and complex migrations:

* Bulk export and import
* Data conversion
* Mapping of column names across the origin and target clusters, even if column names differ
* Automatic detection of table schemas (column names, data types, primary keys, and more)
* Validation:
** Data validation during and after migration
** Log partitions range-level exceptions, and use the exceptions file as input for rerun operations
** Validate advanced data types (Sets, Lists, Maps, UDTs)
** Validate migration accuracy and performance with a smaller randomized dataset.
* Migration of counter tables.
* Migration of UDTs, even if keyspace names differ
* Write times and Time To Live (TTL):
** Preserve write times and TTL
** Add custom fixed write times
** Define separate configurations for write time and TTL columns
** Specify a subset of columns with write time and TTL without requiring that you use all eligible columns to compute the origin value
* Filter records from the origin cluster using write times, CQL conditions, or a list of token ranges
* Guardrails, such as identifying large fields.
* Containerization, including Docker and Kubernetes
* SSL, including custom cipher algorithms
* Migration and validation to and from Azure Cosmos {cass-short}
* Convert data types between the origin and target clusters with preset or custom Codecs
* Add new columns to a target table abd populate them with constant values
* Expand a map column from an origin table into multiple rows in a target table (if the map key is part of the target table's primary key)
* Automatically set `RandomPartitioner` minimum and maximum values

For more information, see xref:ROOT:cassandra-data-migrator.adoc[] and the https://github.com/datastax/cassandra-data-migrator[{cass-migrator} repository].

[TIP]
====
The {cass-migrator} repository includes the following configuration file templates with embedded comments and default values:

* https://github.com/datastax/cassandra-data-migrator/blob/main/src/resources/cdm.properties[cdm.properties]: A subset of common configuration options

* https://github.com/datastax/cassandra-data-migrator/blob/main/src/resources/cdm-detailed.properties[cdm-detailed.properties]: All available options
====

[[dsbulk-migrator-key-features]]
== {dsbulk-migrator} features

{dsbulk-migrator}, which is based on {dsbulk-loader}, is best for migrating smaller amounts of data or when you can shard data from table rows into more manageable quantities.

{dsbulk-migrator} provides the following commands:

* `migrate-live`: Start a live data migration using a the embedded version of {dsbulk-loader} or your own {dsbulk-loader} installation.
A live migration means that the data migration starts immediately and is performed by this migrator tool through the specified {dsbulk-loader} installation.

* `generate-script`: Generate a migration script that you can execute to perform a data migration with a your own {dsbulk-loader} installation.
This command _doesn't_ trigger the migration; it only generates the migration script that you must then execute.

* `generate-ddl`: Read the schema from origin, and then generate CQL files to recreate it in your target {astra-db} database.

For more information, see xref:ROOT:dsbulk-migrator.adoc[] and the https://github.com/datastax/dsbulk-migrator[{dsbulk-migrator} repository].

== {astra-db} {sstable-sideloader}

{sstable-sideloader} is a service running in {astra-db} that directly imports data from snapshot backups that you've uploaded to {astra-db} from an existing {cass-reg} or {dse} cluster.

Because it imports data directly, {sstable-sideloader} can offer several advantages over CQL-based tools like {dsbulk-migrator} and {cass-migrator}, including faster, more cost-effective data loading, and minimal performance impacts on your origin cluster and target database.

{sstable-sideloader} uses the {astra-db} {devops-api}, your cloud providers CLI, and `nodetool`.

For more information, see xref:sideloader:sideloader-overview.adoc[].