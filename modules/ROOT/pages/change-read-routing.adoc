= Route reads to the target
:page-tag: migration,zdm,zero-downtime,zdm-proxy,read-routing

This topic explains how you can configure the {product-proxy} to route all reads to the target cluster instead of the origin cluster.

image::migration-phase4ra9.png["Phase 4 diagram shows read routing on {product-proxy} was switched to the target."]

For illustrations of all the migration phases, see the xref:introduction.adoc#_migration_phases[Introduction].

== Steps

You would typically perform these steps once you have migrated all the existing data from the origin cluster, and completed all validation checks and reconciliation if necessary.

This operation is a configuration change that can be carried out as explained xref:manage-proxy-instances.adoc#change-mutable-config-variable[here].

[TIP]
====
If you xref:enable-async-dual-reads.adoc[enabled asynchronous dual reads] to test your target cluster's performance, make sure that you disable asynchronous dual reads when you're done testing.

To do this, edit the `vars/zdm_proxy_core_config.yml` file, and then set the `read_mode` variable  to `PRIMARY_ONLY`.

If you don't disable asynchronous dual reads, {product-proxy} instances send asynchronous, duplicate read requests to your origin cluster.
This is harmless but unnecessary.
====

== Changing the read routing configuration

If you're not there already, `ssh` back into the jumphost:

[source,bash]
----
ssh -F ~/.ssh/zdm_ssh_config jumphost
----

On the jumphost, connect to the Ansible Control Host container:
[source,bash]
----
docker exec -it zdm-ansible-container bash
----

You will see a prompt like:
[source,bash]
----
ubuntu@52772568517c:~$
----

Now open the configuration file `vars/zdm_proxy_core_config.yml` for editing.

Change the variable `primary_cluster` to `TARGET`.

Run the playbook that changes the configuration of the existing {product-proxy} deployment:

[source,bash]
----
ansible-playbook rolling_update_zdm_proxy.yml -i zdm_ansible_inventory
----

Wait for the {product-proxy} instances to be restarted by Ansible, one by one.
All instances will now send all reads to the target cluster instead of the origin cluster.

At this point, the target cluster becomes the primary cluster, but the {product-proxy} still keeps the origin cluster up-to-date through dual writes.

== Verifying the read routing change

Once the read routing configuration change has been rolled out, you may want to verify that reads are correctly sent to the target cluster, as expected.
This is not a required step, but you may wish to do it for peace of mind.

[TIP]
====
Issuing a `DESCRIBE` or a read to any system table through the {product-proxy} is *not* a valid verification.

The {product-proxy} handles reads to system tables differently, by intercepting them and always routing them to the origin, in some cases partly populating them at proxy level.

This means that system reads are *not representative* of how the {product-proxy} routes regular user reads.
Even after you switched the configuration to read the target cluster as the primary cluster, all system reads still go to the origin.

Although `DESCRIBE` requests are not system requests, they are also generally resolved in a different way to regular requests, and should not be used as a means to verify the read routing behavior.
====

Verifying that the correct routing is taking place is a slightly cumbersome operation, due to the fact that the purpose of the {product-short} process is to align the clusters and therefore, by definition, the data will be identical on both sides.

For this reason, the only way to do a manual verification test is to force a discrepancy of some test data between the clusters.
To do this, you could consider using the xref:connect-clients-to-proxy.adoc#_themis_client[Themis sample client application].
This client application connects directly to the origin cluster, the target cluster, and the {product-proxy}.
It inserts some test data in its own table, and then you can view the results of reads from each source.
Refer to the Themis README for more information.

Alternatively, you could follow this manual procedure:

* Create a small test table on both clusters, for example a simple key/value table (it could be in an existing keyspace, or in one that you create specifically for this test).
For example `CREATE TABLE test_keyspace.test_table(k TEXT PRIMARY KEY, v TEXT);`.
* Use `cqlsh` to connect *directly to the origin cluster*.
Insert a row with any key, and with a value specific to the origin cluster, for example `INSERT INTO test_keyspace.test_table(k, v) VALUES ('1', 'Hello from the origin cluster!');`.
* Now, use `cqlsh` to connect *directly to the target cluster*.
Insert a row with the same key as above, but with a value specific to the target cluster, for example `INSERT INTO test_keyspace.test_table(k, v) VALUES ('1', 'Hello from the target cluster!');`.
* Now, use `cqlsh` to xref:connect-clients-to-proxy.adoc#_connecting_cqlsh_to_the_zdm_proxy[connect to the {product-proxy}], and then issue a read request for this test table: `SELECT * FROM test_keyspace.test_table WHERE k = '1';`.
The result will clearly show you where the read actually comes from.
