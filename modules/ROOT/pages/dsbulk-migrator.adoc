= Use {dsbulk-migrator} with {product-proxy}
:navtitle: Use {dsbulk-migrator}
:description: {dsbulk-migrator} extends {dsbulk-loader} with migration commands.
:page-aliases: ROOT:dsbulk-migrator-overview.adoc

{dsbulk-migrator} is an extension of xref:dsbulk:overview:dsbulk-about.adoc[{dsbulk-loader}] that adds the following three commands:

* `migrate-live`: Immediately runs a live data migration using {dsbulk-loader}.

* `generate-script`: Generates a migration script that you can use to run a data migration with a standalone {dsbulk-loader} installation.
This command _doesn't_ trigger the migration; it only generates the migration script.

* `generate-ddl`: Reads the origin cluster's schema, and then generates CQL files that you can use to recreate the schema on your target cluster in preparation for data migration.

{dsbulk-migrator} is best for smaller migrations and migrations that don't require extensive data validation aside from post-migration row counts.
You might also use this tool for migrations where you can shard data from large tables into more manageable quantities.

You can use {dsbulk-migrator} alone or with {product-proxy}.

== Install {dsbulk-migrator}

. Install Java 11 and https://maven.apache.org/download.cgi[Maven] 3.9.x.

. Optional: If you don't want to use the embedded {dsbulk-loader} that is bundled with {dsbulk-migrator}, you must xref:dsbulk:overview:install.adoc[install {dsbulk-loader}] before installing {dsbulk-migrator}.

. Clone the {dsbulk-migrator-repo}[{dsbulk-migrator} repository]:
+
[source,bash]
----
git clone git@github.com:datastax/dsbulk-migrator.git
----

. Change to the cloned directory:
+
[source,bash]
----
cd dsbulk-migrator
----

. Use Maven to build {dsbulk-migrator}:
+
[source,bash]
----
mvn clean package
----
+
[[dsbulk-jar]]The build produces two distributable fat jars.
You will use one of these jars when you run a {dsbulk-migrator} command.
+
* `dsbulk-migrator-**VERSION**-embedded-dsbulk.jar`: Contains an embedded {dsbulk-loader} installation and an embedded Java driver.
+
Supports all {dsbulk-migrator} operations, but it is larger than the other JAR due to the presence of the {dsbulk-loader} classes.
+
Use this jar if you _don't_ want to use your own {dsbulk-loader} installation.

* `dsbulk-migrator-**VERSION**-embedded-driver.jar`: Contains an embedded Java driver only.
+
Suitable for using the `generate-script` and `migrate-live` commands with your own {dsbulk-loader} installation.
+
You cannot use this jar for `migrate-live` with the embedded {dsbulk-loader} because the required {dsbulk-loader} classes aren't present in this jar.

. https://github.com/datastax/simulacron[Clone and build Simulacron], which is required for some {dsbulk-migrator} integration tests.
+
Note the https://github.com/datastax/simulacron?tab=readme-ov-file#prerequisites[prerequisites for Simulacron], particularly for macOS.

. Run the {dsbulk-migrator} integration tests:
+
[source,bash]
----
mvn clean verify
----

After you install, build, and test {dsbulk-migrator}, you can run it from the command line, specifying your desired jar, command, and options.

For a quick test, try the `<<get-help-for-dsbulk-migrator,--help>>` option.

For information and examples for each command, see the following:

* <<dsbulk-live>>
* <<dsbulk-script>>
* <<dsbulk-ddl>>

[#get-help-for-dsbulk-migrator]
== Get help for {dsbulk-migrator}

Use `--help` (`-h`) to get information about {dsbulk-migrator} commands and options:

* Print the available {dsbulk-migrator} commands:
+
[source,bash]
----
java -jar /path/to/dsbulk-migrator.jar --help
----
+
Replace `/path/to/dsbulk-migrator.jar` with the path to your <<dsbulk-jar,{dsbulk-migrator} fat jar>>.

* Print help for a specific command:
+
[source,bash,subs="+quotes"]
----
java -jar /path/to/dsbulk-migrator.jar **COMMAND** --help
----
+
Replace the following:
+
** `/path/to/dsbulk-migrator.jar`: The path to your <<dsbulk-jar,{dsbulk-migrator} fat jar>>.
** `COMMAND`: The command for which you want to get help, one of `migrate-live`, `generate-script`, or `generate-ddl`.

[#dsbulk-live]
== Run a live migration

The `migrate-live` command immediately runs a live data migration using the embedded version of {dsbulk-loader} or your own {dsbulk-loader} installation.
A _live migration_ means the data migration starts immediately, and it is handled by the migrator tool through the specified {dsbulk-loader} installation.

To run the `migrate-live` command, provide the path to your <<dsbulk-jar,{dsbulk-migrator} fat jar>> followed by `migrate-live` and any options:

[source,bash,subs="+quotes"]
----
java -jar /path/to/dsbulk-migrator.jar migrate-live **OPTIONS**
----

The following examples show how to use either fat jar to perform a live migration where the target cluster is an {astra-db} database.
The password parameters are left blank so that {dsbulk-migrator} prompts for them interactively during the migration.
All unspecified options use their default values.

[tabs]
======
Use the embedded {dsbulk-loader}::
+
--
If you want to run the migration with the embedded {dsbulk-loader}, you must use the `dsbulk-migrator-**VERSION**-embedded-dsbulk.jar` fat jar and the `--dsbulk-use-embedded` option:

[source,bash,subs="+quotes"]
----
    java -jar target/dsbulk-migrator-**VERSION**-embedded-dsbulk.jar migrate-live \
        --data-dir=/path/to/data/dir \
        --dsbulk-use-embedded \
        --dsbulk-log-dir=/path/to/log/dir \
        --export-host=**ORIGIN_CLUSTER_HOSTNAME** \
        --export-username=**ORIGIN_USERNAME** \
        --export-password # Origin password will be prompted \
        --export-dsbulk-option "--connector.csv.maxCharsPerColumn=65536" \
        --export-dsbulk-option "--executor.maxPerSecond=1000" \
        --import-bundle=/path/to/scb.zip \
        --import-username=token \
        --import-password # Application token will be prompted \
        --import-dsbulk-option "--connector.csv.maxCharsPerColumn=65536" \
        --import-dsbulk-option "--executor.maxPerSecond=1000"
----
--

Use your own {dsbulk-loader} installation::
+
--
If you want to run the migration with your own {dsbulk-loader} installation, use the `dsbulk-migrator-**VERSION**-embedded-driver.jar` fat jar, and use the `--dsbulk-cmd` option to specify the path to your {dsbulk-loader} installation:

[source,bash,subs="+quotes"]
----
    java -jar target/dsbulk-migrator-**VERSION**-embedded-driver.jar migrate-live \
        --data-dir=/path/to/data/dir \
        --dsbulk-cmd=${DSBULK_ROOT}/bin/dsbulk \
        --dsbulk-log-dir=/path/to/log/dir \
        --export-host=**ORIGIN_CLUSTER_HOSTNAME** \
        --export-username=**ORIGIN_USERNAME** \
        --export-password # Origin password will be prompted \
        --import-bundle=/path/to/scb.zip \
        --import-username=token \
        --import-password # Application token will be prompted
----

--
======

=== Options for migrate-live

Options for the `migrate-live` command are used to configure the migration parameters and connect to the origin and target clusters.

Most options have sensible default values and don't need to be specified unless you want to override the default value.

[cols="2"]
|===
| Option | Description

| `--dsbulk-cmd=CMD` (`-c`)
| The external {dsbulk-loader} command to use.
Ignored if the embedded {dsbulk-loader} is being used.
The default is simply `dsbulk`, assuming that the command is available through the `PATH` variable contents.

| `--data-dir=PATH` (`-d`)
| The directory where data will be exported to and imported from.
The default is a `data` subdirectory in the current working directory.
The data directory will be created if it does not exist.
Tables will be exported and imported in subdirectories of the data directory specified here.
There will be one subdirectory per keyspace in the data directory, then one subdirectory per table in each keyspace directory.

| `--dsbulk-use-embedded` (`-e`)
| Use the embedded {dsbulk-loader} version instead of an external one.
The default is to use an external {dsbulk-loader} command.

| `--export-bundle=PATH`
| The path to a secure connect bundle to connect to the origin cluster, if that cluster is a {company} {astra-db} cluster.
Options `--export-host` and `--export-bundle` are mutually exclusive.

| `--export-consistency=CONSISTENCY`
| The consistency level to use when exporting data.
The default is `LOCAL_QUORUM`.

| `--export-dsbulk-option=OPT=VALUE`
| An extra {dsbulk-loader} option to use when exporting.
Any valid {dsbulk-loader} option can be specified here, and it will passed as is to the {dsbulk-loader} process.
{dsbulk-loader} options, including driver options, must be passed as `--long.option.name=<value>`.
Short options are not supported.

| `--export-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the origin cluster.
If the port is not specified, it will default to `9042`.
This option can be specified multiple times.
Options `--export-host` and `--export-bundle` are mutually exclusive.

| `--export-max-concurrent-files=NUM\|AUTO`
| The maximum number of concurrent files to write to.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--export-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--export-max-records=NUM`
| The maximum number of records to export for each table.
Must be a positive number or `-1`.
The default is `-1` (export the entire table).

| `--export-password`
| The password to use to authenticate against the origin cluster.
Options `--export-username` and `--export-password` must be provided together, or not at all.
Omit the parameter value to be prompted for the password interactively.

| `--export-splits=NUM\|NC`
| The maximum number of token range queries to generate.
Use the `NC` syntax to specify a multiple of the number of available cores.
For example, `8C` = 8 times the number of available cores.
The default is `8C`.
This is an advanced setting; you should rarely need to modify the default value.

| `--export-username=STRING`
| The username to use to authenticate against the origin cluster.
Options `--export-username` and `--export-password` must be provided together, or not at all.

| `--import-bundle=PATH`
| The path to a {scb} to connect to a target {astra-db} cluster.
Options `--import-host` and `--import-bundle` are mutually exclusive.

| `--import-consistency=CONSISTENCY`
| The consistency level to use when importing data.
The default is `LOCAL_QUORUM`.

| `--import-default-timestamp=<defaultTimestamp>`
| The default timestamp to use when importing data.
Must be a valid instant in ISO-8601 syntax.
The default is `1970-01-01T00:00:00Z`.

| `--import-dsbulk-option=OPT=VALUE`
| An extra {dsbulk-loader} option to use when importing.
Any valid {dsbulk-loader} option can be specified here, and it will passed as is to the {dsbulk-loader} process.
{dsbulk-loader} options, including driver options, must be passed as `--long.option.name=<value>`.
Short options are not supported.

| `--import-host=**HOST_OR_IP**`

`--import-host=**HOST**:**PORT**`
| The host name or IP and, optionally, the port of a node on the target cluster.
If the port is not specified, it will default to `9042`.
This option can be specified multiple times.
Options `--import-host` and `--import-bundle` are mutually exclusive.

| `--import-max-concurrent-files=**NUMBER**`

`--import-max-concurrent-files=**AUTO**`
| The maximum number of concurrent files to read from.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--import-max-concurrent-queries=**NUMBER**`

`--import-max-concurrent-queries=**AUTO**`
| The maximum number of concurrent queries to execute.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--import-max-errors=**NUMBER**`
| The maximum number of failed records to tolerate when importing data.
The default is `1000`.
Failed records will appear in a `load.bad` file in the {dsbulk-loader} operation directory.

| `--import-password`
| The password to use to authenticate against the target cluster.
Options `--import-username` and `--import-password` must be provided together, or not at all.
Omit the parameter value to be prompted for the password interactively.

| `--import-username=STRING`
| The username to use to authenticate against the target cluster. Options `--import-username` and `--import-password` must be provided together, or not at all.

| `--keyspaces=REGEX` (`-k`)
| A regular expression to select keyspaces to migrate.
The default is to migrate all keyspaces except system keyspaces, {dse-short}-specific keyspaces, and the OpsCenter keyspace.
Case-sensitive keyspace names must be entered in their exact case.

| `--dsbulk-log-dir=PATH` (`-l`)
| The directory where the {dsbulk-loader} should store its logs.
The default is a `logs` subdirectory in the current working directory.
This subdirectory will be created if it does not exist.
Each {dsbulk-loader} operation will create a subdirectory in the log directory specified here.

| `--max-concurrent-ops=NUM`
| The maximum number of concurrent operations (exports and imports) to carry.
The default is `1`.
Set this to higher values to allow exports and imports to occur concurrently.
For example, with a value of `2`, each table will be imported as soon as it is exported, while the next table is being exported.

| `--skip-truncate-confirmation`
| Skip truncate confirmation before actually truncating tables.
Only applicable when migrating counter tables, ignored otherwise.

| `--tables=REGEX` (`-t`)
| A regular expression to select tables to migrate.
The default is to migrate all tables in the keyspaces that were selected for migration with `--keyspaces`.
Case-sensitive table names must be entered in their exact case.

| `--table-types=regular\|counter\|all`
| The table types to migrate.
The default is `all`.

| `--truncate-before-export`
| Truncate tables before the export instead of after.
The default is to truncate after the export.
Only applicable when migrating counter tables, ignored otherwise.

| `--dsbulk-working-dir=PATH` (`-w`)
| The directory where `dsbulk` should be executed.
Ignored if the embedded {dsbulk-loader} is being used.
If unspecified, it defaults to the current working directory.

|===

[#dsbulk-script]
== Generate a migration script

The `generate-script` command generates a migration script that you can use to perform a data migration with your own {dsbulk-loader} installation.
This command _doesn't_ trigger the migration; it only generates the migration script that you must then run.

To run the `generate-script` command, provide the path to your <<dsbulk-jar,{dsbulk-migrator} fat jar>> followed by `generate-script` and any options:

[source,bash,subs="+quotes"]
----
java -jar /path/to/dsbulk-migrator.jar generate-script **OPTIONS**
----

The following example generates a migration script where the target cluster is an {astra-db} database.
All unspecified options use their default values.

[source,bash,subs="+quotes"]
----
    java -jar target/dsbulk-migrator-**VERSION**-embedded-driver.jar generate-script \
        --data-dir=/path/to/data/dir \
        --dsbulk-cmd=${DSBULK_ROOT}/bin/dsbulk \
        --dsbulk-log-dir=/path/to/log/dir \
        --export-host=**ORIGIN_CLUSTER_HOSTNAME** \
        --export-username=**ORIGIN_USERNAME** \
        --export-password=**ORIGIN_PASSWORD** \
        --import-bundle=/path/to/scb.zip \
        --import-username=token \
        --import-password=**ASTRA_APPLICATION_TOKEN**
----

=== Options for generate-script

When generating a migration script, most options become default values in the generated scripts.
However, {dsbulk-migrator} needs to access the origin cluster to gather metadata about the tables to migrate.

The following options are available for the `generate-script` command.
Most options have sensible default values and don't need to be specified unless you want to override the default value.

[cols="2"]
|===
| Option | Description

| `--dsbulk-cmd=CMD` (`-c`)
| The {dsbulk-loader} command to use.
The default is simply `dsbulk`, assuming that the command is available through the `PATH` variable contents.

| `--data-dir=PATH` (`-d`)
| The directory where data will be exported to and imported from. 
The default is a `data` subdirectory in the current working directory. 
The data directory will be created if it does not exist. 

| `--export-bundle=PATH`
| The path to a secure connect bundle to connect to the origin cluster, if that cluster is a {company} {astra-db} cluster.
Options `--export-host` and `--export-bundle` are mutually exclusive.

| `--export-consistency=CONSISTENCY`
| The consistency level to use when exporting data.
The default is `LOCAL_QUORUM`.

| `--export-dsbulk-option=OPT=VALUE`
| An extra {dsbulk-loader} option to use when exporting.
Any valid {dsbulk-loader} option can be specified here, and it will passed as is to the {dsbulk-loader} process.
{dsbulk-loader} options, including driver options, must be passed as `--long.option.name=<value>`.
Short options are not supported.

| `--export-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the origin cluster.
If the port is not specified, it will default to `9042`.
This option can be specified multiple times.
Options `--export-host` and `--export-bundle` are mutually exclusive.

| `--export-max-concurrent-files=NUM\|AUTO`
| The maximum number of concurrent files to write to.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--export-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--export-max-records=NUM`
| The maximum number of records to export for each table.
Must be a positive number or `-1`.
The default is `-1` (export the entire table).

| `--export-password`
| The password to use to authenticate against the origin cluster.
Options `--export-username` and `--export-password` must be provided together, or not at all.
Omit the parameter value to be prompted for the password interactively.

| `--export-splits=NUM\|NC`
| The maximum number of token range queries to generate.
Use the `NC` syntax to specify a multiple of the number of available cores.
For example, `8C` = 8 times the number of available cores.
The default is `8C`.
This is an advanced setting.
You should rarely need to modify the default value.

| `--export-username=STRING`
| The username to use to authenticate against the origin cluster.
Options `--export-username` and `--export-password` must be provided together, or not at all.

| `--import-bundle=PATH`
| The path to a Secure Connect Bundle to connect to a target {astra-db} cluster.
Options `--import-host` and `--import-bundle` are mutually exclusive.

| `--import-consistency=CONSISTENCY`
| The consistency level to use when importing data.
The default is `LOCAL_QUORUM`.

| `--import-default-timestamp=<defaultTimestamp>`
| The default timestamp to use when importing data.
Must be a valid instant in ISO-8601 syntax.
The default is `1970-01-01T00:00:00Z`.

| `--import-dsbulk-option=OPT=VALUE`
| An extra {dsbulk-loader} option to use when importing.
Any valid {dsbulk-loader} option can be specified here, and it will passed as is to the {dsbulk-loader} process.
{dsbulk-loader} options, including driver options, must be passed as `--long.option.name=<value>`.
Short options are not supported.

| `--import-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node on the target cluster.
If the port is not specified, it will default to `9042`.
This option can be specified multiple times.
Options `--import-host` and `--import-bundle` are mutually exclusive.

| `--import-max-concurrent-files=NUM\|AUTO`
| The maximum number of concurrent files to read from.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--import-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute.
Must be a positive number or the special value `AUTO`.
The default is `AUTO`.

| `--import-max-errors=NUM`
| The maximum number of failed records to tolerate when importing data.
The default is `1000`.
Failed records will appear in a `load.bad` file in the {dsbulk-loader} operation directory.

| `--import-password`
| The password to use to authenticate against the target cluster.
Options `--import-username` and `--import-password` must be provided together, or not at all.
Omit the parameter value to be prompted for the password interactively.

| `--import-username=STRING`
| The username to use to authenticate against the target cluster.
Options `--import-username` and `--import-password` must be provided together, or not at all.

| `--keyspaces=REGEX` (`-k`)
| A regular expression to select keyspaces to migrate.
The default is to migrate all keyspaces except system keyspaces, {dse-short}-specific keyspaces, and the OpsCenter keyspace.
Case-sensitive keyspace names must be entered in their exact case.

| `--dsbulk-log-dir=PATH` (`-l`)
| The directory where {dsbulk-loader} should store its logs.
The default is a `logs` subdirectory in the current working directory.
This subdirectory will be created if it does not exist.
Each {dsbulk-loader} operation will create a subdirectory in the log directory specified here.

| `--tables=REGEX` (`-t`)
| A regular expression to select tables to migrate.
The default is to migrate all tables in the keyspaces that were selected for migration with `--keyspaces`.
Case-sensitive table names must be entered in their exact case.

| `--table-types=regular\|counter\|all`
| The table types to migrate. The default is `all`.

|===

[#dsbulk-ddl]
== Generate DDL files

The `generate-ddl` command reads the origin cluster's schema, and then generates CQL files that you can use to recreate the schema on your target CQL-compatible cluster.

To run the `generate-ddl` command, provide the path to your <<dsbulk-jar,{dsbulk-migrator} fat jar>> followed by `generate-ddl` and any options:

[source,bash,subs="+quotes"]
----
java -jar /path/to/dsbulk-migrator.jar generate-ddl **OPTIONS**
----

The following example generates DDL files that are optimized for recreating the schema on an {astra-db} database:

[source,bash,subs="+quotes"]
----
    java -jar target/dsbulk-migrator-**VERSION**-embedded-driver.jar generate-ddl \
        --data-dir=/path/to/data/directory \
        --export-host=**ORIGIN_CLUSTER_HOSTNAME** \
        --export-username=**ORIGIN_USERNAME** \
        --export-password=**ORIGIN_PASSWORD** \
        --optimize-for-astra
----

=== Options for generate-ddl

The `generate-ddl` command ignores all `import-{asterisk}` options and {dsbulk-loader}-related options because they aren't relevant to this operation.

Origin cluster connection details (`export-{asterisk}` options) are required so that {dsbulk-migrator} can access the origin cluster to gather metadata about the keyspaces and tables for the DDL statements.

Most options have sensible default values and don't need to be specified unless you want to override the default value.

[cols="2"]
|===
| Option | Description

| `--optimize-for-astra` (`-a`)
| Produce CQL scripts optimized for {company} {astra-db}.
{astra-db} does not allow some options in DDL statements.
Using this {dsbulk-migrator} command option, forbidden {astra-db} options will be omitted from the generated CQL files.

| `--data-dir=PATH` (`-d`)
| The directory where data will be exported to and imported from.
The default is a `data` subdirectory in the current working directory.
The data directory will be created if it does not exist.

| `--export-bundle=PATH`
| The path to a secure connect bundle to connect to the origin cluster, if that cluster is a {company} {astra-db} cluster.
Options `--export-host` and `--export-bundle` are mutually exclusive.

| `--export-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the origin cluster.
If the port is not specified, it will default to `9042`.
This option can be specified multiple times.
Options `--export-host` and `--export-bundle` are mutually exclusive.

| `--export-password`
| The password to use to authenticate against the origin cluster.
Options `--export-username` and `--export-password` must be provided together, or not at all.
Omit the parameter value to be prompted for the password interactively.

| `--export-username=STRING`
| The username to use to authenticate against the origin cluster.
Options `--export-username` and `--export-password` must be provided together, or not at all.

| `--keyspaces=REGEX` (`-k`)
| A regular expression to select keyspaces to migrate.
The default is to migrate all keyspaces except system keyspaces, {dse-short}-specific keyspaces, and the OpsCenter keyspace.
Case-sensitive keyspace names must be entered in their exact case.

| `--tables=REGEX` (`-t`)
| A regular expression to select tables to migrate.
The default is to migrate all tables in the keyspaces that were selected for migration with `--keyspaces`.
Case-sensitive table names must be entered in their exact case.

| `--table-types=regular\|counter\|all`
| The table types to migrate.
The default is `all`.

|===

== See also

* xref:dsbulk:reference:dsbulk-cmd.adoc#escape-and-quote-command-line-arguments[Escape and quote {dsbulk-loader} command line arguments]