= Components
:page-tag: migration,zdm,zero-downtime,zdm-proxy,components

The main component of the {company} {product} product suite is **{product-proxy}**, which by design is a simple and lightweight proxy that handles all the real-time requests generated by your client applications.

{product-proxy} is open-source software (OSS) and available in its https://github.com/datastax/zdm-proxy[Public GitHub repo].
You can view the source files and contribute code for potential inclusion via Pull Requests (PRs) initiated on a fork of the repo.

The {product-proxy} itself doesn't have any capability to migrate data or knowledge that a migration may be ongoing, and it is not coupled to the migration process in any way.

* {company} {product} also provides the **{product-utility}** and **{product-automation}** to set up and run the Ansible playbooks that deploy and manage the {product-proxy} and its monitoring stack.

* Multiple data migration tools such as **{cass-migrator}** and **{dsbulk-migrator}** are available.

== Role of {product-proxy}

{company} created {product-proxy} to function between the application and both the origin and target databases.
The databases can be any CQL-compatible data store, such as {cass-reg}, {dse}, and {astra-db}.
The proxy always sends every write operation (Insert, Update, Delete) synchronously to both clusters at the desired Consistency Level:

* If the write is successful in both clusters, it returns a successful acknowledgement to the client application.
* If the write fails on either cluster, the failure is passed back to the client application so that it can retry it as appropriate, based on its own retry policy.

This design ensures that new data is always written to both clusters, and that any failure on either cluster is always made visible to the client application.
{product-proxy} also sends all reads to the primary cluster, and then returns the result to the client application.
The primary cluster is initially the origin cluster, and you change it to the target cluster at the end of the migration process.

{product-proxy} is designed to be highly available. It can be scaled horizontally, so typical deployments are made up of a minimum of 3 servers.
{product-proxy} can be restarted in a rolling fashion, for example, to change configuration for different phases of the migration.

[TIP]
====
{product-proxy} has been designed to run in a **clustered** fashion so that it is never a single point of failure.
Unless it is for a demo or local testing environment, a {product-proxy} deployment should always comprise multiple {product-proxy} instances.

The term {product-proxy} indicates the whole deployment, and {product-proxy} instance refers to an individual proxy process in the deployment.
====

=== Key features of {product-proxy}

* Allows you to lift-and-shift existing application code from your origin cluster to your target cluster by changing only the connection string.

* Reduces risks to upgrades and migrations by decoupling the origin cluster from the target cluster, and allowing you to determine an explicit cut-over point once you're ready to commit to using the target cluster permanently.

* Bifurcates writes synchronously to both clusters during the migration process.

* Returns (for read operations) the response from the primary cluster, which is its designated source of truth.
During a migration, the primary cluster is typically the origin cluster.
Near the end of the migration, you shift the primary cluster to be the target cluster.

* Can be configured to also read asynchronously from the target cluster.
This capability is called **Asynchronous Dual Reads** (also known as **Read Mirroring**), and it allows you to observe what read latencies and throughput the target cluster can achieve under the actual production load.
** Results from the asynchronous reads executed on the target cluster are not sent back to the client application.
** This design implies that a failure on asynchronous reads from the target cluster does not cause an error on the client application.
** Asynchronous dual reads can be enabled and disabled dynamically with a rolling restart of the {product-proxy} instances.

[NOTE]
====
When using Asynchronous Dual Reads, any additional read load on the target cluster may impact its ability to keep up with writes.
This behavior is expected and desired.
The idea is to mimic the full read and write load on the target cluster so there are no surprises during the last migration phase; that is, after cutting over completely to the target cluster.
====

=== {product-utility} and {product-automation}

https://www.ansible.com/[Ansible] is a suite of software tools that enables infrastructure as code.
It is open source and its capabilities include software provisioning, configuration management, and application deployment functionality.

The Ansible automation for {product-short} is organized into playbooks, each implementing a specific operation.
The machine from which the playbooks are run is known as the Ansible Control Host.
In {product-short}, the Ansible Control Host will run as a Docker container.

You will use the **{product-utility}** to set up Ansible in a Docker container, and **{product-automation}** to run the Ansible playbooks from the Docker container created by {product-utility}.
In other words,the {product-utility} creates the Docker container acting as the **Ansible Control Host**, from which the {product-automation} allows you to deploy and manage the {product-proxy} instances and the associated monitoring stack - Prometheus metrics and Grafana visualization of the metric data.

{product-utility} and {product-automation} expect that you have already provisioned the recommended infrastructure, as outlined in xref:deployment-infrastructure.adoc[].

The source for both of these tools are in a public repo.

For details, see:

* xref:setup-ansible-playbooks.adoc[]
* xref:deploy-proxy-monitoring.adoc[]

== Data migration tools

As part of the overall migration process, you can use {cass-migrator} and/or {dsbulk-migrator} to migrate your data.
Other technologies such as Apache Spark(TM) can be used to write your own custom data migration process.

=== {cass-migrator}

[TIP]
====
To use {cass-migrator}, the schema on your origin and target clusters must match.
====

Use {cass-migrator} to:

* Migrate your data from any CQL-supported origin cluster to any CQL-supported target cluster. 
Examples of databases that support CQL are {cass-reg}, {dse}, and {astra-db}.
* Validate migration accuracy and performance using examples that provide a smaller, randomized data set.
* Preserve internal `writetime` timestamps and Time To Live (TTL) values.
* Take advantage of advanced data types (Sets, Lists, Maps, UDTs).
* Filter records from the origin cluster's data, using {cass-short}'s internal `writetime` timestamp.
* Use SSL Support, including custom cipher algorithms.

{cass-migrator} is designed to:

* Connect to and compare your target database/cluster with the origin database/cluster.
* Report differences in a detailed log file.
* Optionally reconcile any missing records and fix any data inconsistencies in the target cluster by enabling `autocorrect` in a config file.

=== {dsbulk-migrator}

You can also take advantage of {dsbulk-migrator} to migrate smaller sets of data. 

For more about both tools, see xref:migrate-and-validate-data.adoc[].
