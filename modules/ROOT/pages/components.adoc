= Compare {company} migration tools
:navtitle: Compare migration tools
:description: Learn about {company} migration tools.
:page-tag: migration,zdm,zero-downtime,zdm-proxy,components

{company} migration tools include the {product} {product-short} toolkit and three data migration tools.

{product-short} is comprised of {product-proxy}, {product-utility}, and {product-automation}, which orchestrate activity-in-transition on your clusters.
To move and validate data, you use {sstable-sideloader}, {cass-migrator}, or {dsbulk-migrator}.

You can also use {sstable-sideloader}, {cass-migrator-short}, and {dsbulk-migrator} on their own, outside the context of {product-short}.

== {product-proxy}

The main component of the {company} {product} toolkit is {product-proxy}, which is designed to be a lightweight proxy that handles all real-time requests generated by your client applications during the migration process.

{product-proxy} is open-source software that is available from the {product-proxy-repo}[zdm-proxy GitHub repo].
This project is open for public contributions.

The {product-proxy} is an orchestrator for monitoring application activity and keeping multiple clusters in sync through dual writes.
{product-proxy} isn't linked to the actual migration process.
It doesn't perform data migrations and it doesn't have awareness of ongoing migrations.
Instead, you use a data migration tool, like {sstable-sideloader}, {cass-migrator}, or {dsbulk-migrator}, to perform the data migration and validate migrated data.

=== How {product-proxy} works

{company} created {product-proxy} to function between the application and both the origin and target databases.
The databases can be any CQL-compatible data store, such as {cass-reg}, {dse}, and {astra-db}.
The proxy always sends every write operation (Insert, Update, Delete) synchronously to both clusters at the desired Consistency Level:

* If the write is successful in both clusters, it returns a successful acknowledgement to the client application.
* If the write fails on either cluster, the failure is passed back to the client application so that it can retry it as appropriate, based on its own retry policy.

This design ensures that new data is always written to both clusters, and that any failure on either cluster is always made visible to the client application.
{product-proxy} also sends all reads to the primary cluster, and then returns the result to the client application.
The primary cluster is initially the origin cluster, and you change it to the target cluster at the end of the migration process.

{product-proxy} is designed to be highly available. It can be scaled horizontally, so typical deployments are made up of a minimum of 3 servers.
{product-proxy} can be restarted in a rolling fashion, for example, to change configuration for different phases of the migration.

=== Key features of {product-proxy}

* Allows you to lift-and-shift existing application code from your origin cluster to your target cluster by changing only the connection string, if all else is compatible.

* Reduces risks to upgrades and migrations by decoupling the origin cluster from the target cluster.
You can determine an explicit cut-over point once you're ready to commit to using the target cluster permanently.

* Bifurcates writes synchronously to both clusters during the migration process.

* Read operations return the response from the primary (origin) cluster, which is its designated source of truth.
+
During a migration, the primary cluster is typically the origin cluster.
Near the end of the migration, you shift the primary cluster to be the target cluster.

* Option to read asynchronously from the target cluster as well as the origin cluster
This capability is called **Asynchronous Dual Reads** or **Read Mirroring**, and it allows you to observe what read latencies and throughput the target cluster can achieve under the actual production load.
+
** Results from the asynchronous reads executed on the target cluster are not sent back to the client application.
** This design implies that a failure on asynchronous reads from the target cluster does not cause an error on the client application.
** Asynchronous dual reads can be enabled and disabled dynamically with a rolling restart of the {product-proxy} instances.

[NOTE]
====
When using Asynchronous Dual Reads, any additional read load on the target cluster may impact its ability to keep up with writes.
This behavior is expected and desired.
The idea is to mimic the full read and write load on the target cluster so there are no surprises during the last migration phase; that is, after cutting over completely to the target cluster.
====

=== Run multiple {product-proxy} instances

{product-proxy} has been designed to run in a clustered fashion so that it is never a single point of failure.
Unless it is for a demo or local testing environment, a {product-proxy} deployment should always comprise multiple {product-proxy} instances.

Throughout the documentation, the term _{product-proxy} deployment_ refers to the entire deployment, and _{product-proxy} instance_ refers to an individual proxy process in the deployment.

You can use the {product-utility} and {product-automation} to set up and run Ansible playbooks that deploy and manage {product-proxy} and its monitoring stack.

== {product-utility} and {product-automation}

You can use the {product-automation-repo}[{product-utility} and {product-automation}] to set up and run Ansible playbooks that deploy and manage {product-proxy} and its monitoring stack.

https://www.ansible.com/[Ansible] is a suite of software tools that enables infrastructure as code.
It is open source and its capabilities include software provisioning, configuration management, and application deployment functionality.
The Ansible automation for {product-short} is organized into playbooks, each implementing a specific operation.
The machine from which the playbooks are run is known as the Ansible Control Host.
In {product-short}, the Ansible Control Host runs as a Docker container.

You use the {product-utility} to set up Ansible in a Docker container, and then you use {product-automation} to run the Ansible playbooks from the Docker container created by {product-utility}.

The {product-utility} creates the Docker container acting as the Ansible Control Host, from which {product-automation} allows you to deploy and manage the {product-proxy} instances and the associated monitoring stack, which includes Prometheus metrics and Grafana visualizations of the metrics data.

To use {product-utility} and {product-automation}, you must prepare the recommended infrastructure, as explained in xref:deployment-infrastructure.adoc[].

For more information, see xref:setup-ansible-playbooks.adoc[] and xref:deploy-proxy-monitoring.adoc[].

include::ROOT:migrate-and-validate-data.adoc[tags=migration-tool-summaries]