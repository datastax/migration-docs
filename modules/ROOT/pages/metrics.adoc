= Monitor {product-proxy}

{product-proxy} can gather a large number of metrics that provide you with insight into its health and operations, communication with client applications and clusters, and request handling.

This visibility is important to your migration.
It builds confidence in the health of your deployment, and it helps you investigate errors or performance degradation.

{company} strongly recommends that you use {product-automation} to deploy the {product-proxy} monitoring stack, or import the pre-built Grafana dashboards into your own monitoring infrastructure.

Do this after you xref:ROOT:setup-ansible-playbooks.adoc[set up {product-automation}] and xref:ROOT:deploy-proxy-monitoring.adoc[deploy the {product-proxy} instances].

== Deploy the monitoring stack

{product-automation} enables you to easily set up a self-contained monitoring stack that is preconfigured to collect metrics from your {product-proxy} instances and display them in ready-to-use Grafana dashboards.

The monitoring stack is deployed entirely on Docker.
It includes the following components, all deployed as Docker containers:

* Prometheus node exporter, which runs on each {product-proxy} host and makes OS- and host-level metrics available to Prometheus.
* Prometheus server, to collect metrics from {product-proxy}, its Golang runtime, and the Prometheus node exporter.
* Grafana, to visualize the metrics in preconfigured dashboards.

After running the monitoring playbook, you will have a fully configured monitoring stack connected to your {product-proxy} deployment.

Aside from xref:ROOT:deployment-infrastructure.adoc[preparing the infrastructure], you don't need to install any {product-proxy} dependencies on the monitoring host machine. The playbook automatically installs all required software packages.

. Connect to the Ansible Control Host Docker container.
You can do this from the jumphost machine by running the following command:
+
[source,bash]
----
docker exec -it zdm-ansible-container bash
----
+
.Result
[%collapsible]
====
[source,bash]
----
ubuntu@52772568517c:~$
----
====

. To configure the Grafana credentials, edit the `zdm_monitoring_config.yml` file that is located at `zdm-proxy-automation/ansible/vars`:
+
* `grafana_admin_user`: Leave unset or as the default value `admin`.
If unset, it defaults to `admin`.
* `grafana_admin_password`: Provide a password.
Make note of this password because you need it later.

. Optional: Use the `metrics_port` variable to change the xref:ROOT:deploy-proxy-monitoring.adoc#ports[metrics collection port].
The default port for metrics collection is `14001`.

. Edit any other configuration settings as needed.
+
[IMPORTANT]
====
These steps assume you will deploy the monitoring stack on the jumphost machine.
If you plan to deploy the monitoring stack on another machine, you must set the configuration accordingly so the playbook runs against the correct machine from the Ansible Control Host.
====

. Make sure you are in the `ansible` directory at `/home/ubuntu/zdm-proxy-automation/ansible`, and then run the monitoring playbook:
+
[source,bash]
----
ansible-playbook deploy_zdm_monitoring.yml -i zdm_ansible_inventory
----

. Wait while the playbook runs.

. To verify that the stack is running, check the Grafana dashboard at `http://**MONITORING_HOST_PUBLIC_IP**:3000`.
+
If you deployed the monitoring stack on the jumphost machine, replace `**MONITORING_HOST_PUBLIC_IP**` with the public IP address or hostname of the jumphost.
+
If you deployed your monitoring stack on another machine, replace `**MONITORING_HOST_PUBLIC_IP**` with the public IP address or hostname of that machine.

. Login with your Grafana `username` and `password`.

[#call-the-liveliness-and-readiness-endpoints]
== Call the `liveness` and `readiness` HTTP endpoints

{product-short} metrics provide `/health/liveness` and `/health/readiness` HTTP endpoints, that you can call to determine the state of the {product-proxy} instances.

For example, after deploying the {product-proxy} monitoring stack, you can use the `liveness` and `readiness` HTTP endpoints to confirm that your {product-proxy} instances are running.

[tabs]
======
Liveliness endpoint::
+
--
[source,plaintext,subs="+quotes"]
----
http://**ZDM_PROXY_PRIVATE_IP**:**METRICS_PORT**/health/liveness
----

Replace the following:

* `**METRICS_PORT**`: The `metrics_port` you set in the `zdm_monitoring_config.yml` file.
The default port is 14001.

* `**ZDM_PROXY_PRIVATE_IP**`: The private IP address, hostname, or other valid identifier for the {product-proxy} instance you want to check.

Example request with variables:

[source,bash]
----
curl -G "http://{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address'] }}:{{ metrics_port }}/health/liveliness"
----

Example request with plaintext values:

[source,bash]
----
curl -G "http://172.18.10.40:14001/health/liveliness"
----

--

Readiness endpoint::
+
--
[source,plaintext,subs="+quotes"]
----
http://**ZDM_PROXY_PRIVATE_IP**:**METRICS_PORT**/health/readiness
----

Replace the following:

* `**METRICS_PORT**`: The `metrics_port` you set in the `zdm_monitoring_config.yml` file.
The default port is 14001.

* `**ZDM_PROXY_PRIVATE_IP**`: The private IP address, hostname, or other valid identifier for the {product-proxy} instance you want to check.

Example request with variables:

[source,bash]
----
curl -G "http://{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address'] }}:{{ metrics_port }}/health/readiness"
----

Example request with plaintext values:

[source,bash]
----
curl -G "http://172.18.10.40:14001/health/readiness"
----

Example result:

[source,json]
----
{
   "OriginStatus":{
      "Addr":"<origin_node_addr>",
      "CurrentFailureCount":0,
      "FailureCountThreshold":1,
      "Status":"UP"
   },
   "TargetStatus":{
      "Addr":"<target_node_addr>",
      "CurrentFailureCount":0,
      "FailureCountThreshold":1,
      "Status":"UP"
   },
   "Status":"UP"
}
----

--
======

== Inspect {product-proxy} metrics

{product-proxy} exposes an HTTP endpoint that returns metrics in the Prometheus format.

After you deploy the monitoring stack, the {product-proxy} Grafana dashboards automatically start displaying these metrics as they are scraped from the instances.

If you already have a Grafana deployment, then you can import the dashboards from the {product-automation-repo}/tree/main/grafana-dashboards[{product-short} dashboard files] in the {product-short} GitHub repository.

=== Grafana dashboard for {product-proxy} metrics

The {product-proxy} metrics dashboard includes proxy level metrics, node level metrics, and asynchronous read requests metrics.

image::zdm-grafana-proxy-dashboard1.png[Grafana dashboard shows three categories of {product-short} metrics for the proxy.]

This dashboard can help you identify issues such as the following:

* If the number of client connections is near 1000 per {product-proxy} instance, be aware that {product-proxy} will start rejecting client connections after accepting 1000 connections.
* If Prepared Statement cache metrics are always increasing, check the **entries** and **misses** metrics.
* If you have error metrics reporting for specific error types, evaluate these issues on a case-by-case basis.

[#proxy-level-metrics]
==== Proxy-level metrics

* **Latency**:
+
** **Read Latency**: Total latency measured by {product-proxy} per read request, including post-processing, such as response aggregation.
This metric has two labels: `reads_origin` and `reads_target`.
The label that has data depends on which cluster is receiving the reads, which is the current xref:ROOT:faqs.adoc#what-are-origin-target-primary-and-secondary-clusters[primary cluster].
** **Write Latency**: Total latency measured by {product-proxy} per write request, including post-processing, such as response aggregation.
This metric is measured as the total latency across both clusters for a single xref:ROOT:components.adoc#how-zdm-proxy-handles-reads-and-writes[bifurcated write request].

* **Throughput**: Follows the same structure as the latency metrics but measures **Read Throughput** and **Write Throughput**.

* **In-flight requests**

* **Number of client connections**

* **Prepared Statement cache**:
+
** **Cache Misses**: If a prepared statement is sent to {product-proxy} but the statement's `preparedID` isn't present in the node's cache, then {product-proxy} sends an `UNPREPARED` response to the client to reprepare the statement.
This metric tracks the number of times this happens.
** **Number of cached prepared statements**

* **Request Failure Rates**: The number of request failures per interval.
You can set the interval with the **Error Rate interval** dashboard setting.
+
** **Connect Failure Rate**: One `cluster` label with two settings (`origin` and `target`) that represent the cluster to which the connection attempt failed.
** **Read Failure Rate**: One `cluster` label with two settings (`origin` and `target`).
The label that contains data depends on which cluster is currently considered the primary, the same as the latency and throughput metrics explained above.
** **Write Failure Rate**: One `failed_on` label with three settings, `origin`, `target`, and `both`:
+
*** `failed_on=origin`: The write request failed on the origin only.
*** `failed_on=target`: The write request failed on the target only.
*** `failed_on=both`: The write request failed on both the origin and target clusters.

* **Request Failure Counters**: Number of total request failures.
Resets if the {product-proxy} instance restarts.
+
** **Connect Failure Counters**: Has the same labels as the connect failure rate.
** **Read Failure Counters**: Has the same labels as the read failure rate.
** **Write Failure Counters**: Has the same labels as the write failure rate.

For error metrics by error type, see the <<_node_level_metrics,node-level error metrics>>.

[[_node_level_metrics]]
==== Node-level metrics

* **Latency**: Node-level latency metrics report combined read and write latency per cluster, not per request.
For latency by request type, see <<proxy-level-metrics>>.
+
** **Origin**: Latency, as measured by {product-proxy}, up to the point that it received a response from the origin connection.
** **Target**: Latency, as measured by {product-proxy}, up to the point it received a response from the target connection.

* **Throughput**: Same as the node-level latency metrics.
Reads and writes are combined.

* **Number of connections per origin node and per target node**

* **Number of Used Stream Ids**: Tracks the total number of used xref:manage-proxy-instances.adoc#zdm_proxy_max_stream_ids[stream IDs] (request IDs) per connection type (`Origin`, `Target`, and `Async`).

* **Number of errors per error type per origin node and per target node**:
Possible values for the `error` type label:
+
** `error=client_timeout`
** `error=read_failure`
** `error=read_timeout`
** `error=write_failure`
** `error=write_timeout`
** `error=overloaded`
** `error=unavailable`
** `error=unprepared`

[[_asynchronous_read_requests_metrics]]
==== Asynchronous read requests metrics

These metrics are recorded only if you xref:ROOT:enable-async-dual-reads.adoc[enable asynchronous dual reads].

These metrics track the following information for asynchronous read requests:

* *Latency*
* *Throughput*
* *Number of dedicated connections per node for the cluster receiving the asynchronous read requests*
* *Number of errors per node, separated by error type*

=== Go runtime metrics dashboard

The Go runtime metrics dashboard is used less often than the {product-proxy} metrics dashboard.

This dashboard can be helpful for troubleshooting {product-proxy} performance issues.
It provides metrics for memory usage, Garbage Collection (GC) duration, open FDs (file descriptors), and the number of goroutines.

image::zdm-golang-dashboard.png[Golang metrics dashboard example is shown.]

Watch for the following problem areas on the Go runtime metrics dashboard:

* An always increasing **open fds** metric; this can indicate leaked connections.
* GC latencies that are frequently near or above hundreds of milliseconds.
* Persistently increasing memory usage.
* Persistently increasing number of goroutines.

=== System-level metrics dashboard

The {product-short} monitoring stack's system-level dashboard metrics are collected through the Prometheus Node Exporter.
This dashboard contains hardware and OS-level metrics for the host on which the proxy runs.
This can be useful to check the available resources and identify low-level bottlenecks or issues.

== Next steps

To continue Phase 1 of the migration, xref:ROOT:connect-clients-to-proxy.adoc[connect your client applications to {product-proxy}].