= Introduction to Zero Downtime Migrations

Enterprises today depend on the ability to reliably migrate mission-critical apps and data to cloud environments with zero downtime during the migration.

At DataStax, we've developed a set of thoroughly-tested self-service tools, automation scripts, examples, and documented procedures that walk you through well-defined migration phases.

We call this feature {company} {zdm-product} ({zdm-shortproduct}). 

ZDM provides a simple and reliable way for you to migrate an existing Apache Cassandra&reg; or DataStax Enterprise (DSE) cluster to Astra DB, or to any Cassandra/DSE cluster, without any interruption of service to the apps and data. For example:

* You can move your application to Astra DB, DataStax Enterprise or Apache Cassandra with no downtime and with minimal configuration changes
* Your clusters will be kept in sync at all times by a **dual-write logic** configuration
* You can easily rollback at any point, for complete peace of mind

== Migration scenarios

You can migrate:

* From an existing self-managed Cassandra or DSE cluster to cloud-native Astra DB. For example:
** Cassandra 2.1.x, 3.11.x or 4.0.x to Astra DB
** DSE 4.8.x, 5.1.x or 6.8.x to Astra DB
** And more
* From an existing Cassandra or DSE cluster to another Cassandra or DSE cluster. For example:
** Cassandra 2.1.x or 3.11.x to 4.0.x
** DSE 4.8.x or 5.1.x to DSE 6.8.x
** Cassandra 2.1.x, 3.11.x or 4.0.x to DSE 6.8.x
** DSE 4.8.x to Cassandra 4.0.x
** And more
* From Astra DB Classic to Astra DB Serverless

== Migration phases

Your migration project can occur through a sequence of phases, with zero downtime, as illustrated in this high-level view.

First, a few important terms:

* The **Origin** cluster is your existing Cassandra-based environment, whether it's open-source Apache Cassandra or DSE.
* The **Target** cluster is the new environment to which you want to migrate apps and data with zero downtime.

image:zdm-migration-phases4.png[Migration phases from start to finish]

. Connecting your client applications to the {zdm-proxy}. This activates the dual-write logic: writes will be "bifurcated" (meaning, sent both to Origin and Target), while reads will be executed on Origin only.
. Migrating your existing data using the Database Migrator.
. Validating that the migrated data is correct, while continuing to perform dual writes.
. Changing the proxy configuration to route reads to Target, effectively using Target as the source of truth while still keeping Origin in sync.
. Moving your client applications off the {zdm-proxy} and connecting them directly to Target.

== Migration workflow

Here's a diagram to illustrate the overall migration strategy when moving apps and data from your Origin to your Target. In this example, we're looking at a migration from a generic Origin cluster to an Astra DB Target cluster.

image:zdm-workflow1.png[Migration workflow from client application to {zdm-proxy} with dual writes to Origin and Target clusters]

* With no changes required to your client application code itself, {zdm-proxy} does the work to route writes to Origin and Target.
* Cassandra Data Migrator and/or DSBulk Migrator can migrate data between clusters of any supported types. See the next section for an introduction to these tools.
* Initially during the migration, {zdm-proxy} always reads from Origin.
* Later during the migration, the read routing is switched to Target so that all reads are executed on it, while writes are still sent to both clusters. In other words, Target becomes the primary source of truth.
* Finally, the client application can be moved off the proxy and connected directly to Target, at which point the migration is complete.

Note that, at any point during this process until the very last phase, if you hit any unexpected issue and need to "rollback" the migration you can always easily revert your client applications to connect directly to Origin. The migration can be started from scratch once the issue has been addressed.

After moving your client applications off the proxy, writes are no longer sent to both clusters and you lose this seamless rollback option. Should you decide to move back to Origin at a later point, you can perform the migration in reverse.

== Migration components

The primary component of the migration tool set is **{zdm-proxy}**, which by design is a simple and lightweight proxy that handles all the real-time requests generated by your client applications.

{company} {zdm-product} also provides a {zdm-utility} to setup Ansible-based playbooks, and {zdm-automation} to run the Docker container created by {zdm-utility}. {zdm-automation} allows you to deploy and manage the {zdm-proxy} instances and its associated monitoring stack. The {zdm-automation} creates the Docker container acting as the Ansible Control Host, from which the Ansible playbooks can be run easily.

The {zdm-proxy} itself doesn't have any capability to migrate data or knowledge that a migration may be ongoing, and it is not coupled to the migration process in any way.

Two data migration tools are available -- {cstar-data-migrator} and {dsbulk-migrator} -- to migrate your data. See the xref:migration-introduction.adoc#_data_migration_tools[summary of features] below.

=== Role of {zdm-proxy}

We created {zdm-proxy} to function between the application and the Origin and Target clusters so that every write operation (Insert, Update, Delete) is passed through synchronously to both clusters at the desired Consistency Level:

We created ZDM Proxy to function between the application and both databases (Origin and Target). The databases can be any CQL-compatible data store (e.g. Apache Cassandra, Datastax Enterprise and Astra DB). The proxy always sends every write operation (Insert, Update, Delete) synchronously to both clusters at the desired Consistency Level:

* If the write is successful in both clusters, it returns a successful acknowledgement to the client application
* If the write fails on either cluster, the failure is passed back to the client application so that it can retry it as appropriate, based on its own retry policy.

This design ensures that new data is always written to both clusters, and that any failure on either cluster is always made visible to the client application. {zdm-proxy} also sends all reads to the primary cluster (initially Origin, and later Target) and returns the result to the client application.

{zdm-proxy} is designed to be highly available. It can be scaled horizontally so typical deployments are made up of a minimum of 3 servers. {zdm-proxy} can be restarted in a rolling fashion, for example, to change configuration for different phases of the migration.

[TIP]
====
{zdm-proxy} has been designed to run in a **clustered** fashion so that it is never a single point of failure. Unless it is for a demo or local testing environment, a {zdm-proxy} deployment should always comprise multiple {zdm-proxy} instances.

We will often use the term **{zdm-proxy}** to indicate the whole deployment, and **{zdm-proxy} instance** to refer to the individual proxy processes in the deployment.
====

=== Key features of {zdm-proxy}

* Allows you to lift-and-shift existing application code from **Origin** to **Target** with a simple change of a connection string.

* Reduces risks to upgrades and migrations by decoupling Origin from Target, and allowing there to be an explicit cut-over point once you're satisfied with Target.

* Bifurcates writes to both clusters during the migration process synchronously.

* Returns (for read operations) the response from the primary cluster, which is its designated source of truth. During a migration, Origin is typically the primary cluster. Near the end of the migration, you'll shift the primary to be Target.

* Can be configured to also read asynchronously from Target. This capability is called **Asynchronous Dual Reads** (also known as **Read Mirroring**) and allows you to observe what read latencies and throughput Target can achieve under the actual production load.
** Results from the asynchronous reads executed on Target are not sent back to the client application.
** This design implies that failure on asynchronous reads from Target does not cause an error on the client application.
** Asynchronous dual reads can be enabled and disabled dynamically with a rolling restart of the {zdm-proxy} instances.

[NOTE]
====
When using Asynchronous Dual Reads, any additional read load on Target may impact its ability to keep up with writes. This behavior is expected and desired. The idea is to mimic the full read and write load on Target so there are no surprises during the last migration phase; that is, after cutting over completely to Target.
====

=== {zdm-automation}

The {zdm-automation} uses **Ansible** to deploy and configure the {zdm-proxy} and monitoring stack via playbooks. The utility expects that you have already provisioned the infrastructure.

https://www.ansible.com/[Ansible] is a suite of software tools that enables infrastructure as code. It is open source and its capabilities include software provisioning, configuration management, and application deployment functionality.

The Ansible automation is organized into playbooks, each implementing a specific operation. The machine from which the playbooks are run is known as the Ansible Control Host. In ZDM, the Ansible Control Host will run as a Docker container.

For details, see:

* xref:migration-setup-ansible-playbooks.adoc[]
* xref:migration-run-ansible-playbooks.adoc[]

=== Data migration tools

As part of the overall migration process, you can use {cstar-data-migrator} and/or {dsbulk-migrator} to migrate your data.

Use {cstar-data-migrator} to:

* Migrate your data from any CQL supported Origin to any CQL supported Target. Examples of databases that support CQL are Apache Cassandra, DataStax Enterprise and Astra DB.
* Validate migration accuracy and performance using examples that provide a smaller, randomized data set
* Preserve internal `writetime` timestamps and Time To Live (TTL) values
* Take advantage of advanced data types (Sets, Lists, Maps, UDTs)
* Filter records from the Origin data, using Cassandra's internal `writetime` timestamp
* Use SSL Support, including custom cipher algorithms

Cassandra Data Migrator is designed to:

* Connect to and compare your Target database with Origin
* Report differences in a detailed log file
* Optionally reconcile any missing records and fix any data inconsistencies in Target, if you enable `autocorrect` in a config file

[TIP]
====
An important **prerequisite** is that you already have the matching schema on Target.
====

You can also take advantage of {dsbulk-migrator} to migrate smaller sets of data. 

For more about both tools, see xref:migration-validate-data.adoc[Migrate and validate data, window="_blank"].

== What's next?

If you're new here, check out our xref:migration-faqs.adoc[FAQs].

Or jump right in and learn about the recommended xref:migration-deployment-infrastructure.adoc[deployment considerations], which include  infrastructure requirements to support your migration.
