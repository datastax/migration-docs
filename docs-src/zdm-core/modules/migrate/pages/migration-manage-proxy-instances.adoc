= Manage your {zdm-proxy} instances

In this topic, we'll learn how to perform simple operations on your {zdm-proxy} deployment with no interruption to its availability:

* Do a simple rolling restart of the {zdm-proxy} instances
* View or collect the logs of all {zdm-proxy} instances
* Change a mutable configuration property
* Upgrade the {zdm-proxy} version

All these operations can be easily done by running Ansible playbooks.

Make sure you are connected to the Ansible Control Host Docker container. As explained before, you can do so from the jumphost machine by running:
```bash
docker exec -it zdm-ansible-container bash
```
You will see a prompt like:
```bash
ubuntu@52772568517c:~$
```

== Perform a rolling restart of the proxies

Connect to the Ansible Control Host Docker container as explained above and run:

```bash
ansible-playbook rolling_restart_zdm_proxy.yml -i zdm_ansible_inventory
```

This is all that is needed.

This playbook restarts each proxy container one by one, without impacting the availability of the {zdm-proxy} deployment. It automates the following steps:

. It stops one container gracefully, waiting for it to shut down
. It starts the container again
. It checks that the container has come up successfully by checking the readiness endpoint:
.. If unsuccessful, it repeats the check for six times at 5-second intervals and eventually interrupts the whole process if the check still fails
.. If successful, it waits for 10 seconds and then moves on to the next container

== Access the proxy logs

To confirm that the {zdm-proxy} instances are operating normally, or investigate any issue, you can view or collect their logs.

=== View the logs

The {zdm-proxy} runs as a Docker container on each proxy host. Its logs can be viewed by connecting to a {zdm-proxy} host and running the following command:

```bash
sudo docker container logs zdm-proxy-container
```
To leave the logs open and continuously output the latest log messages, append the `--follow` option to the command above.

=== Collect the logs

You can easily retrieve the logs of all {zdm-proxy} instances using a dedicated playbook (`collect_zdm_proxy_logs.yml`). You can view the playbook's configuration values in `vars/zdm_proxy_log_collection_config.yml`, but no changes to it are required.

Connect to the Ansible Control Host container as explained above and run:

```bash
ansible-playbook collect_zdm_proxy_logs.yml -i zdm_ansible_inventory
```

This playbook creates a single zip file, called `zdm_proxy_logs_<current_timestamp>.zip`, containing the logs from all proxy instances, and stores it on the Ansible control host docker container in the directory `/home/ubuntu/zdm_proxy_archived_logs`.

To copy the zip file from the container to the jumphost, open a shell on the jumphost and run the following command:
```bash
docker cp zdm-ansible-container:/home/ubuntu/zdm_proxy_archived_logs/<zdm_proxy_logs zip archive name> <destination_directory_on_jumphost>
```
The archive will be copied to the specified destination directory on the jumphost.

== Change a mutable configuration property

The following configuration values are considered mutable and can be changed in a rolling fashion on an existing ZDM proxy deployment.

Commonly changed values, located in `vars/zdm_proxy_core_config.yml`:

* `primary_cluster` boolean flag:
** This flag determines which cluster is currently considered the "primary" source of truth. While writes are always sent to both clusters, the primary cluster is the one to which all synchronous reads are always sent, and their results are returned to the client application. At the start of the migration, the primary cluster is Origin, as it contains all the data.  In Phase 4 of the migration, once all the existing data has been transferred and any validation / reconciliation step has been successfully executed, you can switch the primary cluster to be Target. [TODO should this move to the glossary, and then we just link to it from here?]
** Valid values: `ORIGIN`, `TARGET`
* `read_mode`:
** Valid values:
*** `PRIMARY_ONLY`: reads are only sent synchronously to the primary cluster. This is the default behavior.
*** `DUAL_ASYNC_ON_SECONDARY`: reads are sent synchronously to the primary cluster and also asynchronously to teh secondary cluster. See xref:migration-enable-async-dual-reads.adoc[].
** Typically, when choosing `DUAL_ASYNC_ON_SECONDARY` you will want to ensure that `primary_cluster` is still set to `ORIGIN`. When you are ready to use Target as the primary cluster, you should revert `read_mode` to `PRIMARY_ONLY`.
* `log_level`:
** Defaults to `INFO`.
** Only set to `DEBUG` if necessary and revert to `INFO` as soon as possible, as the extra logging can have a slight performance impact.

Other, rarely changed values:

* Origin username / password, in `vars/zdm_proxy_core_config.yml`)
* Target username / password, in `vars/zdm_proxy_core_config.yml`)
* Advanced configuration values, located in `vars/zdm_proxy_advanced_config.yml`:
** `zdm_proxy_max_clients_connections`:
*** Maximum number of client connections that the {zdm-proxy} should accept. Each client connection results in additional cluster connections and causes the allocation of several in-memory structures, so this parameter can be tweaked to cap the total number on each instance. A high number of client connections per proxy instance may cause some performance degradation, especially at high throughput.
*** Defaults to `1000`
** `replace_cql_functions`:
*** Whether the {zdm-proxy} should replace standard CQL function calls in write requests with a value computed at proxy level.
*** Currently, only the replacement of `now()` is supported.
*** Disabled by default. Enabling this will have a noticeable performance impact.
** `zdm_proxy_request_timeout_ms`:
*** Global timeout (in ms) of a request at proxy level.
*** This parameter determines how long the {zdm-proxy} will wait for one cluster (in case of reads) or both clusters (in case of writes) to reply to a request. If this timeout is reached, the {zdm-proxy} will abandon that request and no longer consider it as pending, thus freeing up the corresponding internal resources. Note that, in this case, the {zdm-proxy} will not return any result or error: when the client application's own timeout is reached, the driver will time out the request on its side.
*** Defaults to `10000` ms. If your client application has a higher client-side timeout because it is expected to generate requests that take longer to complete, you need to increase this timeout accordingly.
** `origin_connection_timeout_ms` and `target_connection_timeout_ms`:
*** Timeout (in ms) when attempting to establish a connection from the proxy to Origin or Target.
*** Defaults to `30000` ms
** `async_handshake_timeout_ms`:
*** Timeout (in ms) when performing the initialization (handshake) of a proxy-to-secondary cluster connection that will be used solely for asynchronous dual reads.
*** If this timeout occurs, the asynchronous reads will not be sent. This has no impact on the handling of synchronous requests: the {zdm-proxy} will continue to handle all synchronous reads and writes normally.
*** Defaults to `4000` ms
** `metrics_enable`:
*** Whether metrics collection should be enabled
*** Defaults to `true`, but can be set to `false` to completely disable metrics collection. This is not recommended.

Deprecated settings, which will be removed in a future {zdm-proxy} release:

* `forward_client_credentials_to_origin`:
** Whether the credentials provided by the client application are for Origin.
** Defaults to `false` (the client application is expected to pass Target credentials), can be set to `true` if the client passes credentials for Origin instead.

To change any of these settings, edit the desired values in `vars/zdm_proxy_core_config.yml` and/or `vars/zdm_proxy_advanced_config.yml`.

To apply the configuration changes to the {zdm-proxy} instances in a rolling fashion, run the following command:

```bash
ansible-playbook update_zdm_proxy.yml -i zdm_ansible_inventory
```

This playbook operates in a rolling fashion by recreating each proxy container one by one. The {zdm-proxy} deployment remains available at all times and can be safely used throughout this operation. It automates the following steps:

. It stops one container gracefully, waiting for it to shut down
. It recreates the container and starts it up:
.. This is because containers are considered immutable, so a configuration change is a destructive action
.. Please note that this will remove the previous container and its logs. Make sure you collect the logs prior to this operation if you want to keep them.
. It checks that the container has come up successfully by checking the readiness endpoint:
.. If unsuccessful, it repeats the check for six times at 5-second intervals and eventually interrupts the whole process if the check still fails
.. If successful, it waits for 10 seconds and then moves on to the next container

== Upgrade the proxy version

The {zdm-proxy} version is displayed at startup, in a message such as `Starting ZDM proxy version ...`. It can also be retrieved at any time by using the `version` option as in the following command.
```bash
TODO add command to view version
```

The playbook for configuration changes can also be used to upgrade the {zdm-proxy} version in a rolling fashion. All containers will be recreated with the image of the specified version. The same behavior and observations as above apply here.

To perform an upgrade, change the version tag number to the desired version in `vars/zdm_proxy_container.yml`:
```bash
zdm_proxy_image: datastax/zdm-proxy:x.y.z
````
Replacing x.y.z with the version you would like to upgrade to.

Then simply run the same playbook as above, with the following command:

```bash
ansible-playbook update_zdm_proxy.yml -i zdm_ansible_inventory
```
