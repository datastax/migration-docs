= Advanced troubleshooting

[TODO]
====
TODO: DELETE THIS PAGE
====

This advanced troubleshooting topic presents solutions to error conditions that may occur only in rare cases.

== Scaling recommendation

{zdm-automation} doesn't provide a way to perform scaling up/down operations in a rolling fashion. Instead, we recommend that you deploy a new proxy cluster on the side, and move the client applications to the new proxy cluster (if absolutely necessary). To do so, create a new inventory file so that it contains one line for each machine where you want a proxy instance to be deployed and run the `deploy_zdm_proxy.yml` playbook again. This will result in a brief interruption of availability of the whole ZDM proxy deployment.

If you are not using the {zdm-automation} and want to remove or add a proxy **manually**, follow these steps:

. If adding a proxy instance, prepare and configure it appropriately based on the other instances
. Update the `ADDRESSES` configuration setting on all proxy instances - removing or adding the proxy instance's address to the list
. Perform a rolling restart on all proxy instances

Keep in mind that if the client applications are not configured to retry requests in case of `connection closed` errors, there may be a short downtime while the rolling restart is being done.

== Offline deployment of ZDM with Docker for local testing

While not specifically a ZDM issue, you can use `docker save` and `docker load` to troubleshoot the container. Examples:

. Run the following commands where you have Internet access:
+
```bash
docker pull datastax/zdm-proxy:2.0.0
docker save --output zdm-proxy2.tar datastax/zdm-proxy:2.0.0
```
. Move the tar file to the host node and run the following command:
+
```bash
docker load --input zdm-proxy2.tar
```
. At this point, `docker run` should work as the image is already on the local repo.

If you're using {zdm-automation}, there may be other steps required for the deployment to be successful without Internet access.

