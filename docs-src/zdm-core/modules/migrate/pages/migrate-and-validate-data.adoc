= Phase 2: Migrate and validate data
:page-tag: migration,zdm,zero-downtime,validate-data
ifdef::env-github,env-browser,env-vscode[:imagesprefix: ../images/]
ifndef::env-github,env-browser,env-vscode[:imagesprefix: ]

This topic explains how to use the following open-source data migration tools during Phase 2 of your migration project:

* {cstar-data-migrator}
* {dsbulk-migrator}

These tools provide sophisticated features that help you migrate your data from any Cassandra **Origin** (Apache Cassandra&reg;, {company} Enterprise (DSE), {company} {astra_db}) to any Cassandra **Target** (Apache Cassandra, DSE, {company} {astra_db}). 

Illustrated view of this phase:

image::{imagesprefix}migration-phase2ra.png[Phase 2 diagram shows using tools to migrate data from Origin to Target.]

For illustrations of all the migration phases, see the xref:introduction.adoc#_migration_phases[Introduction].

== What's the difference between these data migration tools?

In general:

* {cstar-data-migrator} (CDM) is the best choice to migrate large data quantities, and where detailed logging, data verifications, table column renaming (if needed), and reconciliation options are provided. 

* {dsbulk-migrator} leverages {company} Bulk Loader (DSBulk) to perform the data migration, and provides new commands specific to migrations. {dsbulk-migrator} is ideal for simple migration of smaller data quantities, and where data validation (other than post-migration row counts) is not necessary.

== Open-source repos with essential data migration tools

Refer to the following GitHub repos:

* https://github.com/datastax/cassandra-data-migrator[Cassandra Data Migrator^] repo.

* https://github.com/datastax/dsbulk-migrator[{dsbulk-migrator}^] repo.

A number of helpful assets are provided in each repo. In particular, the CDM repo provides two configuration templates, with embedded comments and default values, which you can customize to match your data migration's requirements:

* https://github.com/datastax/cassandra-data-migrator/blob/main/src/resources/cdm.properties[cdm-detailed.properties, window="_blank"] provides a subset of configuration options with commonly required settings.

* https://github.com/datastax/cassandra-data-migrator/blob/main/src/resources/cdm-detailed.properties[cdm-detailed.properties, window="_blank"] with all available options.

== Key features

Here's a summary of the key features per data migration tool.  

=== {cstar-data-migrator}

CDM offers functionalities like bulk export, import, data conversion, mapping of column names between Origin and Target, and validation. 
The CDM capabilities are extensive:

* Automatic detection of each table's schema - column names, types, keys, collections, UDTs, and other schema items.
* Validation - Log partitions range-level exceptions, use the exceptions file as input for rerun operations.
* Counter tables to validate the quantity of data migrations.
* Preserves writetimes and TTLs.
* Validation of advanced data types - Sets, Lists, Maps, UDTs.
* Filter records from Origin using writetimes, and/or CQL conditions, and/or a list of token ranges.
* Guardrail checks, such as identifying large fields.
* Fully containerized support - Docker and Kubernetes friendly.
* SSL support - including custom cipher algorithms.
* Migration/validation from and to Azure Cosmos Cassandra.
* Validate migration accuracy and performance using a smaller randomized data-set.
* Support for adding custom fixed writetime.

With new or enhanced capabilities in https://github.com/datastax/cassandra-data-migrator/blob/main/RELEASE.md#400---2023-06-02[CDM v4.x releases, window="_blank"], starting in early June 2023:

* Column names can differ between Origin and Target.
* UDTs can be migrated from Origin to Target, even when the keyspace names differ.
* Predefined Codecs allow for data type conversion between Origin and Target; you can add custom Codecs.
* Separate Writetime and TTL configuration supported. Writetime columns can differ from TTL columns.
* A subset of columns can be specified with Writetime and TTL: Not all eligible columns need to be used to compute the Origin value.
* Automatic `RandomPartitioner` min/max: Partition min/max values no longer need to be manually configured.
* You can populate Target columns with constant values: New columns can be added to the Target table, and populated with constant values.
* Expand Origin Map Column into Target rows: A Map in Origin can be expanded into multiple rows in Target when the Map key is part of the Target primary key.

=== {dsbulk-migrator}

{dsbulk-migrator}, which is based on {company} Bulk Loader (DSBulk), is best for migrating smaller amounts of data, and/or when you can shard data from table rows into more manageable quantities.  

{dsbulk-migrator} provides the following commands:

* `migrate-live` starts a live data migration using a pre-existing DSBulk installation, or alternatively, the embedded DSBulk version. A "live" migration means that the data migration will start immediately and will be performed by this migrator tool through the desired DSBulk installation.

* `generate-script` generates a migration script that, once executed, will perform the desired data migration, using a pre-existing DSBulk installation. Please note: this command does not actually migrate the data; it only generates the migration script.

* `generate-ddl` reads the schema from Origin and generates CQL files to recreate it in an {astra_db} cluster used as Target.


[[using-cdm]]
== Using {cstar-data-migrator}

Use {cstar-data-migrator} to migrate and validate tables between Origin and Target Cassandra clusters, with available logging and reconciliation support.

* xref:#cstar-prereqs[{cstar-data-migrator} prerequisites]
* xref:#cstar-install-as-container[Install {cstar-data-migrator} as a Container]
* xref:#cstar-install-as-jar[Install {cstar-data-migrator} as a JAR file]
* xref:#cstar-steps[{cstar-data-migrator} steps]
* xref:#cstar-validation-steps[{cstar-data-migrator} in validation mode]


[[cstar-prereqs]]
=== {cstar-data-migrator} prerequisites

* Install or switch to Java 8. The Spark binaries are compiled with this version of Java.
* Install https://archive.apache.org/dist/spark/spark-3.3.1/[Spark 3.3.1^] on a single VM (no cluster necessary) where you want to run this job. 

You can install Apache Spark by running the following commands:

[source,bash]
----
wget https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz 

tar -xvzf spark-3.3.1-bin-hadoop3.tgz
----

[[cstar-install-as-container]]
=== Install {cstar-data-migrator} as a Container

Get the latest image that includes all dependencies from https://hub.docker.com/r/datastax/cassandra-data-migrator[DockerHub^].

All migration tools (`cassandra-data-migrator` + `dsbulk` + `cqlsh`) are available in the `/assets/` folder of the container.

[[cstar-install-as-jar]]
=== Install {cstar-data-migrator} as a JAR file

Download the *latest* JAR file from the {cstar-data-migrator} https://github.com/datastax/cassandra-data-migrator/packages/[GitHub repo^]. 

[NOTE]
====
Version 4.x of {cstar-data-migrator} is not backward-compatible with *.properties files created in previous versions, and package names have changed. As of 21-Jun-2023, 4.1.0 is the latest version.
====


[[cstar-steps]]
=== {cstar-data-migrator} steps

1. Configure for your environment the `cdm*.properties` file that's provided in the GitHub repo. Parameter descriptions and defaults are described in the file. The file can have any name. It does not need to be `cdm.properties` or `cdm-detailed.properties`.
   * See the simplified sample properties configuration, https://github.com/datastax/cassandra-data-migrator/blob/main/src/resources/cdm.properties[cdm.properties^]. This version may be sufficient for your environment because it contains the most commonly needed configurable settings.
   * See the complete sample properties configuration, https://github.com/datastax/cassandra-data-migrator/blob/main/src/resources/cdm-detailed.properties[cdm-detailed.properties^], for the full set of configurable settings.

2. Place the properties file where it can be accessed while running the job via `spark-submit`.

3. Run the job using `spark-submit` command:

[source,bash]
----
./spark-submit --properties-file cdm.properties /
--conf spark.cdm.schema.origin.keyspaceTable="<keyspacename>.<tablename>" /
--master "local[*]" /
--class com.datastax.cdm.job.Migrate cassandra-data-migrator-4.x.x.jar &> logfile_name_$(date +%Y%m%d_%H_%M).txt
----

[TIP]
====
* The `spark-submit` command generates a log file, `logfile_name_*.txt`, to avoid log output on the Terminal console.
* If the table you're migrating is large (such as over 100GB), you can add the option `--driver-memory 25G --executor-memory 25G`. Example:

[source,bash]
----
./spark-submit --properties-file cdm.properties /
--conf spark.cdm.schema.origin.keyspaceTable="<keyspacename>.<tablename>" /
--master "local[*]" --driver-memory 25G --executor-memory 25G /
--class com.datastax.cdm.job.Migrate cassandra-data-migrator-4.x.x.jar &> logfile_name_$(date +%Y%m%d_%H_%M).txt
----
====

[[cstar-validation-steps]]
=== {cstar-data-migrator} steps in validation mode

To run your migration job with {cstar-data-migrator} in **data validation mode**, use class option `--class com.datastax.cdm.job.DiffData`. 
Example:

[source,bash]
----
./spark-submit --properties-file cdm.properties /
--conf spark.cdm.schema.origin.keyspaceTable="<keyspacename>.<tablename>" /
--master "local[*]" /
--class com.datastax.cdm.job.DiffData cassandra-data-migrator-4.x.x.jar &> logfile_name_$(date +%Y%m%d_%H_%M).txt
----

The {cstar-data-migrator} validation job will report differences as `ERROR` entries in the log file. 
Example:

[source,bash]
----
23/04/06 08:43:06 ERROR DiffJobSession: Mismatch row found for key: [key3] Mismatch: Target Index: 1 Origin: valueC Target: value999) 
23/04/06 08:43:06 ERROR DiffJobSession: Corrected mismatch row in target: [key3]
23/04/06 08:43:06 ERROR DiffJobSession: Missing target row found for key: [key2]
23/04/06 08:43:06 ERROR DiffJobSession: Inserted missing row in target: [key2]
----


[TIP]
====
To get the list of missing or mismatched records, grep for all `ERROR` entries in the log files. Differences noted in the log file are listed by primary-key values.
====

You can also run the cstar-data-migrator} validation job in an **AutoCorrect** mode. This mode can:

* Add any missing records from Origin to Target.
* Update any mismatched records between Origin and Target; this action makes Target the same as Origin.

To enable or disable this feature, use one or both of the following settings in your *.properties configuration file.

[source,properties]
----
spark.cdm.autocorrect.missing                     false|true
spark.cdm.autocorrect.mismatch                    false|true
----

[IMPORTANT]
====
The {cstar-data-migrator} validation job will never delete records from Target. The job only adds or updates data on Target.
====




// ------------------------

[[using-dsbulk-migrator]]
== Using {dsbulk-migrator}

Use {dsbulk-migrator} to perform simple migration of smaller data quantities, where data validation (other than post-migration row counts) is not necessary.

* xref:#prereqs-dsbulk-migrator[Prerequisites]
* xref:#building-dsbulk-migrator[Building {dsbulk-migrator}]
* xref:#testing-dsbulk-migrator[Testing {dsbulk-migrator}]
* xref:#running-dsbulk-migrator[Running {dsbulk-migrator}]

Also see the xref:#dsbulk-migrator-command-line-examples[{dsbulk-migrator} command-line examples].


[[prereqs-dsbulk-migrator]]
=== Prerequisites

* Java (TODO: list recommended and supported versions)
* https://maven.apache.org/download.cgi[Maven^] (TODO: list recommended and supported versions)
* https://github.com/datastax/simulacron#prerequisites[Simulcron^] for testing (TODO: list recommended and supported versions)

[[building-dsbulk-migrator]]
=== Building {dsbulk-migrator}

Building {dsbulk-migrator} is accomplished with Maven. First, clone the git repo to your local machine. Example:

[source,bash]
----
cd ~/github
git clone git@github.com:datastax/dsbulk-migrator.git
cd dsbulk-migrator
----

Then run:

[source,bash]
----
mvn clean package
----

The build produces two distributable fat jars:

* `dsbulk-migrator-<VERSION>-embedded-driver.jar` : contains an embedded Java driver; suitable for
  live migrations using an external DSBulk, or for script generation. This jar is NOT suitable for
  live migrations using an embedded DSBulk, since no DSBulk classes are present.

* `dsbulk-migrator-<VERSION>-embedded-dsbulk.jar`: contains an embedded DSBulk and an embedded Java
  driver; suitable for all operations. Note that this jar is much bigger than the previous one, due
  to the presence of DSBulk classes.


[[testing-dsbulk-migrator]]
=== Testing {dsbulk-migrator}

The project contains a few integration tests. Run them with:

[source,bash]
----
mvn clean verify
----

The integration tests require https://github.com/datastax/simulacron[Simulacron^]. Be sure to meet
all the https://github.com/datastax/simulacron#prerequisites[Simulacron prerequisites^] before running the
tests.


[[running-dsbulk-migrator]]
=== Running {dsbulk-migrator}

Launch the {dsbulk-migrator} tool:

[source,bash]
----
java -jar /path/to/dsbulk-migrator.jar { migrate-live | generate-script | generate-ddl } [OPTIONS]
----

When doing a live migration, the options are used to effectively configure DSBulk and to connect to
the clusters.

When generating a migration script, most options serve as default values in the generated scripts.
Note however that, even when generating scripts, this tool still needs to access the Origin cluster
in order to gather metadata about the tables to migrate.

When generating a DDL file, only a few options are meaningful. Because standard DSBulk is not used, and the
import cluster is never contacted, import options and DSBulk-related options are ignored. The tool
still needs to access the Origin cluster in order to gather metadata about the keyspaces and tables
for which to generate DDL statements.

==== **Live Migration Command Line Options**

The following options are available for the `migrate-live` command. Most options have sensible default values and do not
need to be specified, unless you want to override the default value.

[cols="2,8,14"]
|===

| `-c`
| `--dsbulk-cmd=CMD`
| The external DSBulk command to use. 
Ignored if the embedded DSBulk is being used. 
The default is simply 'dsbulk', assuming that the command is available through the `PATH` variable contents.

| `-d`
| `--data-dir=PATH`
| The directory where data will be exported to and imported from. 
The default is a 'data' subdirectory in the current working directory. 
The data directory will be created if it does not exist. 
Tables will be exported and imported in subdirectories of the data directory specified here. 
There will be one subdirectory per keyspace in the data directory, then one subdirectory per table in each keyspace directory.

| `-e`
| `--dsbulk-use-embedded`
| Use the embedded DSBulk version instead of an external one. 
The default is to use an external DSBulk command.

| 
| `--export-bundle=PATH`
| The path to a secure connect bundle to connect to the Origin cluster, if that cluster is a {company} {astra_db} cluster. 
Options `--export-host` and `--export-bundle` are mutually exclusive.

| 
| `--export-consistency=CONSISTENCY`
| The consistency level to use when exporting data. 
The default is `LOCAL_QUORUM`.

| 
| `--export-dsbulk-option=OPT=VALUE`
| An extra DSBulk option to use when exporting. 
Any valid DSBulk option can be specified here, and it will passed as is to the DSBulk process. 
DSBulk options, including driver options, must be passed as `--long.option.name=<value>`. 
Short options are not supported.

| 
| `--export-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the Origin cluster. 
If the port is not specified, it will default to `9042`. 
This option can be specified multiple times. 
Options `--export-host` and `--export-bundle` are mutually exclusive.

| 
| `--export-max-concurrent-files=NUM\|AUTO`
| The maximum number of concurrent files to write to. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--export-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--export-max-records=NUM`
| The maximum number of records to export for each table. 
Must be a positive number or `-1`. 
The default is `-1` (export the entire table).

| 
| `--export-password`
| The password to use to authenticate against the Origin cluster. 
Options `--export-username` and `--export-password` must be provided together, or not at all. 
Omit the parameter value to be prompted for the password interactively.

| 
| `--export-splits=NUM\|NC`
| The maximum number of token range queries to generate. 
Use the `NC` syntax to specify a multiple of the number of available cores. 
For example, `8C` = 8 times the number of available cores. 
The default is `8C`. 
This is an advanced setting; you should rarely need to modify the default value.

| 
| `--export-username=STRING`
| The username to use to authenticate against the Origin cluster. 
Options `--export-username` and `--export-password` must be provided together, or not at all.

| `-h` 
| `--help`
| Displays this help text.

| 
| `--import-bundle=PATH`
| The path to a secure connect bundle to connect to the Target cluster, if it's a {company} {astra_db} cluster. 
Options `--import-host` and `--import-bundle` are mutually exclusive.

| 
| `--import-consistency=CONSISTENCY`
| The consistency level to use when importing data. 
The default is `LOCAL_QUORUM`.

| 
| `--import-default-timestamp=<defaultTimestamp>`
| The default timestamp to use when importing data. 
Must be a valid instant in ISO-8601 syntax. 
The default is `1970-01-01T00:00:00Z`.

| 
| `--import-dsbulk-option=OPT=VALUE`
| An extra DSBulk option to use when importing. 
Any valid DSBulk option can be specified here, and it will passed as is to the DSBulk process. 
DSBulk options, including driver options, must be passed as `--long.option.name=<value>`. 
Short options are not supported.

| 
| `--import-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the Target cluster. 
If the port is not specified, it will default to `9042`. 
This option can be specified multiple times. 
Options `--import-host` and `--import-bundle` are mutually exclusive. 

| 
| `--import-max-concurrent-files=NUM\|AUTO` 
| The maximum number of concurrent files to read from. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--import-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--import-max-errors=NUM`
| The maximum number of failed records to tolerate when importing data. 
The default is `1000`. 
Failed records will appear in a `load.bad` file in the DSBulk operation directory.

| 
| `--import-password`
| The password to use to authenticate against the Target cluster. 
Options `--import-username` and `--import-password` must be provided together, or not at all. 
Omit the parameter value to be prompted for the password interactively.

| 
| `--import-username=STRING`
| The username to use to authenticate against the Target cluster.
                               Options `--import-username` and `--import-password` must be provided
                               together, or not at all.

| `-k`
| `--keyspaces=REGEX`
| A regular expression to select keyspaces to migrate. 
The default is to migrate all keyspaces except system keyspaces, DSE-specific keyspaces, and the OpsCenter keyspace. 
Case-sensitive keyspace names must be entered in their exact case.

| `-l`
| `--dsbulk-log-dir=PATH`
| The directory where DSBulk should store its logs. 
The default is a 'logs' subdirectory in the current working directory. 
This subdirectory will be created if it does not exist. 
Each DSBulk operation will create a subdirectory in the log directory specified here.

| 
| `--max-concurrent-ops=NUM`
| The maximum number of concurrent operations (exports and imports) to carry. 
The default is `1`. 
Set this to higher values to allow exports and imports to occur concurrently. 
For example, with a value of `2`, each table will be imported as soon as it is exported, while the next table is being exported.

| 
| `--skip-truncate-confirmation`
| Skip truncate confirmation before actually truncating tables. 
Only applicable when migrating counter tables, ignored otherwise.

| `-t`
| `--tables=REGEX` 
| A regular expression to select tables to migrate. 
The default is to migrate all tables in the keyspaces that were selected for migration with `--keyspaces`. 
Case-sensitive table names must be entered in their exact case.

| 
| `--table-types=regular\|counter\|all`
| The table types to migrate. 
The default is `all`.

| 
| `--truncate-before-export`
| Truncate tables before the export instead of after. 
The default is to truncate after the export. 
Only applicable when migrating counter tables, ignored otherwise.

| `-w`
| `--dsbulk-working-dir=PATH`
| The directory where DSBulk should be executed. 
Ignored if the embedded DSBulk is being used. 
If unspecified, it defaults to the current working directory.

|===


==== **Script Generation Command Line Options**

The following options are available for the `generate-script` command. 
Most options have sensible default values and do not need to be specified, unless you want to override the default value.


[cols="2,8,14"]
|===

| `-c` 
| `--dsbulk-cmd=CMD`
| The DSBulk command to use. 
The default is simply 'dsbulk', assuming that the command is available through the `PATH` variable contents.

| `-d`
| `--data-dir=PATH`
| The directory where data will be exported to and imported from. 
The default is a 'data' subdirectory in the current working directory. 
The data directory will be created if it does not exist. 

| 
| `--export-bundle=PATH`
| The path to a secure connect bundle to connect to the Origin cluster, if that cluster is a {company} {astra_db} cluster. 
Options `--export-host` and `--export-bundle` are mutually exclusive.

| 
| `--export-consistency=CONSISTENCY`
| The consistency level to use when exporting data. 
The default is `LOCAL_QUORUM`.

| 
| `--export-dsbulk-option=OPT=VALUE`
| An extra DSBulk option to use when exporting. 
Any valid DSBulk option can be specified here, and it will passed as is to the DSBulk process. 
DSBulk options, including driver options, must be passed as `--long.option.name=<value>`. 
Short options are not supported.

| 
| `--export-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the Origin cluster. 
If the port is not specified, it will default to `9042`. 
This option can be specified multiple times. 
Options `--export-host` and `--export-bundle` are mutually exclusive.

| 
| `--export-max-concurrent-files=NUM\|AUTO`
| The maximum number of concurrent files to write to. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--export-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--export-max-records=NUM`
| The maximum number of records to export for each table. 
Must be a positive number or `-1`. 
The default is `-1` (export the entire table).

| 
| `--export-password`
| The password to use to authenticate against the Origin cluster. 
Options `--export-username` and `--export-password` must be provided together, or not at all. 
Omit the parameter value to be prompted for the password interactively.

| 
| `--export-splits=NUM\|NC`
| The maximum number of token range queries to generate. 
Use the `NC` syntax to specify a multiple of the number of available cores. 
For example, `8C` = 8 times the number of available cores. 
The default is `8C`. 
This is an advanced setting. You should rarely need to modify the default value.

| 
| `--export-username=STRING`
| The username to use to authenticate against the Origin cluster. 
Options `--export-username` and `--export-password` must be provided together, or not at all.

| `-h` 
| `--help`
| Displays this help text.

| 
| `--import-bundle=PATH`
| The path to a secure connect bundle to connect to the Target cluster, if it's a {company} {astra_db} cluster. 
Options `--import-host` and `--import-bundle` are mutually exclusive.

| 
| `--import-consistency=CONSISTENCY`
| The consistency level to use when importing data. 
The default is `LOCAL_QUORUM`.

| 
| `--import-default-timestamp=<defaultTimestamp>`
| The default timestamp to use when importing data. 
Must be a valid instant in ISO-8601 syntax. 
The default is `1970-01-01T00:00:00Z`.

| 
| `--import-dsbulk-option=OPT=VALUE`
| An extra DSBulk option to use when importing. 
Any valid DSBulk option can be specified here, and it will passed as is to the DSBulk process. 
DSBulk options, including driver options, must be passed as `--long.option.name=<value>`. 
Short options are not supported.

| 
| `--import-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the Target cluster. 
If the port is not specified, it will default to `9042`. 
This option can be specified multiple times. 
Options `--import-host` and `--import-bundle` are mutually exclusive. 

| 
| `--import-max-concurrent-files=NUM\|AUTO`
| The maximum number of concurrent files to read from. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--import-max-concurrent-queries=NUM\|AUTO`
| The maximum number of concurrent queries to execute. 
Must be a positive number or the special value `AUTO`. 
The default is `AUTO`.

| 
| `--import-max-errors=NUM`
| The maximum number of failed records to tolerate when importing data. 
The default is `1000`. 
Failed records will appear in a `load.bad` file in the DSBulk operation directory.

| 
| `--import-password`
| The password to use to authenticate against the Target cluster. 
Options `--import-username` and `--import-password` must be provided together, or not at all. 
Omit the parameter value to be prompted for the password interactively.

| 
| `--import-username=STRING`
| The username to use to authenticate against the Target cluster.
Options `--import-username` and `--import-password` must be provided together, or not at all.

| `-k`
| `--keyspaces=REGEX`
| A regular expression to select keyspaces to migrate. 
The default is to migrate all keyspaces except system keyspaces, DSE-specific keyspaces, and the OpsCenter keyspace. 
Case-sensitive keyspace names must be entered in their exact case.

| `-l`
| `--dsbulk-log-dir=PATH`
| The directory where DSBulk should store its logs. 
The default is a 'logs' subdirectory in the current working directory. 
This subdirectory will be created if it does not exist. 
Each DSBulk operation will create a subdirectory in the log directory specified here.


| `-t`
| `--tables=REGEX` 
| A regular expression to select tables to migrate. 
The default is to migrate all tables in the keyspaces that were selected for migration with `--keyspaces`. 
Case-sensitive table names must be entered in their exact case.

| 
| `--table-types=regular\|counter\|all`
| The table types to migrate. The default is `all`.

|===


==== **DDL Generation Command Line Options**

The following options are available for the `generate-ddl` command. 
Most options have sensible default values and do not need to be specified, unless you want to override the default value.

[cols="2,8,14"]
|===

| `-a`
| `--optimize-for-astra`
| Produce CQL scripts optimized for {company} {astra_db}. 
{astra_db} does not allow some options in DDL statements. 
Using this {dsbulk-migrator} command option, forbidden {astra_db} options will be omitted from the generated CQL files.

| `-d`
| `--data-dir=PATH`
| The directory where data will be exported to and imported from. 
The default is a 'data' subdirectory in the current working directory.
The data directory will be created if it does not exist. 

| 
| `--export-bundle=PATH`
| The path to a secure connect bundle to connect to the Origin cluster, if that cluster is a {company} {astra_db} cluster. 
Options `--export-host` and `--export-bundle` are mutually exclusive.

| 
| `--export-host=HOST[:PORT]`
| The host name or IP and, optionally, the port of a node from the Origin cluster. 
If the port is not specified, it will default to `9042`. 
This option can be specified multiple times. 
Options `--export-host` and `--export-bundle` are mutually exclusive.

| 
| `--export-password`
| The password to use to authenticate against the Origin cluster. 
Options `--export-username` and `--export-password` must be provided together, or not at all. 
Omit the parameter value to be prompted for the password interactively.

| 
| `--export-username=STRING`
| The username to use to authenticate against the Origin cluster. 
Options `--export-username` and `--export-password` must be provided together, or not at all.

| `-h` 
| `--help`
| Displays this help text.

| `-k`
| `--keyspaces=REGEX`
| A regular expression to select keyspaces to migrate. 
The default is to migrate all keyspaces except system keyspaces, DSE-specific keyspaces, and the OpsCenter keyspace. 
Case-sensitive keyspace names must be entered in their exact case.

| `-t`
| `--tables=REGEX` 
| A regular expression to select tables to migrate. 
The default is to migrate all tables in the keyspaces that were selected for migration with `--keyspaces`. 
Case-sensitive table names must be entered in their exact case.

| 
| `--table-types=regular\|counter\|all`
| The table types to migrate. 
The default is `all`.

|===


[[getting-help-with-dsbulk-migrator]]
=== Getting help with {dsbulk-migrator}

Use the following command to display the available {dsbulk-migrator} commands:

[source,bash]
----
java -jar /path/to/dsbulk-migrator-embedded-dsbulk.jar --help
----

For individual command help and each one's options:

[source,bash]
----
java -jar /path/to/dsbulk-migrator-embedded-dsbulk.jar COMMAND --help
----

[[dsbulk-migrator-command-line-examples]]
=== {dsbulk-migrator} command-line examples

[NOTE]
====
These examples show sample `username` and `password` values that are for demonstration purposes only. 
Do not use these values in your environment.
==== 

==== **Generate migration script**

Generate a migration script to migrate from an existing Origin cluster to a Target {astra_db} cluster:

[source,bash]
----
    java -jar target/dsbulk-migrator-<VERSION>-embedded-driver.jar migrate-live \
        --data-dir=/path/to/data/dir \
        --dsbulk-cmd=${DSBULK_ROOT}/bin/dsbulk \
        --dsbulk-log-dir=/path/to/log/dir \
        --export-host=my-origin-cluster.com \
        --export-username=user1 \
        --export-password=s3cr3t \
        --import-bundle=/path/to/bundle \
        --import-username=user1 \
        --import-password=s3cr3t
----

==== **Migrate live using external DSBulk install**

Migrate live from an existing Origin cluster to a Target {astra_db} cluster using an external DSBulk installation. 
Passwords will be prompted interactively:

[source,bash]
----
    java -jar target/dsbulk-migrator-<VERSION>-embedded-driver.jar migrate-live \
        --data-dir=/path/to/data/dir \
        --dsbulk-cmd=${DSBULK_ROOT}/bin/dsbulk \
        --dsbulk-log-dir=/path/to/log/dir \
        --export-host=my-origin-cluster.com \
        --export-username=user1 \
        --export-password # password will be prompted \
        --import-bundle=/path/to/bundle \
        --import-username=user1 \
        --import-password # password will be prompted
----

==== **Migrate live using embedded DSBulk install**

Migrate live from an existing Origin cluster to a Target {astra_db} cluster using the embedded DSBulk installation. 
Passwords will be prompted interactively. 
In this example, additional DSBulk options are passed. 

[source,bash]
----
    java -jar target/dsbulk-migrator-<VERSION>-embedded-dsbulk.jar migrate-live \
        --data-dir=/path/to/data/dir \
        --dsbulk-use-embedded \
        --dsbulk-log-dir=/path/to/log/dir \
        --export-host=my-origin-cluster.com \
        --export-username=user1 \
        --export-password # password will be prompted \
        --export-dsbulk-option "--connector.csv.maxCharsPerColumn=65536" \
        --export-dsbulk-option "--executor.maxPerSecond=1000" \
        --import-bundle=/path/to/bundle \
        --import-username=user1 \
        --import-password # password will be prompted \
        --import-dsbulk-option "--connector.csv.maxCharsPerColumn=65536" \
        --import-dsbulk-option "--executor.maxPerSecond=1000" 
----

[NOTE]
====
In the example above, you must use the `dsbulk-migrator-<VERSION>-embedded-dsbulk.jar` fat jar. 
Otherwise, an error will be raised because no embedded DSBulk can be found.
====

==== **Generate DDL**

Generate DDL files to recreate the Origin schema in a Target {astra_db} cluster:

[source,bash]
----
    java -jar target/dsbulk-migrator-<VERSION>-embedded-driver.jar generate-ddl \
        --data-dir=/path/to/data/dir \
        --export-host=my-origin-cluster.com \
        --export-username=user1 \
        --export-password=s3cr3t \
        --optimize-for-astra
----
