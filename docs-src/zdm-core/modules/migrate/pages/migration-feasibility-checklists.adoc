= Feasibility checks

Before starting your migration, refer to the following considerations to ensure that your application workload and Origin cluster are suitable for this {zdm-product} process.

== Protocol version and cluster version support

{zdm-proxy} supports protocol versions v3, v4, DseV1 and DseV2. We plan to support v2 very soon. It technically doesn't support v5 but handles protocol negotiation so that the client application properly downgrades the protocol version to v4 if v5 is requested. 

*Thrift is not supported by {zdm-proxy}.* If you are using a very old driver or cluster version that only supports Thrift then you need to upgrade before starting the migration process.

In practice this means that {zdm-proxy} supports these following cluster versions (as both Origin and Target):

* Apache Cassandra from 2.1+ up to (and including) Apache Cassandra 4.x. Apache Cassandra 2.0 support will be introduced when protocol version v2 is supported.
* Datastax Enterprise 4.8+. Datastax Enterprise 4.6 and 4.7 support will be introduced when protocol version v2 is supported.
* Datastax Astra DB

[TIP]
====
Ensure that you test your client application with Target (connected directly without the {zdm-proxy}) before the migration process begins.
====

== Considerations for Astra DB migrations

DataStax Astra DB implements guardrails and sets limits to ensure good practices, foster availability, and promote optimal configurations for your databases. https://docs.datastax.com/en/astra-serverless/docs/plan/planning.html#_astra_db_database_guardrails_and_limits[Please check the list of guardrails and limits here^] and make sure your application workload can be successful in the presence of these limits. 

If you need to make changes to the application or data model to ensure that your workload can run successfully in Datastax Astra then you need to do these changes before you start the migration process.

It is also highly recommended to perform tests and benchmarks when connected directly to Astra DB prior to the migration so that you don't find unexpected issues during the migration process.

=== Read only applications

If a client application only sends `SELECT` statements to a database connection then you may find that {zdm-proxy} terminates these read only connections periodically which may result in request errors if the driver is not configured to retry these requests in these conditions. 

This happens because Datastax Astra terminates idle connections after some inactivity period (usually around 10 minutes) so if Datastax Astra is your Target and a client connection is only sending read requsts to the {zdm-proxy} then the Datastax Astra connection that is "paired" to that client connection will remain idle and will be eventually terminated.

*We are planning a fix to this behavior* so that {zdm-proxy} sends heartbeats to the secondary cluster connections when a client connection only sends read requests but in the meantime you may want to implement some kind of work around for this issue. 

A potential work around is to not connect these read only applications to {zdm-proxy} but you need to ensure that these applications switch reads to Target before the migration is done.

Another work around is to implement a mechanism in your client application that creates a new `Session` periodically to avoid the Datastax Astra inactivity timeout. You can also implement some kind of meaningless write request that the application sends periodically to make sure the Datastax Astra connection doesn't idle.

== Advanced workloads (Datastax Enterprise)

=== Graph

{zdm-proxy} handles all DataStax Graph requests as write requests even if the traversals are read only. There is no special handling for these requests so you need to take a look at the traversals that your client application sends and determine whether the traversals are idempotent. If the traversals are non-idempotent then the reconciliation step is needed.

Keep in mind that our recommended tools for data migration (and reconciliation) are CQL based so they can be used for migrations where Origin is a database that uses the new Datastax Graph engine (released with DSE 6.8) but they *can not be used for the old graph engine* that older DSE versions relied on. xref:#non-idempotent-operations[Check this section] for more information about non-idempotent operations.

=== Search

We recommend moving search workloads direcly from Origin to Target without {zdm-proxy} being involved but if you need the read routing capabilities from {zdm-proxy} then you can connect your search workloads to it as long as you are using the Datastax drivers to submit these queries (which means that the queries are regular CQL `SELECT` statements so {zdm-proxy} handles them as regular read requests).

If you use the HTTP API then you either modify your applications to use the CQL API instead or you will have to move those applications directly from Origin to Target when the migration is complete if that is acceptable.

== Client compression

The binary protocol used by Apache Cassandra, DSE and Astra DB supports optional compression of transport-level requests and responses that reduces network traffic at the cost of CPU overhead.

{zdm-proxy} doesn't support protocol compression at this time but we plan to introduce support for it soon. This kind of compression is disabled by default on all of our Datastax drivers so if you enabled it on your client application then you will need to disable it before starting the migration process.

This is *NOT* related to storage compression which you can configure on a table by table basis with the `compression` table property. Storage/table compression does not affect the client application or {zdm-proxy} in any way.

== Schema / keyspace compability

{zdm-proxy} does not modify or transform cql statements besides the optional feature that replaces `now()` functions with timestamp literals (xref:#cql-function-replacement[refer to this section for more information about this feature]).

A cql statement that your client application sends to {zdm-proxy} must be able to succeed on both clusters. This means that any keyspace that your client application uses must exist on both Origin and Target with the same name (although they can have different replication strategies and durable writes settings).

The schema doesn't have to be an exact match as long as the cql statements can be executed successfully on both clusters. For example, if a table has 10 columns but your client application only uses 5 of those columns then you could create that table on Target with just those 5 columns. 

You can also change the primary key in some cases too. For example, if your compound primary key is `PRIMARY KEY (A, B)` and you always provide parameters for the `A` and `B` columns in your CQL statements then you could change the key to `PRIMARY KEY (B, A)` when creating the schema on Target because your CQL statements will still run successfully.

== Authenticator and Authorizer configuration

{zdm-proxy} supports the following cluster authenticator configurations:

* No authenticator
* `PasswordAuthenticator`
* `DseAuthenticator` with `internal` or `ldap` scheme

{zdm-proxy} does *not* support `DseAuthenticator` with `kerberos` scheme.

While the authenticator has to be supported, the *authorizer* does not affect client applications or {zdm-proxy} so you should be able to use any kind of authorizer configuration on both of your clusters.

[#cql-function-replacement]
== Server-side non-deterministic functions in the primary key

Statements with functions like `now()` and `uuid()` will result in data inconsistency between Origin and Target because the values are computed at cluster level. 

If these functions are used for columns that are not part of the primary key then you may find it acceptable to have different values in the two clusters depending on your application business logic. However, if these columns are part of the primary key then the data migration phase will not be successful as there will be data inconsistencies between the two clusters and they will never be in sync.

{zdm-proxy} is able to compute timestamps and replace `now()` function references with such timestamps in CQL statements at proxy level to ensure that these parameters will have the same value when it is sent to both clusters. However, this feature is disabled by default because it might result in performance degradation so we highly recommend users to test this properly before using it in production. Also keep in mind that this feature is only supported for `now()` functions at the moment, we plan to add support for other functions such as  `uuid()` soon. To enable this feature, set the `ZDM_REPLACE_CQL_FUNCTIONS` setting to `true`.

If you find that the performance is not acceptable when this feature is enabled, or the feature doesn't cover a particular function that your application is using then you will have to make a change to your application so that the value is computed locally (at application level) before the statement is sent to the database. Most drivers have utility methods that help you compute these values locally, please refer to the documentation of the driver you are using.

[#non-idempotent-operations]
== Lightweight Transactions and other non-idempotent operations

Examples of non-idempotent operations in CQL are:

* Lightweight Transactions (LWTs)
* Counter updates
* Collection updates with `+=` and `-=` operators
* Non-deterministic functions like `now()` and `uuid()` as mentioned in the prior section

For more information on how to handle non-deterministic functions please refer to the prior section.

Given that there are two separate clusters involved, the state of each cluster may be different. For conditional writes, this may create a divergent state for a time. It may not make a difference in many cases, but if non-idempotent operations are used, we recommend a reconciliation phase in the migration before and after switching reads to rely on Target (setting Target as the primary cluster). 

For details about using the Cassandra Data Migrator to validate your migration, see xref:migration-migrate-and-validate-data.adoc[Migrate and validate your data].

[TIP]
====
Some application workloads can tolerate inconsistent data in some cases (especially for counter values) in which case you may not need to do anything special to handle those non-idempotent operations.
====

=== Lightweight Transactions applied flag

{zdm-proxy} forwards lightweight transactions to both Origin and Target. However, it only returns the `applied` value from the primary cluster which is the cluster from where read results are returned to the client application (by default, that is Origin). This means that when you set Target as your primary cluster, the `applied` value returned to the client application will come from Target.

== Driver retry policy and query idempotence

As part of the normal migration process, the {zdm-proxy} instances will have to be restarted in between phases to apply configuration changes. In the point of view of the client application, this is a similar behavior to a DSE or Apache Cassandra cluster going through a rolling restart in a non-migration scenario.

If your application already tolerates rolling restarts of your current cluster then you should see no issues when there is a rolling restart of {zdm-proxy} instances.

To ensure that your client application retries requests when a database connection is closed you should check the section of your driver's documentation related to retry policies.

Some Datastax drivers require a statement to be marked as `idempotent` in order to retry it in case of a connection error (such as the termination of a database connection). 

In this section we outline the default behavior of some of these drivers.

=== Datastax Java Driver 4.x

The default retry policy takes idempotence in consideration and the query builder tries to infer idempotence automatically, xref:https://docs.datastax.com/en/developer/java-driver/latest/manual/core/idempotence/[check this documentation section].

=== Datastax Java Driver 3.x

The default retry policy takes idempotence in consideration and the query builder tries to infer idempotence automatically, xref:https://docs.datastax.com/en/developer/java-driver/3.11/manual/idempotence/[check this documentation section].

This behavior was introduced in version 3.1.0 so prior to this version the default retry policy retried all requests regardless of idempotence.

=== Datastax Nodejs Driver 4.x

The default retry policy takes idempotence in consideration, xref:https://docs.datastax.com/en/developer/nodejs-driver/latest/features/speculative-executions/#query-idempotence[check this documentation section].

=== Datastax C# Driver 3.x and Datastax Python Driver 3.x

The default retry policy retries all requests in case of a connection error *regardless of idempotence*. There are retry policies that are idempotency aware but these are not the default policies. Keep in mind that the plan is to make the default retry policy idempotency aware in a future release.

=== Datastax C++ Driver 2.x

Prior to version 2.5.0, this driver did *NOT* retry any requests after they have been written to the socket, it was up to the client application to handle these and retry them if they are suitable for a retry.

With the release of 2.5.0, the driver retries requests that are set as `idempotent`, xref:https://docs.datastax.com/en/developer/cpp-driver/2.16/topics/configuration/#query-idempotence[check this documentation section].

== What's next?

For any migration, we've described important xref:migration-deployment-infrastructure.adoc[deployment and infrastructure considerations] in the next topic.
