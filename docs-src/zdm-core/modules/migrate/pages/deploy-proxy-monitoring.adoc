= Deploy the {zdm-proxy} and monitoring

This topic explains how to use the Ansible automation playbooks that you set up in the xref:setup-ansible-playbooks.adoc[prior topic] to deploy the proxy and its monitoring stack.

Once completed, you will have a working and fully monitored {zdm-proxy} deployment.

== Prerequisites

You must have completed the Ansible setup as described in the xref:setup-ansible-playbooks.adoc[prior topic].

== Access the Docker container

You can connect to the Docker container by opening a shell on it:

```bash
docker exec -it zdm-ansible-container bash
```

You're now connected to the container, at a prompt such as this example:

```bash
ubuntu@52772568517c:~$
```

You can `ls` to see the resources in the Docker container. The most important resource is the `zdm-proxy-automation`.

Now, `cd` into `zdm-proxy-automation/ansible` and `ls`. Example:

image:zdm-ansible-container-ls3.png[Contents of the Ansible Control Host container]

== Configure the {zdm-proxy}

The {zdm-proxy} configuration is broken down into three files:

 * `zdm_proxy_core_config.yml`, containing configuration that is always required
 * `zdm_proxy_advanced_config.yml`, containing advanced configuration that may be required in some scenarios but can be usually left to its default values
 * `zdm_proxy_custom_tls_config.yml`, to configure TLS encryption if desired.

=== Core configuration
The next step is to edit the `zdm_proxy_core_config.yml` file in the Docker container. You'll want to enter your Cassandra/DSE username, password, and other values.

In the container shell, `cd` to `~/zdm-proxy-automation/ansible/vars` and edit `zdm_proxy_core_config.yml`

There are two identical sets of parameters to configure the {zdm-proxy} to connect to the cluster. One set is for Origin and its parameters are prefixed with `origin`, while the one for Target uses the `target` prefix.

These two sections are very important and are always required. Populate the appropriate parameters in each section for the respective cluster, by uncommenting the parameters and specifying their values as follows:

. Cluster credentials:
.. If it is a self-managed cluster, `*_username` and `*_password` must be valid credentials for it. Leave blank if authentication is not enabled on the cluster.
.. If it is an Astra DB cluster, `*_username` must be the Client ID and `*_password` the Client Secret of a valid `R/W User`  Astra role
. Contact points and port (only relevant for self-managed clusters, leave unset for Astra clusters)
.. `*_contact points` comma-separated list of IP addresses of the cluster's seed nodes
.. `*_port`: port on which the cluster listens for client connections. Defaults to 9042.
. For Astra DB clusters, choose one of the following options and leave unset the other (leave both unset for self-managed clusters):
.. If you wish to manually provide the cluster's Secure Connect Bundle:
... Download it from the Astra Portal and place it on the jumphost
... Copy it to the container. Open a new shell on the jumphost, run `docker cp <your_scb.zip> zdm-ansible-container:/home/ubuntu`
... Specify its path in `*_astra_secure_connect_bundle_path`.
.. Otherwise, if you wish the automation to download the cluster's Secure Connect Bundle for you, just specify its database id in `*_astra_db_id` and the token for a valid `R/W User` Astra role in `*_astra_token`

Global settings that do not need changing at this time:
. `primary_cluster`: which cluster is going to be the primary source of truth. This should be left set to its default value of `ORIGIN` at the start of the migration, and will be changed to `TARGET` after migrating all existing data.
. `read_mode`: leave to its default value of `PRIMARY_ONLY`. Global see xref:enable-async-dual-reads.adoc[] for more information on this parameter.
. `log_level`: leave to its default of `INFO`.

Save the file and exit the editor.

Example of a completed `zdm_proxy_core_config.yml` file for a migration from a self-managed Origin to an Astra DB Target:

```yml
##############################
#### ORIGIN CONFIGURATION ####
##############################

## Origin credentials
origin_username: "my_user"
origin_password: "my_password"

## Set the following two parameters only if Origin is a self-managed, non-Astra cluster
origin_contact_points: "191.100.20.135,191.100.21.43,191.100.22.18"
origin_port: 9042

##############################
#### TARGET CONFIGURATION ####
##############################

## Target credentials (partially redacted)
target_username: "dqhg...NndY"
target_password: "Yc+U_2.gu,9woy0w...9JpAZGt+CCn5"

## Set the following two parameters only if Target is an Astra cluster and you would like the automation to download the secure connect bundle automatically
target_astra_db_id: "d425vx9e-f2...c871k"
target_astra_token: "AstraCS:dUTGnRs...jeiKoIqyw:01...29dfb7"

####################################
#### READ ROUTING CONFIGURATION ####
####################################

## Cluster currently considered as "primary" by the ZDM proxy. Valid values are ORIGIN (default) or TARGET.
## This is the current "source of truth" cluster.
## Reads are always sent synchronously to the primary cluster, and their results (or any errors) are returned to the client application.
primary_cluster: ORIGIN

## Read handling behaviour.
## Valid values: PRIMARY_ONLY (read synchronously from primary only) and DUAL_ASYNC_ON_SECONDARY (read synchronously from primary, and asynchronously from secondary)
## Defaults to PRIMARY_ONLY. Set to DUAL_ASYNC_ON_SECONDARY for read mirroring.
read_mode: PRIMARY_ONLY

#################
#### LOGGING ####
#################

## Proxy log level. Defaults to INFO. Setting it to DEBUG may have a small performance impact.
log_level: INFO
```

=== Enable TLS encryption (optional)

If you wish to enable TLS encryption between the client application and the {zdm-proxy}, or between the {zdm-proxy} and one (or both) self-managed clusters, you will need to specify some additional configuration. To do so, please follow the steps on xref:tls.adoc[this page].

=== Advanced configuration (optional)

Here are some additional configuration variables that you may wish to review and change *at deployment time* in specific cases. All these variables are located in `vars/zdm_proxy_advanced_config.yml`.

All advanced configuration variables not listed here are considered mutable and can be changed later if needed (changes can be easily applied to existing deployments in a rolling fashion using the relevant Ansible playbook).

==== Multi-datacenter clusters

If Origin is a multi-datacenter cluster, you will need to specify the name of the datacenter that the {zdm-proxy} should consider local. To do this, set the property `origin_local_datacenter` to the datacenter name. Likewise, for multi-datacenter Target clusters you will need to set `target_local_datacenter` appropriately.

These two properties are located in `vars/zdm_proxy_advanced_configuration.yml`. Note that this is not relevant for multi-region Astra DB clusters, where this is handled through region-specific Secure Connect Bundles.

==== Ports

Each {zdm-proxy} instance listens on port 9042 by default, like a regular Cassandra cluster. This can be overridden by setting `zdm_proxy_listen_port` to a different value. This can be useful if the Origin nodes listen on a port that is not 9042 and you want to configure the {zdm-proxy} to listen on that same port to avoid changing the port in your client application configuration.

The {zdm-proxy} exposes metrics on port 14001 by default. This port is used by Prometheus to scrape the application-level proxy metrics. This can be changed by setting `metrics_port` to a different value if desired.

== Use Ansible to deploy the proxy

Now you can run the playbook that you've configured above. From the shell connected to the container:

```bash
ansible-playbook deploy_zdm_proxy.yml -i zdm_ansible_inventory
```
That's it!

== Indications of success on Origin and Target clusters

The playbook will create one {zdm-proxy} instance for each proxy host listed in the inventory file. It will indicate the operations that it is performing and print out any errors, or a success confirmation message at the end.

How can you confirm that the ZDM proxies are up and running?

After running the playbook, you can `ssh` into one of the servers where one of the deployed {zdm-proxy} instances is running. You can do so from within the Ansible container, or directly from the jumphost machine:

```bash
ssh ubuntu@<zdm proxy ip address>
```

Then, use the `docker logs` command to view the logs of this ZDM proxy instance:

```bash
   .
   .
   .
ubuntu@ip-172-18-10-111:~$ sudo docker logs zdm-proxy-container
   .
   .
   .
time="2022-10-01T22:21:42Z" level=info msg="Initialized origin control connection. Cluster Name: OriginCluster, Hosts: map[3025c4ad-7d6a-4398-b56e-87d33509581d:Host{addr: 191.100.20.61,
port: 9042, host_id: 3025c4ad7d6a4398b56e87d33509581d} 7a6293f7-5cc6-4b37-9952-88a4b15d59f8:Host{addr: 191.100.20.85, port: 9042, host_id: 7a6293f75cc64b37995288a4b15d59f8} 997856cd-0406-45d1-8127-4598508487ed:Host{addr: 191.100.20.93, port: 9042, host_id: 997856cd040645d181274598508487ed}], Assigned Hosts: [Host{addr: 191.100.20.61, port: 9042, host_id: 3025c4ad7d6a4398b56e87d33509581d}]."

time="2022-10-01T22:21:42Z" level=info msg="Initialized target control connection. Cluster Name: cndb, Hosts: map[69732713-3945-4cfe-a5ee-0a84c7377eaa:Host{addr: 10.0.79.213,
port: 9042, host_id: 6973271339454cfea5ee0a84c7377eaa} 6ec35bc3-4ff4-4740-a16c-03496b74f822:Host{addr: 10.0.86.211, port: 9042, host_id: 6ec35bc34ff44740a16c03496b74f822} 93ded666-501a-4f2c-b77c-179c02a89b5e:Host{addr: 10.0.52.85, port: 9042, host_id: 93ded666501a4f2cb77c179c02a89b5e}], Assigned Hosts: [Host{addr: 10.0.52.85, port: 9042, host_id: 93ded666501a4f2cb77c179c02a89b5e}]."
time="2022-07-27T22:21:42Z" level=info msg="Proxy connected and ready to accept queries on 172.18.10.111:9042"
time="2022-07-27T22:21:42Z" level=info msg="Proxy started. Waiting for SIGINT/SIGTERM to shutdown."
```

In the logs, the important information to notice is:

```bash
time="2022-07-27T22:21:42Z" level=info msg="Proxy connected and ready to accept queries on 172.18.10.111:9042"
time="2022-07-27T22:21:42Z" level=info msg="Proxy started. Waiting for SIGINT/SIGTERM to shutdown."
```

Also, you can check the status of the running Docker image. Example:

```bash
ubuntu@ip-172-18-10-111:~$ sudo docker ps
CONTAINER ID  IMAGE                     COMMAND  CREATED      STATUS     PORTS   NAMES
02470bbc1338  datastax/zdm-proxy:2.0.x  "/main"  2 hours ago  Up 2 hours         zdm-proxy-container
```
== Setting up Monitoring on the control host

Follow these steps to install the monitoring stack.  We'll use https://grafana.com/[Grafana] to visualize the data.

Make sure you are connected to the Ansible Control Host docker container. As above, you can do so from the jumphost machine by running:
```bash
docker exec -it zdm-ansible-container bash
```
You will see a prompt like:
```bash
ubuntu@52772568517c:~$
```

=== Configure the Grafana credentials

Edit the file `zdm_monitoring_config.yml`, located in `zdm-proxy-automation/ansible/vars`:
. `grafana_admin_user`: leave unchanged (defaults to `admin`)
. `grafana_admin_password`: set to the password of your choice

=== Run the monitoring playbook

Use the following command:

```bash
ansible-playbook deploy_zdm_monitoring.yml -i zdm_ansible_inventory
```

=== Check the Grafana dashboard

In a browser, open http://<jumphost_public_ip>:3000.

Login with:

**username**: admin

**password**: the password you configured

[TIP]
====
For Grafana dashboard examples and details about the metrics you can observe, see xref:troubleshooting-tips.adoc#how-to-leverage-metrics[this section] of the troubleshooting tips.
====
